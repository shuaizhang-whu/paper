# NSNet：Compression of Large Language Models by Condensed Weight Representation

它与 SVD 系列的区别很大：它不是做低秩分解，而是把一个线性层的整个权重矩阵 $W$ 用一个**一维向量 $S$** 通过“滑动窗口 + 重叠共享”来生成，从而在**层内实现参数共享**。随后用一个**回归式初始化（有闭式解）\**把预训练权重“投影”到这种共享结构上，再用\**知识蒸馏**把性能拉回。Wang 等 - Compression of large l…

------

## 1）原始问题：线性层权重参数量巨大，层内存在“行（神经元）之间的冗余”

一个标准线性层权重：
$$
W\in\mathbb{R}^{D_{\text{out}}\times D_{\text{in}}}
$$
参数量是 $D_{\text{out}}D_{\text{in}}$。论文把 $W$ 的每一行称为一个“neuron”（输出通道对应的权重向量）：
$$
w_i \in \mathbb{R}^{D_{\text{in}}},\quad i=1,\dots,D_{\text{out}}
$$
核心观察：**同一层的不同行权重向量之间存在可共享的信息**，但大多数方法（低秩、剪枝、量化）主要利用“跨层”或“整体矩阵”的结构，较少直接做“层内行与行”的共享编码。

------

## 2）NS 表示：用一维向量 $S$ 生成整个矩阵 $W_c(S)$

### 2.1 NS 向量长度与压缩率

设压缩率（论文里是“压缩后参数量 / 原始参数量”）为：
$$
r\in(0,1)
$$
构造 NS 向量：
$$
S \in \mathbb{R}^{L},\quad L=\big\lfloor D_{\text{in}}D_{\text{out}}\,r\big\rfloor
$$
当 $r\ll 1$ 时，参数量从 $D_{\text{in}}D_{\text{out}}$ 变为 $L$。

### 2.2 滑动窗口 + 重叠：每个 neuron 从 $S$ 中截取一段长度 $D_{\text{in}}$ 的连续片段

定义 stride：
$$
s=\left\lfloor\frac{L-D_{\text{in}}}{D_{\text{out}}}\right\rfloor
$$
第 $i$ 个 neuron 的起始下标（1-based）：
$$
t_i = 1 + (i-1)s
$$
用 $S$ 的连续片段作为第 $i$ 行的“重建权重”：
$$
\hat w_i(S) = S_{t_i:\ t_i + D_{\text{in}}-1}\in\mathbb{R}^{D_{\text{in}}}
$$
把所有 $\hat w_i$ 叠起来得到重建矩阵：
$$
W_c(S)=
\begin{bmatrix}
\hat w_1(S)\\
\hat w_2(S)\\
\vdots\\
\hat w_{D_{\text{out}}}(S)
\end{bmatrix}
\in\mathbb{R}^{D_{\text{out}}\times D_{\text{in}}}
$$
由于 stride 通常小于 $D_{\text{in}}$，相邻行对应的片段会**重叠**，这就导致**多个位置共享同一个 $S_j$**，从而把“层内冗余”编码成共享参数。

### 2.3 前向计算

输入 $x\in\mathbb{R}^{D_{\text{in}}}$，NS-Linear 的输出仍是：
$$
y = x W_c(S)^\top
$$
工程实现上就是：根据 $t_i$ 从 $S$ 切片得到 $W_c$，再做一次 GEMM（论文伪代码就是这个流程）。

------

## 3）从“权重矩阵拟合”到“可解的回归初始化”：优化目标与闭式解

预训练模型没有 $S$，因此要把已有 $W$ 映射到 NS 表示。论文提出 **regression-based initialization**：对每个线性层（或者对全模型所有线性层求和）做最小二乘，让重建矩阵尽量接近原矩阵。

### 3.1 初始化的基本优化目标（MSE / Frobenius）

对第 $m$ 个线性层：
$$
\min_{S^{(m)}}\ \frac{1}{2}\|W_c^{(m)}(S^{(m)}) - W^{(m)}\|_F^2
$$
全模型 $M$ 个线性层合起来：
$$
\min_{\{S^{(m)}\}_{m=1}^M}\ 
\mathcal{L}(S)
=\frac{1}{2}\sum_{m=1}^{M}\|W_c^{(m)}(S^{(m)}) - W^{(m)}\|_F^2
$$
展开到行向量：
$$
\mathcal{L}(S)=\frac{1}{2}\sum_{m=1}^{M}\sum_{i=1}^{D_{\text{out}}^{(m)}} 
\|\hat w_i^{(m)}(S^{(m)}) - w_i^{(m)}\|_2^2
$$

### 3.2 把“重叠共享”写成索引集合：目标可分解到每个 $S_j$

关键是：每个 $S_j$ 会被多个 neuron 的多个位置复用。定义（对某一层，省略 $m$）：

- $\Omega_j = \{(i,k)\ |\ \hat w_i[k] = S_j\}$：所有使用了 $S_j$ 的（行 $i$、列内偏移 $k$）位置集合
- $c_j = |\Omega_j|$：$S_j$ 被使用的次数
- 对应原矩阵中“同位置”的真实权重记为 $w_{i,k}$

则误差（论文附录给了等价表达）可以写成：
$$
\mathrm{Err}(S)=\sum_{i=1}^{D_{\text{out}}}\|\hat w_i(S)-w_i\|_2^2
=\sum_{j=1}^{L}\ \sum_{(i,k)\in\Omega_j}\ (w_{i,k}-S_j)^2
$$
注意这个式子非常重要：它表明目标对不同 $j$ **完全可分离**。

### 3.3 闭式最优解：每个 $S_j$ 等于其覆盖到的权重的平均值

对每个 $j$：
$$
\min_{S_j}\ \sum_{(i,k)\in\Omega_j}\ (w_{i,k}-S_j)^2
$$
一元二次，令导数为 0：
$$
\frac{\partial}{\partial S_j}\sum_{(i,k)\in\Omega_j}(w_{i,k}-S_j)^2
= -2\sum_{(i,k)\in\Omega_j}(w_{i,k}-S_j)=0
$$
得到：
$$
S_j^*=\frac{1}{c_j}\sum_{(i,k)\in\Omega_j} w_{i,k}
$$
这就是论文说的：**初始化本质是把被同一个 NS 元素覆盖的权重做“求均值”**。
 它比常规“直接训练一个 student”要轻很多：对每层一次遍历即可完成。

### 3.4 一个直观解释：NS 初始化 = “把权重向量化后做带结构约束的共享聚合”

如果把 $W$ 展平成 $D_{\text{out}}D_{\text{in}}$ 个标量，那么 NS 表示相当于规定了一个固定的“分组共享映射”：
$$
\text{每个组共享一个参数 } S_j
$$
初始化就是对每组做最小二乘的均值解。

------

## 4）为什么仅初始化还不够：引入知识蒸馏微调（KD）

由于共享与重叠带来的逼近误差不可避免，初始化后的 NSNet 与 teacher（原模型）存在性能差距。于是论文用 teacher-student 蒸馏把 student 的输出分布对齐回 teacher。

### 4.1 蒸馏 + 监督的总目标

$$
\mathcal{L}_{\text{fine-tune}}
=\mathcal{L}_{\text{distill}}+\lambda\mathcal{L}_{\text{task}}
$$

其中 $\lambda$ 是权重系数。

### 4.2 蒸馏项：温度缩放的 KL

给定样本 $x_i$，teacher 与 student 的 logits 分别为 $z_i^{(\text{tea})}, z_i^{(\text{stu})}$，温度 $\tau>1$：
$$
\mathcal{L}_{\text{distill}}
=\sum_{i=1}^{N}\tau^2\,
\mathrm{KL}\Big(
\mathrm{softmax}(z_i^{(\text{tea})}/\tau)\ \|\ 
\mathrm{softmax}(z_i^{(\text{stu})}/\tau)
\Big)
$$
$\tau^2$ 是常见的温度补偿因子。

### 4.3 任务项：按具体任务选择（分类/生成/相似度）

论文描述为通用形式：
$$
\mathcal{L}_{\text{task}}=\frac{1}{N}\sum_{i=1}^{N}\ell_T(y_i, z_i^{(\text{stu})})
$$
例如分类/生成常用交叉熵或 KL，相似度可用 $\ell_2$。

------

## 5）NSNet 的完整 Step 实现流程（从预训练模型到可用压缩模型）

### Step 0：选择压缩率 $r$ 并构造每层 NS 形状

对每个线性层，记录 $D_{\text{in}},D_{\text{out}}$，计算：
$$
L=\lfloor D_{\text{in}}D_{\text{out}}r\rfloor,\quad
s=\left\lfloor\frac{L-D_{\text{in}}}{D_{\text{out}}}\right\rfloor,\quad
t_i=1+(i-1)s
$$
并创建可训练参数 $S\in\mathbb{R}^{L}$。

### Step 1：回归初始化（闭式均值解）

给定原层权重 $W$：

1. 对每个 $j\in[1,L]$ 找到其覆盖到的集合 $\Omega_j$（可以通过遍历所有行的切片位置构造）
2. 设 $c_j=|\Omega_j|$
3. 赋值：

$$
S_j \leftarrow \frac{1}{c_j}\sum_{(i,k)\in\Omega_j} w_{i,k}
$$

这样得到初始化后的 NS-Linear 层。

（实现技巧：不需要显式存 $\Omega_j$，可以用“累加器 + 计数器”一次遍历完成：遇到某个 $S_j$ 被用到，就把对应的 $w_{i,k}$ 加到 sum[j]，count[j]++。）

### Step 2：替换网络结构

把原模型所有 linear 层替换为 NS-Linear（LayerNorm/head 可保持不变，论文也常这么做）。

### Step 3：知识蒸馏微调

以原模型为 teacher、NSNet 为 student，最小化：
$$
\mathcal{L}_{\text{fine-tune}}
=\mathcal{L}_{\text{distill}}+\lambda\mathcal{L}_{\text{task}}
$$
训练若干 epoch，把性能补回来。

### Step 4：推理时的权重构造与计算

前向时按 $t_i$ 从 $S$ 切片堆出 $W_c(S)$ 或者直接以切片方式实现等价计算，然后做 GEMM 得到输出。

------

## 6）这篇方法的“理论本质”与 SVD/低秩方法的对照（帮助你后续融合时抓住差异点）

1. **SVD 系列**：
    在一个矩阵上做

$$
W \approx UV^\top,\quad \mathrm{rank}(U)=\mathrm{rank}(V)=k
$$

压缩依赖“谱能量集中”和（激活/白化）加权误差。

1. **NS 系列**：
    不是分解矩阵，而是引入一种**参数共享生成器**：

$$
W_c(S) = \text{Stack}\big(S_{t_i:t_i+D_{\text{in}}-1}\big)_{i=1}^{D_{\text{out}}}
$$

它依赖的不是谱结构，而是“行与行之间局部相似/可共享”。

1. **NS 初始化可闭式求解**这一点也很关键：
    SVD-LLM/ASVD 的“初始化”是一次分解；NS 的初始化是一次“按共享结构做均值投影”。

------

## 7）一些可延展的 idea（只给方向，不展开详细结合方案）

1. **把 NS 的共享结构从“固定 stride”改为“可学习/自适应 stride 或可学习索引”**
    现在共享图完全由 $s$ 决定。可以考虑让每行起点 $t_i$ 由一个小网络或离散优化产生（例如让相似的 neuron 更重叠），目标仍是最小化 $\|W_c-W\|_F^2$ 或激活加权误差。
2. **把初始化目标从 $\|W_c-W\|_F^2$ 升级为 activation-aware 版本**
    把回归初始化改成：

$$
\min_S \ \|(W_c(S)-W)X\|_F^2
$$

这样 NS 的“投影”会对齐校准激活分布，可能更贴近你熟悉的 ASVD/SVD-LLM 逻辑。

1. **NS 作为“第二阶段压缩器”：先低秩，再把低秩因子做 NS 编码**
    论文本身就提到：可以把 $W$ 替换为某种低秩方法得到的 $U,V$（或 LoSparse 的低秩部分），再对 $U,V$ 各自用 NS-Linear 表示继续压缩。这样整体会变成“低秩结构 + 共享结构”的复合。
2. **把 NS 的共享看成一种结构化“分组量化/共享码本”，与量化联合**
    NS 的每个 $S_j$ 被复用多次，本质像“共享参数”。如果再做低比特量化，可能出现更强的存储收益。更进一步可以把 $S$ 变成码本 + 索引（类似向量量化）以获得离散压缩。
3. **用误差传播视角重新设计重叠：重要层/重要矩阵更少共享**
    共享越多（stride 越小或 $r$ 越低），逼近误差越大。你可以让不同层使用不同 $r_\ell$ 或不同 stride，甚至对 attention/FFN 采用不同共享强度，以控制误差传播风险。