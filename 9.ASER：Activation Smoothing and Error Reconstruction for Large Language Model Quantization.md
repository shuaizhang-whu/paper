# ASER（Activation Smoothing and Error Reconstruction）论文分析：理论推导 + Step 实现流程 + Ideas

ASER 面向 **PTQ（Post-Training Quantization）低比特量化**场景，核心要解决的是：即使有 SmoothQuant / GPTQ 等技巧，在 **W4A8、尤其是更低激活比特**时，量化误差会显著放大，导致性能崩塌。ASER 的切入点是把量化看作一个“压缩优化”问题，并发现 **“激活-权重量化误差”在数据空间呈现低秩结构**，于是用 **LoRA-style 低秩补偿**去重构误差；同时发现误差主要来源于 **outlier 通道**，所以再配合 **激活平滑（把激活 outlier 迁移到权重并单独处理）**来让低秩补偿更有效。

------

## 1) 从基线量化目标到 ASER 改进目标：优化对象怎么变

### 1.1 基线：量化目标是最小化输出误差

对单层线性映射（忽略 bias）：

- 全精度权重：$W$
- 量化后权重：$W_q=Q(W)$
- 校准输入激活矩阵：$X$

典型量化优化目标可写为：
$$
\min_{W_q}\ \|WX - W_qX\|_F
\tag{1}
$$
令权重量化误差：
$$
E_q = W - W_q
$$
则目标等价为让 $\|E_qX\|_F$ 尽量小。

### 1.2 ASER：把“量化”转为“误差重构”

ASER 不直接再去改 $W_q$ 的量化过程，而是学习一个误差近似 $\tilde E_q$（低秩、LoRA-style），用它补偿量化误差：
$$
\min_{\tilde E_q}\ \|(E_q-\tilde E_q)X\|_F
\tag{2}
$$
推理时用：
$$
\tilde W = W_q + \tilde E_q,\qquad
\tilde WX = W_qX + \tilde E_qX
$$
这把问题变成了：**用很少参数（低秩）去拟合数据感知的误差 $E_qX$**。

------

## 2) 误差结构分析：为什么能用低秩补偿（ASER 的关键观察）

### 2.1 “数据感知误差” $E_qX$ 比 “权重误差” $E_q$ 更低秩

ASER 对比了奇异谱：$E_q$ 与 $E_qX$ 的 singular values 分布，观察到 **$E_qX$ 的谱更“尖”**（少数大奇异值 + 长尾），更符合低秩近似假设。

### 2.2 用 Effective Rank 衡量“层间/层内低秩强弱差异”

定义矩阵 $Z$ 的奇异值 $\{\sigma_k\}_{k=1}^n$，先做归一化：
$$
p_k = \frac{\sigma_k(Z)}{\sum_{i=1}^n \sigma_i(Z)}+\varepsilon
$$
定义 effective rank：
$$
\mathrm{EffRank}(Z)=\exp\left(-\sum_{k=1}^n p_k\log p_k\right)
\tag{3}
$$
结论（对设计很重要）：

- MHSA 相关线性层误差往往更低秩（更适合小 rank 补偿）
- FFN 相关线性层误差有效维度更高（可能需要更大 rank 或更强 outlier 处理）
- 模型浅层误差常更低秩（rank 可更小）

=> **rank 应该按层自适应，而不是所有层固定同一个 rank。**

### 2.3 Outlier 通道贡献了主要误差

作者发现把通道按 $\bar X\odot \bar W$（激活通道均值与权重通道均值的乘积）排序后，误差能量趋势与其高度一致：极少数（<1%）通道贡献了数量级更大的误差。
 => 仅靠低秩拟合“整体误差”会被 outlier 拉坏，需要先“平滑/迁移”这些 outlier。

------

## 3) ASER 组件一：Error Reconstruction（白化 SVD 推导出 LoRA-style 补偿）

### 3.1 白化激活：构造 $S$ 让通道独立同分布（近似）

目标是找到一个 $S$ 使得白化后：
$$
(S^{-1}X)(S^{-1}X)^\top = I
$$
ASER 用 Cholesky 从二阶矩得到 $S$（工程上你会对 $XX^\top$ 或等价形式做分解）：
$$
S^{-1}XX^\top(S^{-1})^\top = I
\tag{4}
$$
直觉：白化后，各通道不相关，使得后面的奇异值与“输出误差贡献”建立直接对应关系。

### 3.2 对 $E_qS$ 做 SVD 并截断

对误差做变换：
$$
E_qS = U\Sigma V^\top
$$
取 rank-$r$ 截断，得到：
$$
\tilde E_q = U_r\Sigma_rV_r^\top S^{-1}
\tag{5}
$$

### 3.3 核心推导：截断第 $i$ 个奇异分量的损失正比于 $\sigma_i$

ASER 给出一个关键等式：在白化设定下，去掉第 $i$ 个奇异分量带来的数据感知误差满足：
$$
L_i = \|(E_q-\tilde E_q^{(i)})X\|_F
      = \|\sigma_i u_i v_i^\top S^{-1}X\|_F
      = \sigma_i
\tag{6}
$$
这意味着：**奇异值越大，对 $\|E_qX\|_F$ 的贡献越大**，因此保留 top 奇异值的 rank-$r$ 是“最有效补偿”。

### 3.4 直接落地成 LoRA-style 窄矩阵

把式(5) 改写成 LoRA 形式：
$$
\tilde E_q = L_A L_B
$$
推理补偿：
$$
W_qX + L_A L_B X
$$

### 3.5 rank 选择：用奇异值累计占比阈值 $\alpha$

不一定固定 rank，而是用阈值 $\alpha\in(0,1)$ 选最小的 $r$ 使得累计奇异值比例满足：
$$
\frac{\sum_{i=1}^{r}\sigma_i}{\sum_{i=1}^{n}\sigma_i} \ge \alpha
\tag{8}
$$
（实现时通常是：对 $\sigma$ 排序后累加，找到刚超过 $\alpha$ 的位置。）

------

## 4) ASER 组件二：Activation Smoothing（outlier 迁移 + 免量化拆分 + 再重构）

ASER 的 activation smoothing 借鉴 SmoothQuant 的“难度迁移”思想，但目的是：**让 outlier 通道从激活侧迁移到权重侧，并把 outlier 权重拆出来不量化**，然后把“量化误差 + outlier 权重残差”一起交给低秩重构。

### 4.1 用对角缩放 $M$ 做迁移：把激活难度推到权重

$$
WX = W M \cdot M^{-1}X
\tag{9}
$$

其中 $M=\mathrm{diag}(m_1,\dots,m_n)$。

### 4.2 用 $\bar X\odot \bar W$ 找 outlier 通道集合 $I_f$

令：

- $\bar X_i$：第 $i$ 个通道激活绝对值均值
- $\bar W_i$：第 $i$ 个通道权重绝对值均值
- outlier 指标：$\bar X\odot \bar W$

取 top-$f$ 个通道构成集合 $I_f$。

### 4.3 构造 $M$：把 outlier 激活压到一个更平滑范围

对 outlier 通道：
$$
m_i = \frac{\bar X_i}{\min(\bar X_{I_f})},\quad i\in I_f
$$
其他通道：
$$
m_i = 1
\tag{10}
$$
这样 $M^{-1}X$ 的 outlier 会被缩小，更容易量化。

### 4.4 拆分权重：outlier 部分不量化

将缩放后的权重记为：
$$
W_M = WM = W_s + W_o
\tag{11}
$$
其中：

- $W_o$：outlier 通道对应的权重部分（按 $I_f$ 选取的结构部分），**不量化**
- $W_s$：剩余部分，正常量化：$Q(W_s)$

于是等价写成：
$$
WM\cdot M^{-1}X = (W_s+W_o)M^{-1}X
              = (Q(W_s) + (E_q+W_o))M^{-1}X
\tag{12}
$$
这里把“需要补偿的对象”从单纯的 $E_q$ 扩展为：
$$
E_q + W_o
$$
即：**量化误差 + outlier 权重残差** 一起补偿。

### 4.5 在平滑后的激活上做白化 + SVD，重构 $(E_q+W_o)$

对白化矩阵改用平滑后激活：
$$
X' = M^{-1}X
$$
用 $X'$ 求白化 $S$，并对：
$$
(E_q + W_o)S
$$
做 SVD，然后构造：
$$
L_A L_B \approx (E_q+W_o)S
\tag{13}
$$
最终推理成为：
$$
W_qX + L_A L_B X
$$
（工程实现里要注意把 $M$ 的变换合并进权重或输入路径，避免额外开销。）

------

## 5) ASER 的完整 Step-by-step 实现流程（对应可复现的工程管线）

下面给你一个“按层处理”的可落地版本（PTQ 常见做法：逐层统计 + 构造补偿矩阵）：

### Step 0：校准数据前向，缓存激活

- 用校准集 $D$（例如 128 samples、seq=2048 的设定）前向推理
- 对每个线性层缓存其输入激活 $X$（或统计量 $\bar X$，以及二阶矩用于白化）

------

### Step 1：对每层（每个线性权重矩阵）做 Activation Smoothing（可选开关）

1. 计算通道统计：
   $$
   \bar X,\ \bar W,\ \bar X\odot \bar W
   $$

2. 取 outlier 通道集合 $I_f$（top-$f$）

3. 构造对角缩放 $M$（式(10)）

4. 形成 $W_M=WM$，并按 outlier 通道拆分：
   $$
   W_M=W_s+W_o
   $$

   - $W_o$ 保留 FP（不量化）
   - $W_s$ 做量化：$Q(W_s)$

若不开启 A.S.：直接 $W_q=Q(W)$，并令 $W_o=0$。

------

### Step 2：构造“要补偿的误差矩阵”

- 无 A.S.：
  $$
  E_q = W - Q(W)
  $$

- 有 A.S.：
  $$
  E_q = W - Q(W_s) + W_o
  $$

（你可以把它理解为：把 outlier 的未量化部分也当作“需要被低秩吸收的残差”。）

------

### Step 3：白化矩阵 $S$

- 无 A.S.：用 $X$ 求 $S$
- 有 A.S.：用 $X' = M^{-1}X$ 求 $S$

目标是让白化后通道独立（式(4)）。

------

### Step 4：Whitening SVD + rank 选择

1. 计算：
   $$
   E_qS = U\Sigma V^\top
   $$

2. 按阈值 $\alpha$ 选 $r$（式(8)）

3. 构造 LoRA-style 补偿：
   $$
   L_A = U_r\Sigma_r,\quad L_B = V_r^\top S^{-1}
   $$

------

### Step 5：部署推理图

该层线性在量化推理中变为：
$$
y = W_q x\ (+W_o x\ \text{若你把 }W_o\text{单独显式保留})\ +\ L_A(L_Bx)
$$
并保证 $M$ 的迁移变换在输入/权重侧一致实现（通常把 $M$ 合并进权重量化前的缩放，避免推理端多一次显式 $M^{-1}$）。

------

## 6) 一些可延展的 Ideas（不展开“如何结合”，只给方向）

### Idea 1：把 outlier 选择从启发式 top-$f$ 升级为“误差贡献最大化”的最优化

目前 $I_f$ 用 $\bar X\odot \bar W$ 排序。可以直接以校准误差为目标，做一个近似的子集选择：
$$
\max_{I:\ |I|\le f}\ \| (E_qX)_{\text{channels }I}\|_F
$$
或者用可微近似（Gumbel-topk / L0 门控）学出 outlier 集合，让 smoothing 更“对准真正的误差通道”。

### Idea 2：rank 阈值 $\alpha$ 改为“按层自适应”，并与 Effective Rank 联动

可以用 $\mathrm{EffRank}(E_qX)$ 或 $\mathrm{EffRank}(E_qS)$ 来预测该层需要的 rank 上限：
$$
r_\ell = g(\mathrm{EffRank}_\ell)
$$
再在 $r_\ell$ 内用 $\alpha$ 选最小满足阈值的 rank，实现更稳的预算-精度折中。

### Idea 3：把白化从“全通道一次 Cholesky”扩展到“分块白化”

全通道白化可能数值不稳、开销也大。可做 block-diagonal 的 $S$（按 head / 按通道块）：
$$
S=\mathrm{blockdiag}(S_1,\dots,S_b)
$$
在不显著增加复杂度的前提下提升稳健性。

### Idea 4：补偿不只做单次 SVD：对补偿残差再做一次小 rank（两段式补偿）

一次 rank-$r$ 可能不足以覆盖 FFN 的高有效维误差。可做：
$$
E_q \approx L_A^{(1)}L_B^{(1)} + L_A^{(2)}L_B^{(2)}
$$
第二段只在误差大的层启用（稀疏触发），以更小的额外参数实现更强恢复。

### Idea 5：将 smoothing 的 $M$ 与补偿的 $L_A,L_B$ 做“联合目标”迭代更新

现在是先定 $M$ 再做 SVD。可以交替：

1. 固定 $M$ 求最优低秩补偿
2. 固定补偿，再更新 $M$ 让剩余误差更低秩
    类似 EM / alternating minimization，目标是让 $E_qX$ 更“可低秩”。