# ERC-SVD: Error-controlled SVD for large language model compression

它的核心非常“直给”：不是改 whitening，也不是改截断准则，而是从“误差控制”角度补上两块很多 SVD-LLM/ASVD 没做的东西：
 1）**截断残差的二次补偿（Residual Compensation, REC）**：把第一次截断留下的残差矩阵再做一次低秩截断并加回去；
 2）**只压最后若干层（Partial-layer Compression, PLC）**：在固定整体压缩率下，保持前面层完全不压，避免误差从早期层开始一路传播到末层。Bai 等 - ERC-SVD Error-controlle…

------

## 1）原始优化目标：activation-aware 的截断误差最小化

他们沿用 SVD-LLM/ASVD 的 post-training 设定：对每个权重矩阵 $W\in\mathbb{R}^{m\times n}$，用校准数据得到输入激活 $X$，以输出误差衡量截断损失：
$$
\hat W_r
=\arg\min_{W_r}\ \|WX - W_rX\|_F
\tag{2}
$$
等价写成：
$$
L=\|WX-W_rX\|_F=\|(W-W_r)X\|_F
\tag{3}
$$
然后他们指出：现有方法虽然在 $W_r$ 的求法（whitening、迭代等）上做改进，但**都把被截断部分当作“丢掉就丢掉”**，没把“残差矩阵”当作可利用的信息源。于是他们把目标进一步“回退”到权重逼近视角（便于解释残差补偿）：
$$
\hat W_r = \arg\min_{W_r}\ \|W-W_r\|
\tag{4}
$$
这里的范数在论文表述上是为了说明“最终就是逼近 $W$”；实际算法仍结合 whitening 做 truncation。Bai 等 - ERC-SVD Error-controlle…

------

## 2）改进 1：Residual Compensation（REC）——两阶段 SVD 截断 + 残差再截断

### 2.1 传统单次截断（回顾）

标准 SVD：
$$
W=U\Sigma V^\top
$$
传统方法直接取前 $r$ 个奇异值：
$$
W_r = U_r\Sigma_r V_r^\top
\tag{1}
$$

### 2.2 他们的关键改动：把“总 rank $r$”拆成两段 $r_i+r_r=r$

他们先定义一个“矩阵有效尺度”：
$$
\alpha=\frac{mn}{m+n}
\tag{5}
$$
并把“层压缩率” $R_l$ 定义为**删除参数比例**（removed fraction）。于是目标 rank（对应保留参数量）写成：
$$
r=(1-R_l)\cdot \alpha
$$
然后引入残差补偿因子 $\beta$（实验中固定 0.05），定义残差补偿 rank：
$$
r_r=\alpha\cdot \beta,\qquad r_i=r-r_r
$$
也就是：先用 $r_i$ 对 $W$ 做一次截断，再用 $r_r$ 对残差 $R$ 截断。Bai 等 - ERC-SVD Error-controlle…

### 2.3 两阶段截断的“改进后目标”与构造

第一阶段（中间近似）：
$$
W_{r_i}=U_{r_i}\Sigma_{r_i}V_{r_i}^\top
\tag{6}
$$
残差矩阵：
$$
R = W - W_{r_i}
\tag{7}
$$
第二阶段对残差做 SVD 截断，保留前 $r_r$：
$$
R_{r_r}=U_{r_r}\Sigma_{r_r}V_{r_r}^\top
$$
最终压缩矩阵是相加：
$$
\hat W_r = W_{r_i}+R_{r_r}
\tag{8}
$$
**直觉**：
 单次 rank-$r$ 截断只能捕获最大的 $r$ 个谱方向；而残差里仍可能包含“很多中等能量但对输出很关键”的结构（特别是经过 whitening 后，谱重要性会变化）。第二次截断相当于在“被丢掉的子空间”里再挑一部分重要方向加回来，减少 truncation loss。Bai 等 - ERC-SVD Error-controlle…

### 2.4 为什么相加后推理仍是“两个低秩乘”（不会回到大矩阵）

他们在算法里把奇异值吸收到左右因子里（附录 Algorithm 2 第 8-10 行）：
$$
\hat U_{r_i}=U_{r_i}\sqrt{\Sigma_{r_i}},\quad \hat V_{r_i}=\sqrt{\Sigma_{r_i}}V_{r_i}^\top
$$
再把两个因子“拼接”成一个更宽的低秩分解（列拼接/行拼接）：
$$
\hat U_r=[\hat U_{r_i}\ \ \hat U_{r_r}],\quad
\hat V_r=\begin{bmatrix}\hat V_{r_i}\\ \hat V_{r_r}\end{bmatrix}
$$
于是
$$
\hat W_r = \hat U_r\hat V_r
$$
rank 仍是 $r_i+r_r=r$，推理仍是一次 $\hat V_rX$ 再一次 $\hat U_r(\cdot)$。Bai 等 - ERC-SVD Error-controlle…

------

## 3）改进 2：Partial-layer Compression（PLC）——固定整体压缩率，只压最后 k 层

### 3.1 动机：误差传播与“早期压缩灾难”

论文在 page 6 的 Figure 3/4（图和文字一起）给出很明确的现象：
 如果你为了满足整体压缩率，把前几层也压得很狠（例如只压前 8 层，为了吃掉整体预算，每层要极高压缩），那么这些早期层的逼近误差会被后续所有层不断放大，导致**层误差一路累积**，最终性能很差。相反，只压最后若干层，前面层误差为 0，整体累计误差显著更低。Bai 等 - ERC-SVD Error-controlle…

### 3.2 固定整体压缩率 $R_o$ 下，k 与层压缩率 $R_l$ 的关系

设模型总层数 $N$，只压最后 $k$ 层。要满足整体删除比例 $R_o$，则每个被压层需要承担更高的删除比例 $R_l$：
$$
R_l = \frac{N\cdot R_o}{k},\qquad
k\in\{1,2,\dots,N-1\mid R_l<1\}
\tag{9}
$$
这条式子很关键：它把“整体预算”变成“最后 k 层的局部预算”。Bai 等 - ERC-SVD Error-controlle…

### 3.3 如何选 k：用校准集直接最小化“末层误差”

他们发现“末层误差”和 zero-shot 平均准确率高度相关（Figure 4 的 Kendall 相关非常强），所以提出：枚举候选 $k$（步长 $s$），对每个候选先按 $R_l$ 压缩最后 $k$ 层，然后计算压缩模型相对原模型的**最后一层输出误差**，选误差最小的 $k$。附录 Algorithm 3 给了清晰伪代码。Bai 等 - ERC-SVD Error-controlle…

------

## 4）ERC-SVD 的完整 Step 实现流程（对齐论文 Algorithm 1/2/3）

下面按“整体算法”的工程步骤写一遍（基本就是论文三段伪代码合起来）：

### Step A：准备校准数据与 whitening（沿用 SVD-LLM 一套）

1. 从数据集中随机采样校准样本 $CD$（论文实验是 256 条、长度 2048）
2. 对要压缩的层/矩阵收集输入激活，构造 whitening 矩阵集合 $S_i$（Algorithm 2 第 2-4 行）
    实际上他们是先 whitening 再做截断，这点与 SVD-LLM 一致。Bai 等 - ERC-SVD Error-controlle…

### Step B：PLC 选 k 和 $R_l$

1. 枚举候选 $k'\in\{s,2s,\dots,N-s\}$
2. 计算 $R_l'=(N R_o)/k'$
3. 临时复制模型，只对最后 $k'$ 层做 REC 压缩
4. 在校准集上算末层输出误差 $\mathrm{Err}$，记录最小误差对应的 $k,R_l$（Algorithm 3）Bai 等 - ERC-SVD Error-controlle…

### Step C：对选中的最后 k 层逐层做 REC（Residual Compensation）

对每个被压缩层里的每个矩阵 $W\in\mathbb{R}^{m\times n}$：

1. 计算 $\alpha=\frac{mn}{m+n}$
2. 计算总 rank：$r=(1-R_l)\alpha$
3. 取 $r_r=\alpha\beta$，$r_i=r-r_r$
4. 第一阶段：对 $WS$ 做截断 SVD 得 $W_{r_i}$（Algorithm 2 第 5 行）
5. 残差：$R=W-W_{r_i}$（第 6 行）
6. 第二阶段：对 $R$ 做截断 SVD 得 $R_{r_r}$（第 7 行）
7. 吸收奇异值并拼接因子得到 $\hat U_r,\hat V_r$，替换原矩阵（第 8-10 行）Bai 等 - ERC-SVD Error-controlle…

### Step D：更新模型并做一次校准（可选）

Algorithm 1 第 13 行有一个 UPDATE，用校准数据把替换后的权重写回并做必要的结构更新（比如把分解形式装进模块）。Bai 等 - ERC-SVD Error-controlle…

------

## 5）它与 SVD-LLM v2 / SAES-SVD 的主要理论区别（抓关键点）

1. **与 SVD-LLM v2（截断感知/whitening）**

- SVD-LLM v2 的主线：通过特定 whitening 让“奇异值 ↔ truncation loss”对齐，解决“截断该丢谁”的问题。
- ERC-SVD 的主线：假设你已经能做出不错的截断（他们也用 whitening），但依然会损失明显；于是用“残差再截断 + 只压末层”从误差控制角度补齐“丢掉以后怎么办”和“误差会不会传爆”。

1. **与 SAES-SVD（累计误差/局部误差自适应抑制）**

- SAES-SVD 是在“压所有层”的前提下，显式抑制“累计误差”和“局部误差”的协同放大。
- ERC-SVD 更激进：直接把大头问题（误差传播）用结构策略解决——**不压前层**，让累计误差链条从源头断掉；然后用残差补偿削减单层误差。

1. **与 Nested activation-aware decomposition（你刚分析过的 NSVD）**

- NSVD 是“两段低秩相加”：第一段 activation-aware，第二段 matrix-aware（残差补偿是对 $W$ 的残差）。
- ERC-SVD 的 REC 在形式上非常像“对 $W$ 做两段 SVD 相加”，但它的第二段是对**第一次截断残差**再做一次 SVD；更接近“谱域补偿”，而 NSVD 的动机更偏“分布外泛化（避免对校准激活过拟合）”。

------

## 6）一些可延展的 idea（只给方向，不展开详细融合方案）

1. **把残差补偿从“固定 $\beta$”改成“按层/按矩阵自适应 $\beta_i$”**
    论文把 $\beta$ 固定 0.05，说明鲁棒但也说明分配不精细。可以用“该层末层误差贡献”或“残差谱能量占比”决定 $\beta_i$，让 $r_r$ 更像一个资源分配问题。
2. **把 REC 的第二次截断改成 activation-aware（对残差的加权截断）**
    现在第二次是对 $R$ 直接 SVD。你可以把目标换成：

$$
\min_{\operatorname{rank}(B)=r_r}\ \|(R-B)X\|_F
$$

这样补回来的部分更贴近功能输出误差（尤其在 outlier 激活下可能更有效）。

1. **PLC 的“只压最后 k 层”可以扩展为“跳层压缩（隔层压缩）”或“分段压缩”**
    只压末层是一种极端策略。更一般是选一个层集合 $\mathcal{K}$，在固定预算下最小化末层误差：

$$
\min_{\mathcal{K},\{r_\ell\}} \ \mathrm{Err}_{\text{final}}
\quad \text{s.t.}\ \sum_{\ell\in\mathcal{K}} \text{params\_removed} = R_o\cdot \text{total}
$$

这会把它变成更通用的“误差控制层选择”。

1. **把“末层误差”换成“多点探针误差”**
    末层误差相关性很强，但对一些任务可能不够。可以在若干关键层（比如每 4 层一个 probe）都测输出误差，做加权和，得到更稳定的层选择指标。
2. **把 REC 视为一种“二次基展开”，推广到多次残差展开（Residual boosting）**
    形式上：

$$
W \approx \sum_{t=1}^{T} B_t,\quad B_t=\text{rank-}r_t\text{ SVD}(R_{t-1})
$$

其中 $R_{t}=R_{t-1}-B_t$。当 $T=2$ 就是 ERC-SVD。更大的 $T$ 可能在同等 rank 下带来更好的能量覆盖（代价是分解成本与工程复杂度）。