# GRASP：用“梯度敏感的奇异参数”替代冗余层的结构化压缩

GRASP 的目标不是做常规“低秩压缩每一层”，而是先**找出一批功能冗余的层**（通常是相邻层输出几乎不变），然后**不直接删层**，而是把这些冗余层里的关键“谱方向”保留下来：对冗余层的每个权重矩阵做 SVD，把奇异分量按“对任务损失的梯度敏感度”打分，只保留最关键的一小部分奇异组（比如 10%），用这些参数来**替代被视为冗余的层**，从而在接近“删层”的推理收益下，显著减小精度损失。

------

## 1) 从“删层”到“保留少量奇异参数”：问题与改进目标

### 1.1 直接删层的问题

若把冗余层直接跳过（skip），会破坏中间表征的对齐与信息流，导致性能下降（尤其在推理/对齐任务上更明显）。GRASP 的思路是：**冗余层虽然整体变化小，但仍包含少量对下游损失敏感的方向**；删层会把这些方向一起删掉。

### 1.2 GRASP 的替代目标（结构化替换而非训练新模块）

对选中的冗余层 $l$，对其内部多个线性权重（attention/MLP 的投影矩阵集合）分别做：

- SVD 分解：$W = U\Sigma V^\top$
- 在奇异组 $\Phi_k=\{u_k,\sigma_k,v_k\}$ 上做重要性打分
- 只保留 top-$r\%$ 的奇异组，重构 $\tilde W$

最终被替代层在前向上变成“低秩谱截断后的等效线性映射”，但**截断准则不是奇异值大小，而是梯度敏感度**。

------

## 2) Step 1：冗余层选择（输出表征相似度）

对第 $i$ 层输入/输出 hidden state（按 token 或按 batch 聚合），记为：
$$
H_i\in\mathbb{R}^{d},\quad H_{i+1}\in\mathbb{R}^{d}
$$
用余弦相似度衡量该层对表征的“改变量”：
$$
\cos(H_i,H_{i+1})=\frac{H_i^\top H_{i+1}}{\|H_i\|_2\ \|H_{i+1}\|_2}
\tag{1}
$$
**相似度越高 ⇒ 层越冗余**。GRASP 选取相似度最高的 top-$L$ 层作为冗余层集合（候选可被替代）。

> 这里的关键是：冗余层选择是“结构层面”的，后续才是在这些层内部做“谱级别”的保留。

------

## 3) Step 2：用“梯度归因”选择要保留的奇异组（核心理论）

### 3.1 SVD 的 rank-one 展开：把权重拆成可打分的“奇异组”

对冗余层中某个权重矩阵 $W\in\mathbb{R}^{m\times n}$：
$$
W = U\Sigma V^\top
$$
等价写成 rank-one 求和：
$$
W=\sum_{k=1}^{\ell} u_k \sigma_k v_k^\top,\quad \ell=\min(m,n)
\tag{2}
$$
定义第 $k$ 个**奇异组**：
$$
\Phi_k=\{u_k,\sigma_k,v_k\}
$$
GRASP 的目标：在所有 $\Phi_k$ 中选出最重要的一小部分保留。

------

### 3.2 “删掉某个参数”对 loss 的影响：Taylor 近似

对任意参数 $\theta$（可以是 $\sigma_k$ 或 $u_k$ 的某个元素、$v_k$ 的某个元素），删掉它的影响可用二阶 Taylor 展开近似：
$$
T(\theta)=\Big|\theta^\top\nabla_\theta \mathcal{L}+\frac12\theta^\top H\theta+O(\|\theta\|^3)\Big|
\tag{4}
$$
其中 $\mathcal{L}$ 是 LM 训练目标（例如自回归负对数似然）：
$$
\mathcal{L}= -\sum_t \log P(y_t\mid x_{\le t})
\tag{5}
$$
为了降低计算，GRASP 省略 Hessian 项，仅保留一阶项（“梯度 × 参数”）：
$$
T(\theta)\approx \big|\theta\cdot \frac{\partial\mathcal{L}}{\partial \theta}\big|
$$

------

### 3.3 奇异组的重要性打分：把组内所有分量的一阶贡献加和

GRASP 给每个 $\Phi_k$ 定义重要性：
$$
I(\Phi_k)=T(\sigma_k)+\sum_{i=1}^{m}T(u_{k,i})+\sum_{j=1}^{n}T(v_{k,j})
\tag{3}
$$
用一阶近似后得到可计算的形式：
$$
I(\Phi_k)=
\Big|\sigma_k\frac{\partial\mathcal{L}}{\partial\sigma_k}\Big|
+
\sum_{i=1}^{m}\Big|u_{k,i}\frac{\partial\mathcal{L}}{\partial u_{k,i}}\Big|
+
\sum_{j=1}^{n}\Big|v_{k,j}\frac{\partial\mathcal{L}}{\partial v_{k,j}}\Big|
\tag{6}
$$

> 直觉：**不是“奇异值大就重要”，而是“在校准数据上，对 loss 更敏感的奇异方向更重要”**。这也解释了论文里做的敏感性实验现象：小奇异值也可能很关键。

------

### 3.4 为什么 $\frac{\partial\mathcal{L}}{\partial\sigma_k}$ 好算：奇异值对 $W$ 的导数

GRASP 在附录给了关键结论（你实现时很有用）：

- 奇异值微分：

$$
\partial \sigma_i = u_i^\top (\partial W)\ v_i
$$

- 因此：

$$
\frac{\partial \sigma_i}{\partial W}=u_i v_i^\top
$$

- 对 loss：

$$
\frac{\partial\mathcal{L}}{\partial\sigma_i}
=
u_i^\top\Big(\frac{\partial\mathcal{L}}{\partial W}\Big)v_i
$$

也就是把权重梯度矩阵 $G=\frac{\partial\mathcal{L}}{\partial W}$ 投影到该奇异方向 $u_i,v_i$ 上。

------

## 4) 用打分做“保留 top-r% 奇异组”并重构权重

对每个冗余层的每个矩阵 $W$：

1. SVD：$W=U\Sigma V^\top$
2. 计算所有 $I(\Phi_k)$
3. 取重要性最高的集合 $\mathcal{K}$，$|\mathcal{K}|=\lceil r\%\cdot \ell\rceil$
4. 用这些奇异组重构：

$$
\tilde W=\sum_{k\in\mathcal{K}} u_k\sigma_k v_k^\top
$$

将该层的原矩阵用 $\tilde W$ 替换，从而“层仍存在，但只剩少量关键谱分量”。

------

## 5) 训练自由（training-free）与轻量补偿（可选）

- **training-free 版本**：只用校准数据（如 WikiText-2）算余弦相似度、算梯度归因、做一次替换即可，不做额外训练。
- **可选补偿**：论文也讨论了在极端压缩下，保留的那部分参数可作为少量可训练参数做快速恢复（比起插入随机初始化轻量模块更好收敛），但 GRASP 的核心卖点是：**即使不训练也能明显优于直接删层**。

------

## 6) 可复现的 Step-by-step 实现流程（工程视角）

下面按论文 Algorithm 的顺序，给一套你直接能写代码的流程（以 Transformer 模型为例）：

### Step 0：准备校准数据与前向/反向能力

- 取一个小校准集 $D$（论文示例 512 条 WikiText-2）
- 需要能对模型做反向传播拿到各层权重梯度（无需更新参数）

------

### Step 1：计算每层冗余度并选冗余层集合

对每层 $i$：

1. 前向收集该层输入输出 hidden state（可对 batch/token 做均值）
2. 计算 $\cos(H_i,H_{i+1})$
3. 选取相似度最高的 top-$L$ 层作为冗余层

------

### Step 2：对冗余层逐层处理（推荐从深到浅）

对每个冗余层 $l$（按从最后一个冗余层往前）：

1. 对该层每个权重矩阵 $W\in\{\text{q,k,v,o, up, down, gate...}\}$：
   - 做 SVD：$W=U\Sigma V^\top$
2. 用校准集做一次（或若干 batch）前向 + loss，反向得到梯度 $G=\partial\mathcal{L}/\partial W$
3. 计算每个奇异组的 $I(\Phi_k)$：
   - $\partial\mathcal{L}/\partial \sigma_k = u_k^\top G v_k$
   - 组内对 $u_k,v_k$ 元素的梯度可从 autograd 中取（或用链式法则进一步推，但工程上直接取更省事）
4. 取 top-$r\%$ 的奇异组并重构 $\tilde W$
5. 用 $\tilde W$ 替换原矩阵（并可将其存为低秩表示以减少存储）

------

### Step 3：导出压缩模型

- 冗余层仍在计算图中，但权重只保留少数谱分量（低秩）
- 推理时开销接近删层（因为这些冗余层可视作“近似低秩、开销很低”），但精度更稳

------

## 7) 这篇论文给你的“可扩展 Ideas”（不展开结合方案）

### Idea 1：把“奇异组打分”改成更直接的“分量置零 Δloss 实测”混合估计

GRASP 用一阶近似 $|\theta\cdot \partial\mathcal{L}/\partial\theta|$。你可以做混合：

- 先用一阶打分筛出 top 候选集合 $\mathcal{C}$
- 再对 $\mathcal{C}$ 内少量奇异组做“置零实测”估计真实 $\Delta\mathcal{L}$
   这样在计算量可控的前提下，提高排序可靠性。

### Idea 2：重要性从“组内求和”升级为“组级张量度量”

当前：
$$
I(\Phi_k)=\sum |u_{k,i}g_{u_{k,i}}|+\cdots
$$
可尝试更稳健的组级范数形式，比如：
$$
I(\Phi_k)=
\big|\sigma_k g_{\sigma_k}\big|
+\lambda_u\|u_k\odot g_{u_k}\|_2
+\lambda_v\|v_k\odot g_{v_k}\|_2
$$
减少维度很大时“求和放大噪声”的问题。

### Idea 3：冗余层选择不只看 $\cos(H_i,H_{i+1})$，加上“下游敏感度校正”

可以用少量校准样本估计该层的 block influence（例如对该层输出加噪/置零观察 loss 变化）来二次过滤，避免“表征相似但对某些任务关键”的层被误判为冗余。

### Idea 4：对不同子模块采用不同 retain ratio（attention vs MLP）

论文给出 10% 是个实践拐点，但你可以按模块自适应：

- attention 投影可能更“低秩可替代”
- MLP 某些层可能需要更高 retain 才稳
   用一维/二维预算分配（例如固定总预算，按子模块分配 retain ratio）。

### Idea 5：把“从后往前处理冗余层”与“累计误差”显式耦合

GRASP 逆序处理冗余层是为了减少层间干扰。你可以进一步做：

- 每替换一层后，更新一次后续层的梯度归因（更贴近当前模型状态）
- 或者用分段迭代（block-wise）在成本与准确间折中