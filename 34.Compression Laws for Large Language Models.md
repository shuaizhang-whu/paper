# Compression Laws for Large Language Models

提出一套“压缩标度律（compression laws）”，用少量可拟合参数把**压缩率 $r$**、**基座模型性能 $L_0$**、**恢复微调数据量 $D$** 与压缩后性能 $L$ 的关系统一刻画，并推导出一个很实用的概念：**critical compression ratio**（超过它，再多恢复微调也回不来）。Sengupta 等 - 2025 - Compression…

------

## 1）原始优化目标：如何在给定压缩率与恢复预算下预测/选择性能最优点

你可以把他们要解决的问题抽象成：给定一个 base 模型（性能 $L_0$），选择压缩率 $r\in(0,1)$ 与恢复微调 token 数 $D\in[0,\infty)$，使得压缩后模型性能 $L(L_0,r,D)$ 尽量好，同时满足算力/参数预算。

如果用“约束式”写，就是类似：
$$
\max_{r,D}\ L(L_0,r,D)\quad 
\text{s.t.}\quad \text{Cost}(r,D)\le B
$$
或给定“性能保留阈值” $\sigma$：
$$
\min_{r,D}\ \text{Cost}(r,D)
\quad \text{s.t.}\quad \frac{L(L_0,r,D)}{L_0^\alpha}\ge \sigma
$$
（这里 $\alpha$ 是他们拟合出来的一个“基座性能影响指数”，下面会出现。）

难点在于：不同模型、不同压缩方法、不同任务（intrinsic loss / extrinsic accuracy）表现差异大，没有统一规律，就很难做“选择决策”。

------

## 2）改进后的目标：提出一个满足可行性条件的“压缩律”函数形式

### 2.1 功能形式（核心公式）

他们提出压缩后性能满足一个幂律形式（文中 Eq.(1)）：
$$
L(L_0,r,D)=L_0^{\alpha}(1+r)^{\beta}\left(1+\frac{1}{D+\epsilon}\right)^{\gamma}
$$
其中：

- $L$：压缩后性能（可以是 test loss 或 zero-shot accuracy）
- $L_0$：未压缩 base 模型在同任务上的性能
- $r$：压缩率（例如 0.1,0.3,...）
- $D$：恢复微调（RFT）使用的数据量（token 数/样本规模）
- $\epsilon>0$：处理 $D=0$ 边界，文中常取 $\epsilon=1$
- $\alpha,\beta,\gamma$：需要拟合的实数参数

他们同时做了两个消融形式（文中 Eq.(2)(3)）：
$$
L(L_0,r)=L_0^{\alpha}(1+r)^{\beta}
$$

### 2.2 可行性条件（为什么这个形式“合理”）

他们要求这个函数满足三个“feasibility conditions”（第 3.1 节）：

1. **幂律/尺度不变性（homogeneous / scale invariant）**：压缩律对 $L_0$、$r$ 的变化具有可比较的尺度行为，便于推导决策边界。
2. **边界一致性**：当 $r\to 0$ 且 $D\to 0$ 时应恢复到 base 表现，即 $L\to L_0^\alpha$（注意不是必须等于 $L_0$，因为 $\alpha$允许“基座影响”是非线性的）。
3. **单调性**：对 accuracy 指标，压缩越大准确率越低、恢复数据越多准确率越高；对 loss 指标则相反。也就是：

- accuracy：$\frac{\partial L}{\partial r}\le 0,\ \frac{\partial L}{\partial D}\ge 0\Rightarrow \beta<0,\gamma<0$
- loss：$\frac{\partial L}{\partial r}\ge 0,\ \frac{\partial L}{\partial D}\le 0\Rightarrow \beta>0,\gamma>0$

这一步其实就是把“经验规律”固化成符号约束，让拟合结果更可解释。

------

## 3）从目标公式到可拟合形式：对数变换 → 线性回归（OLS）

核心技巧：对上式取对数，把幂律拟合转成线性回归（文中 Eq.(4)(5)）：
$$
\log L=\alpha\log L_0+\beta\log(1+r)+\gamma\log\left(1+\frac{1}{D+1}\right)
$$
把
$$
r'=(1+r),\quad D'=\left(1+\frac{1}{D+1}\right)
$$
则回归形式为：
$$
\log L=\alpha\log L_0+\beta\log r'+\gamma\log D'+\epsilon_{\text{noise}}
$$
其中噪声按 OLS 假设为高斯（文中 Eq.(5)）。

这就给出“step 实现”的数学主干：你只要收集很多组 $(L,L_0,r,D)$ 数据点，就能用线性回归得到 $\alpha,\beta,\gamma$。

------

## 4）关键理论推导：critical compression ratio（超过就不可恢复）

他们在第 3.3 节给出定理（Theorem 3.1）与推论（Corollary 3.2）：在 accuracy 场景（$\beta,\gamma<0$）下，要求压缩模型恢复到 base 的某个比例阈值 $\sigma\in(0,1)$：
$$
\frac{L}{L_0^\alpha}\ge \sigma
$$
代入压缩律：
$$
(1+r)^{\beta}\left(1+\frac{1}{D+1}\right)^{\gamma}\ge \sigma
$$
整理（文中推导在附录 A.1）：
$$
\left(1+\frac{1}{D+1}\right)^{\gamma}\ge \sigma(1+r)^{-\beta}
$$
因为 $\gamma<0$，两边取 $1/\gamma$ 次方会翻转单调（这是证明里用到的关键点），得到对 $D$ 的下界条件（文中 Eq.(6)）：
$$
\frac{1}{D+1}\le \Big(\sigma(1+r)^{-\beta}\Big)^{1/\gamma}-1
$$
这个式子直接告诉你：在给定 $r$ 下，要达到阈值 $\sigma$，RFT 数据量至少要多大。

进一步，他们定义 **critical compression ratio**（文中 Corollary 3.2）：
$$
r_{\text{critical}}(\sigma)=\sigma^{1/\beta}-1
$$
并证明：

- 若 $\sigma$ 较低（文中写成 $\sigma\in(0,2^{\beta})$，注意 $\beta<0$ 时这是一个 $<1$ 的阈值），则任意 $r\in(0,1)$ 都存在足够大的 $D$ 使得可恢复到 $\sigma$。
- 若 $\sigma$ 较高（$\sigma\in[2^{\beta},1)$），则当 $r\ge r_{\text{critical}}(\sigma)$ 时，无论 $D$ 多大都无法恢复到阈值 $\sigma$。

直觉：压缩引入的“不可逆结构损失”在高压缩时超过了恢复微调能修补的上限。

他们也给了一个具体数值例子：对某个 LLaMA-3-8B 的 extrinsic law，$\alpha=0.98,\beta=-1.18,\gamma=-0.14$，若希望 $\sigma=0.8$，则 $r_{\text{critical}}\approx0.208$（超过约 20% 就无法恢复到 80%）。这类例子用于“决策建议”非常直接。Sengupta 等 - 2025 - Compression…

------

## 5）Step 实现过程：如何复现他们的压缩律拟合与 critical ratio 计算

我按工程流程写成可执行步骤（与具体剪枝/低秩方法无关，你换成 SVD-LLM/SAES-SVD 的数据也能拟合）：

### Step 0：定义性能指标与数据维度

- 选 intrinsic：test cross-entropy loss（越低越好）
- 选 extrinsic：多任务 zero-shot accuracy 平均（越高越好）
- 固定一个 base checkpoint，得到 $L_0$

### Step 1：采样压缩设置并生成数据点

对每个模型尺寸、每个压缩方法、每个压缩率 $r\in\{0.1,0.3,0.5,0.7,0.9\}$：

1. 压缩得到模型 $M_{r}$
2. 在 $D=0$（无恢复）下评测得 $L$
3. 对多个恢复数据量 $D\in\{1k,4k,25k\}$ 做恢复微调（他们用 LoRA rank=16，1 epoch），评测得 $L$
    这样你收集到大量 $(L_0,r,D,L)$ 样本。

### Step 2：对数变换构造回归数据

对每个样本，构造特征：
$$
x_1=\log L_0,\quad x_2=\log(1+r),\quad x_3=\log\left(1+\frac{1}{D+1}\right)
$$
标签：
$$
y=\log L
$$

### Step 3：OLS 拟合 $\alpha,\beta,\gamma$

做线性回归：
$$
y=\alpha x_1+\beta x_2+\gamma x_3+\epsilon
$$
得到 $\hat\alpha,\hat\beta,\hat\gamma$，并用 adjusted $R^2$、F-statistics 检验拟合优度（他们表 1 显示三变量形式显著优于消融）。Sengupta 等 - 2025 - Compression…

### Step 4：推导决策边界（critical compression ratio）

给定你想保留的性能比例 $\sigma$：

- 直接算：

$$
r_{\text{critical}}(\sigma)=\sigma^{1/\hat\beta}-1
$$

- 或者给定 $r$ 反推需要的最小 $D$：

$$
\frac{1}{D+1}\le \Big(\sigma(1+r)^{-\hat\beta}\Big)^{1/\hat\gamma}-1
$$

得到 $D_{\min}(r,\sigma)$。

### Step 5：额外的“速度律”（runtime scaling）

他们还拟合了推理时延 $S$ 与压缩率的幂律（第 4 节 + Figure 6 / Table 4）：
$$
S = S_0(1+r)^{\beta_S}
$$
其中 $\beta_S<0$ 表示压缩越大，runtime 越小。这个可以用同样 OLS 拟合（对数线性）。Sengupta 等 - 2025 - Compression…

------

## 6）这篇对 SVD-LLM v2 / SAES-SVD 的“理论关系”应该怎么用（不展开融合细节）

这篇对你写 SVD 系列新论文更像“方法论层面的上层规律”，能提供两类东西：

1. **把“rank/压缩率选择”从拍脑袋变成可计算决策**
    SVD-LLM v2 / SAES-SVD 都面临：每层 rank、总压缩率怎么选。你可以用同样的 law，把 $r$ 解释成“有效压缩率/有效秩削减比例”，用少量实验点拟合后，直接给出某个目标 $\sigma$ 下的 $r_{\text{critical}}$。这会让你的论文在“压缩计划/预算选择”上更系统。
2. **解释“intrinsic vs extrinsic”差异**
    他们发现 intrinsic loss 对压缩更敏感、但可通过恢复大幅改善；extrinsic accuracy 下降相对线性且恢复幅度小（表 1/2 体现 $\gamma$ 较小）。这能帮助你解释：为什么一些 SVD 方法对 ppl 很糟但下游还行，或者相反。
3. **把恢复微调当作“可控变量”写进理论**
    SAES-SVD 更偏“结构误差抑制”，SVD-LLM v2 更偏“截断感知”。这篇提供一个把恢复 token 数 $D$ 明确进入公式的框架，能把“是否需要 recovery、需要多少”形式化。

------

## 7）一些可延展的 idea（只给方向）

1. **把压缩律的 $r$ 从“全局比例”升级为“层级向量 $r_\ell$”**
    现在 law 是标量 $r$。你可以做层级分解：

$$
L \approx L_0^\alpha \prod_{\ell}(1+r_\ell)^{\beta_\ell}\left(1+\frac{1}{D+1}\right)^\gamma
$$

其中 $\beta_\ell$ 可由 SAES-SVD 的误差传播强度或 v2 的截断敏感度来估计，实现“可解释的层间 rank 分配”。

1. **把“critical compression ratio”变成 rank 选择的硬约束**
    对给定目标 $\sigma$，把全局/局部压缩率限定在 $r 的可恢复区域内，再在该区域内优化速度或参数量。
2. **把 law 与误差理论结合：用 SAES-SVD 的误差上界预测 $\beta$**
    这篇 $\beta$ 是拟合出来的。你可以尝试把 $\beta$ 与“截断残差能量”或“累计误差增长率”联系起来，给一个半理论半经验的 $\beta$ 估计，减少对大量实验的依赖。
3. **加入“方法类型”条件变量：calibration-free vs calibration-based vs SVD**
    他们比较了 calibration-free 与 calibration-based 的拟合系数差异。你可以把方法类型作为条件，形成 family-wise 的 $(\alpha,\beta,\gamma)$ 先验，用更少样本更快拟合。