# FLRC: Fine-grained Low-Rank Compressor for Efficient LLM Inference

## 1）问题动机：为什么“同一压缩率 + 静态解码 rank”会在生成任务崩掉

多数 SVD/低秩压缩方法在 **prefill（一次性前向）** 上看起来还行，但在 **decoding（多步自回归生成）** 上会累计误差：早期 token 的偏差会影响后续所有 token，从而在摘要、对话等多步生成任务上掉得很厉害。

FLRC 认为有两类关键矛盾：

1. **不同层/不同 projection 的可压缩性不同**，统一压缩率会“过度压缩敏感部件、浪费不敏感部件”。
2. **生成阶段早期 token 更关键**：如果一开始就用很激进的低秩，会在最关键位置引入大偏差；但后期 token 影响相对小，可以更激进地压缩以省算力。

因此 FLRC 提供两块创新：

- **FLRA（Fisher-based Layer-wise Rank Allocation）**：用梯度信息快速分配每层每个 projection 的 rank（很快，比搜索快很多）。
- **PLRD（Progressive Low-rank Decoding）**：生成时按 token 动态降低总体 rank budget（先保守、后激进），用“随时间递减 rank”缓解生成质量崩坏。

> 注意：论文实验里明确说它的 pipeline “遵循 SVD-LLM 的白化分解流程”，但刻意省略了 SVD-LLM 中可能涉及的权重更新步骤，以保证公平比较。Lu 等 - 2025 - FLRC Fine-grained…

------

## 2）理论：从“压缩导致的损失上升”到 FLRC 的 Fisher 重要性与 rank 分配公式

### 2.1 起点：低秩压缩真正想优化的目标（loss 视角）

对某层某个 projection 权重矩阵 $W$，低秩近似得到 $W_r$（秩 $r$），压缩带来的效果应当体现在任务损失 $\mathcal{L}$ 上：
$$
\min_{\{r_{l,p}\}} \ \mathbb{E}_{(x,y)\sim\mathcal{D}}\big[\mathcal{L}(W_{r};x,y)\big]
\quad \text{s.t.}\quad \sum_{l,p} r_{l,p}\le R_{\text{budget}}
$$
但这个形式不可直接解：你没法对每个 $r_{l,p}$都跑训练/验证去找最优。

### 2.2 核心近似：用二阶泰勒 + Fisher（或其对角近似）把“loss 上升”变成可计算的 proxy

常见做法：对压缩扰动 $\Delta W = W_r - W$ 做近似：
$$
\Delta\mathcal{L}\approx
\underbrace{\langle \nabla_W \mathcal{L}, \Delta W\rangle}_{一阶项}
+\frac{1}{2}\underbrace{\Delta W^\top H \Delta W}_{二阶项}
$$
在训练收敛附近，一阶项期望往往接近 0，于是常用二阶项衡量敏感度。把 Hessian 用 Fisher 信息矩阵 $F$ 近似（或用其对角近似）：
$$
\mathbb{E}[\Delta\mathcal{L}] \approx \frac{1}{2}\sum_i F_i (\Delta w_i)^2
$$
如果压缩相当于“对参数施加某种缩放/删除”，常见 zero-cost proxy（NAS/剪枝领域也常见）会用参数的“相对扰动”形式：
$$
(\Delta w_i)^2 \propto w_i^2
\quad\Rightarrow\quad
\mathbb{E}[\Delta\mathcal{L}] \propto \sum_i F_i w_i^2
$$
又因为 Fisher 对角常可用梯度平方近似：$F_i \approx \mathbb{E}[g_i^2]$，就得到：
$$
\text{importance} \ \propto \sum_i (g_i w_i)^2
$$
这就是 FLRC 用来衡量每个 projection 重要性的核心式子（它称为 fisher-based importance）：
$$
\alpha_{l,p}=\sum_i\big(G_{l,p}[i]\cdot W_{l,p}[i]\big)^2
$$
直觉：**梯度大**表示损失对该参数敏感；**权重大**表示该参数幅值大、扰动影响大；二者相乘再平方对应一种“二阶敏感度 proxy”。

> 论文还对比了只用 $\sum W^2$ 或只用 $\sum G^2$ 的替代指标，发现都不如 $\sum (GW)^2$。Lu 等 - 2025 - FLRC Fine-grained…

### 2.3 从“重要性”到“rank 分配”：预算约束下的比例分配

定义所有 projection 的总重要性：
$$
S=\sum_{l}\sum_{p\in P_l}\alpha_{l,p}
$$
给定总体 rank 预算 $R_{\text{budget}}$，FLRC 用一个非常快的闭式分配规则：
$$
r_{l,p} = \mathrm{round}\left(\frac{\alpha_{l,p}}{S}\cdot R_{\text{budget}}\right)
$$
这个形式可以被理解为一种“连续松弛 + 线性分配”的近似解：如果你把“压缩造成的期望损失上升”粗略看成随 rank 下降而增大，并且增大速率与 $\alpha_{l,p}$ 成正比，那么把 budget 按 $\alpha$ 比例分给各 projection 是合理的近似。

------

## 3）实现 Step：FLRC 整体流程（含 rank 分配 + 低秩分解 + 解码时动态降 rank）

### 3.1 Step A：校准集上计算梯度（一次 backward）

**输入**：模型 $M$，校准数据集 $\mathcal{D}$，需要压缩的 projection 集合（如 q/k/v/o，gate/up/down）。

1. 前向计算 loss：$\mathcal{L}(M;\mathcal{D})$
2. 反向传播得到每个 projection 权重的梯度 $G_{l,p}$

这一步只需要一次或少数 batch，因此比 ASVD 那类“反复搜索 rank 配置并跑 PPL”快很多。Lu 等 - 2025 - FLRC Fine-grained…

### 3.2 Step B：计算 fisher proxy 重要性并分配每个 projection 的 rank

对每个 projection：
$$
\alpha_{l,p}=\sum_i(G_{l,p}[i]\cdot W_{l,p}[i])^2
$$
求总和 $S$，按：
$$
r_{l,p} = \mathrm{round}\left(\frac{\alpha_{l,p}}{S}\cdot R_{\text{budget}}\right)
$$
得到全模型的 rank 配置 $\{r_{l,p}\}$。

### 3.3 Step C：按分配的 rank 做低秩分解（论文沿用 SVD-LLM 的白化分解）

FLRC 并不发明新的分解方式，它明确说先用 SVD-LLM 的“truncation-aware data whitening”对白化后再做 SVD 截断（这样奇异值与误差更对齐），然后再应用它的 rank allocation 与 progressive decoding。Lu 等 - 2025 - FLRC Fine-grained…

你可以把这一块写成通用形式（与 SVD-LLM v2 很接近）：

- 采样激活 $X$，构造某种白化矩阵 $S$（例如由 $XX^\top$ 的分解得到）

- 在白化空间对 $WS$ 做 SVD：
  $$
  WS = U\Sigma V^\top
  $$

- 截断到 rank $r_{l,p}$ 并乘回逆白化得到压缩权重 $W'$

具体白化构造取决于你用 v1/v2 的实现细节，但 FLRC 的关键创新不在这里，而在 rank 分配与生成时动态 rank。

### 3.4 Step D：Progressive Low-rank Decoding（生成阶段按 token 动态调整 rank）

生成第 $t$ 个 token 时，FLRC 不使用固定 $R_{\text{budget}}$，而是用一个**随 token 非增**的预算调度器：
$$
R_{\text{budget}}(t+1)\le R_{\text{budget}}(t)
$$
把它代入 rank 分配公式，得到 token-特定的 rank：
$$
r_{l,p}(t)=\mathrm{round}\left(\frac{\alpha_{l,p}}{S}\cdot R_{\text{budget}}(t)\right)
$$
实现层面为何可行：低秩分解后，本质是两个小矩阵相乘（例如 $A\in\mathbb{R}^{m\times r}$, $B\in\mathbb{R}^{r\times n}$），并且通道按奇异值排序，“前面的 rank 更重要”。因此在 decoding 时只需取前 $k$ 个通道即可：
$$
A_k = A[:,1\!:\!k],\quad B_k = B[1\!:\!k,:]
$$
当 $k$ 随 token 递减时，计算量与访存都会下降（论文强调在 offloading/内存带宽瓶颈下尤其有效）。Lu 等 - 2025 - FLRC Fine-grained…

------

## 4）从“优化目标角度”看，FLRC 相对基线的本质区别（给你写 related work/对比会很有用）

### 4.1 FLRC vs SVD-LLM v2：差别不在分解，而在“rank 的决策依据”和“生成时 rank 是否静态”

- **SVD-LLM v2** 更像：给定某个误差度量（多在激活空间），通过白化让截断达到理论最小误差，并进一步做更“理论化”的 ratio/rank 分配（你项目里 v2 是基于理论截断损失分配）。

- **FLRC** 的 rank 分配来自 **Fisher/梯度 proxy**：
  $$
  \alpha=\sum (GW)^2
  $$
  它不需要反复搜索评估，速度极快。

- 最关键：FLRC 引入 **token 维度的动态预算** $R_{\text{budget}}(t)$，使生成早期更保守、后期更激进，这是它专门针对 decoding 崩坏的设计。Lu 等 - 2025 - FLRC Fine-grained…

### 4.2 FLRC vs SAES-SVD：一个是“生成时动态 rank”，一个是“逐层闭环误差抑制”

- **SAES-SVD** 的核心是 teacher/student 误差项（如 $H, \Delta$）与 $\beta$ 的自适应求解，属于“闭环逐层误差控制”。
- **FLRC** 不引入 teacher，也不显式建模误差传播项，而是用 **梯度敏感度**做 rank 分配，并用 **随 token 递减 rank**降低生成误差的影响。

从论文写作角度，你可以把它们归为两种互补方向：

- SAES：**层维度（layerwise）闭环纠错**
- FLRC：**时间维度（tokenwise）动态保真**

------

## 5）一些可写新论文的 idea（只给方向与公式骨架，不展开结合细节）

下面这些点，都能自然地融进你“SAES-SVD + SVD-LLM v2 的新论文主线”，而且每个都有明确公式落点。

### Idea 1：把 v2 的“误差最小截断损失”分配，与 FLRC 的 fisher 分配做统一的多目标分配

你可以把每个 projection 的“重要性”定义成两项的加权和：

- v2 风格误差 proxy（例如白化后截断剩余能量）：
  $$
  e_{l,p}(r)=\sum_{i>r}\sigma_i^2(D_{l,p})
  $$

- fisher proxy：
  $$
  \alpha_{l,p}=\sum_i(GW)^2
  $$

统一成一个分配目标：
$$
\min_{\{r_{l,p}\}} \sum_{l,p}\Big(\lambda\cdot e_{l,p}(r_{l,p}) + (1-\lambda)\cdot \frac{1}{r_{l,p}}\alpha_{l,p}\Big)
\quad \text{s.t. } \sum_{l,p} r_{l,p}\le R_{\text{budget}}
$$
这里 $\frac{1}{r}$ 只是一个示例形式，用来表达“rank 越小越危险”，你也可以换成别的单调函数。

### Idea 2：把 SAES 的 teacher 误差抑制扩展到 token 维度：$\beta(t)$ 与 $R_{\text{budget}}(t)$ 联动

SAES 在层维度用 $\beta$ 控制误差项；FLRC 在 token 维度控制 rank。你可以提出一个更一般的“时间调度”：
$$
R_{\text{budget}}(t)\downarrow,\qquad \beta(t)\downarrow \ \text{或}\ \uparrow
$$
并用一个统一的“生成期累计误差”目标来解释：
$$
\min_{\{R(t),\beta(t)\}} \sum_{t=1}^T w_t \cdot \|(W-W_t')\widetilde{X}_t(\beta(t))\|_F^2
$$
其中 $w_t$ 体现早期 token 更重要（例如 $w_t$ 递减），$\widetilde{X}_t$ 可带 teacher 引导项（你之前想做的新主线）。

### Idea 3：把 FLRC 的梯度重要性从“权重级”提升到“rank 级”，直接为每个奇异方向分配保留概率

现在 FLRC 是给 projection 一个整体 rank。你可以把“梯度信息”投影到奇异向量方向上，得到每个奇异方向的重要性：

令 $WS = U\Sigma V^\top$（白化后），对第 $i$ 个奇异方向定义：
$$
\gamma_i = \| U_i^\top \nabla_{WS}\mathcal{L}\|_2^2
$$
然后选择保留集合 $\mathcal{K}$（大小为 $r$）：
$$
\max_{\mathcal{K},|\mathcal{K}|=r}\ \sum_{i\in\mathcal{K}} \gamma_i
$$
这会把“projection 级 rank 分配”升级成“奇异方向级的更细粒度选择”。

### Idea 4：把 progressive decoding 的调度器从“用校准集枚举”变成“在线信号驱动”

FLRC 的 schedule 需要校准集评估候选。你可以提出一个在线可计算的信号，例如 token 时刻的置信度/熵：
$$
H_t = -\sum_{v} p(v|x_{\le t})\log p(v|x_{\le t})
$$
用它控制 rank：
$$
R_{\text{budget}}(t)=R_{\max}\cdot \phi(H_t)
$$
其中 $\phi$ 是单调函数：不确定性高（熵大）→ 给更多 rank；不确定性低 → 给更少 rank。