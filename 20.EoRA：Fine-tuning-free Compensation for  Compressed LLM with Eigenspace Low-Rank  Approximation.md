# EoRA: Fine-tuning-free Compensation for  Compressed LLM with Eigenspace Low-Rank  Approximation

它要解决的问题不是“再压缩一次模型”，而是：**在一个已经被量化/剪枝等压缩后的 LLM 上，不改动压缩权重本体，仅外挂一个低秩残差支路来补偿压缩误差，并且不做梯度微调（fine-tuning-free）**。Liu 等 - 2025 - EoRA Fine-tuning…

------

## 1）问题设定与基线：为什么“直接对误差做 SVD”不够

设某线性层原权重 $W\in\mathbb{R}^{d\times k}$，压缩后权重 $\hat W\in\mathbb{R}^{d\times k}$。压缩误差定义为
$$
\Delta W = W-\hat W.
$$

### 1.1 压缩（或补偿）真正关心的目标：层输出误差（task-specific）

给定校准数据（来自目标任务），该层输入激活矩阵为
$$
X\in\mathbb{R}^{k\times n}.
$$
层级压缩误差（也可看作“任务特定压缩损失”）自然写成：
$$
\min_{\text{(补偿参数)}}\ \|WX-(\hat W+\text{comp})X\|_F.
$$
若补偿用低秩残差 $\text{comp}=BA$（$B\in\mathbb{R}^{d\times r}, A\in\mathbb{R}^{r\times k}$），则目标变成
$$
\min_{B,A}\ \|WX-(\hat W+BA)X\|_F
= \min_{B,A}\ \|\Delta W\,X - BA\,X\|_F.
\tag{1}
$$
这一步非常关键：**补偿的“好坏”取决于它在校准任务的输入分布 $X$ 上能否把输出误差抹平**。

### 1.2 朴素做法：直接对 $\Delta W$ 做截断 SVD（training-free）

常见的训练自由补偿会写成
$$
\min_{B,A}\ \|\Delta W-BA\|_F,
\tag{2}
$$
然后对 $\Delta W$ 做 rank-$r$ 截断 SVD 得到 $BA$。

问题在于：式 (2) **忽略了 $X$**。它优化的是“权重误差”，并不等价于优化式 (1) 的“输出误差”。因此它经常“看起来补了很多误差”，但对目标任务精度恢复不理想。

------

## 2）EoRA 的核心：把“输出误差最小化”化成一个可用 SVD 闭式解的问题

EoRA 的关键贡献是：把式 (1) 通过一套**特征空间（eigenspace）投影**，变形成一个标准 Frobenius 低秩逼近，从而依然能用 **SVD 的闭式最优性**（Eckart–Young）解决，但又不会丢掉校准数据 $X$。

下面把推导写细。

------

## 3）从原目标到改进目标：详细推导（你要求的“改进后的公式推导”）

从式 (1) 出发：
$$
\min_{B,A}\ \|\Delta W X - BA X\|_F.
\tag{1}
$$

### 3.1 把 Frobenius 范数改写成 trace 形式（把 $X$ 显式拉出来）

利用恒等式 $\|M\|_F^2=\mathrm{tr}(MM^\top)$，先平方（不影响最优解）：
$$
\min_{B,A}\ \|\Delta W X - BA X\|_F^2
= \min_{B,A}\ \mathrm{tr}\Big((\Delta W X - BA X)(\Delta W X - BA X)^\top\Big).
$$
把 $X$ 提到中间：
$$
(\Delta W X - BA X)(\Delta W X - BA X)^\top
= (\Delta W - BA)\,X X^\top\,(\Delta W - BA)^\top.
$$
因此
$$
\min_{B,A}\ \mathrm{tr}\Big((\Delta W - BA)\,XX^\top\,(\Delta W - BA)^\top\Big).
\tag{3}
$$
式 (3) 很直观：这是一个**带权（由 $XX^\top$ 决定）**的矩阵逼近问题。直接对 $\Delta W$ 做 SVD 只能解无权版本（$XX^\top=I$ 时才一致）。

------

### 3.2 对 $XX^\top$ 做特征分解：把“带权”变成“白化后的普通 Frobenius”

注意 $XX^\top\in\mathbb{R}^{k\times k}$ 必然是对称半正定矩阵（即使 $X$ 很秩亏也成立）。做 eigendecomposition：
$$
XX^\top = Q\Lambda Q^\top,
$$
其中 $Q$ 正交，$\Lambda$ 对角且 $\Lambda\succeq 0$。

代回式 (3)：
$$
\mathrm{tr}\Big((\Delta W - BA)\,Q\Lambda Q^\top\,(\Delta W - BA)^\top\Big).
$$
利用 trace 循环不变性，把 $Q$ 移到 $\Delta W-BA$ 旁边：
$$
= \mathrm{tr}\Big((\Delta W Q - BA Q)\,\Lambda\,(\Delta W Q - BA Q)^\top\Big).
\tag{4}
$$

------

### 3.3 把 $\Lambda$ 写成 $\sqrt{\Lambda}\sqrt{\Lambda}$：得到“普通 Frobenius”的形式

因为 $\Lambda$ 对角且非负，所以存在 $\sqrt{\Lambda}$ 满足 $\Lambda=\sqrt{\Lambda}\sqrt{\Lambda}$，且 $\sqrt{\Lambda}=\sqrt{\Lambda}^\top$。

于是式 (4) 变为：
$$
\mathrm{tr}\Big((\Delta W Q - BA Q)\,\sqrt{\Lambda}\sqrt{\Lambda}\,(\Delta W Q - BA Q)^\top\Big).
$$
将 $\sqrt{\Lambda}$ 与左侧合并：
$$
= \mathrm{tr}\Big((\Delta W Q\sqrt{\Lambda} - BA Q\sqrt{\Lambda})
(\Delta W Q\sqrt{\Lambda} - BA Q\sqrt{\Lambda})^\top\Big).
$$
再用 $\mathrm{tr}(ZZ^\top)=\|Z\|_F^2$，得到：
$$
= \|\Delta W Q\sqrt{\Lambda} - BA Q\sqrt{\Lambda}\|_F^2.
\tag{5}
$$
这一步完成了关键变换：**原来的“输出误差最小化”被严格等价地写成了“在一个变换后的空间里做普通 Frobenius 低秩逼近”**。

------

### 3.4 定义投影矩阵 $Q' = Q\sqrt{\Lambda}$，把目标写成标准 SVD 可解形式

令
$$
Q' = Q\sqrt{\Lambda}\quad(\in\mathbb{R}^{k\times k}),
$$
并定义“投影后的误差矩阵”：
$$
\Delta W' = \Delta W Q'.
$$
则式 (5) 变为：
$$
\min_{B,A}\ \|\Delta W' - BA Q'\|_F^2.
\tag{6}
$$
这仍然不是标准形式，因为右边是 $BAQ'$ 而不是 $B'A'$。EoRA 的做法是把它重新参数化：

令
$$
B'A' = BAQ'.
$$
在 rank 约束下（rank 不超过 $r$），我们可以直接把“要逼近的目标矩阵”视为 $\Delta W'$，并求它的 rank-$r$ 最优逼近：
$$
\min_{B',A'}\ \|\Delta W' - B'A'\|_F^2.
\tag{7}
$$
根据 Eckart–Young 定理，式 (7) 的最优解就是对 $\Delta W'$ 做截断 SVD。

这就是 EoRA 的改进目标：**不是对 $\Delta W$ 做 SVD，而是对 $\Delta WQ\sqrt{\Lambda}$ 做 SVD**。它保证逼近误差与任务校准激活 $X$ 的统计对齐。

------

### 3.5 投影回原空间：为什么不增加推理时延

我们有
$$
\Delta W' \approx B'A'.
$$
但我们最终需要的是原空间的误差补偿 $\Delta W \approx BA$。由 $\Delta W'=\Delta WQ'$，可右乘 $Q'^{-1}$ 得：
$$
\Delta W \approx B'A'Q'^{-1}.
$$
于是 EoRA 最终把补偿写成：
$$
\text{comp}(x)=B' A x,\quad A = A'Q'^{-1}.
$$
也就是说：**把 $Q'^{-1}$ 吸收到 $A$ 里**，最终推理时仍然只是两次矩阵乘（低秩 $B'$ 与 $A$），不会额外多一层变换。

关于 $Q'^{-1}$：
 因为 $Q$ 正交，且 $\Lambda$ 对角（一般会对极小特征值做数值稳定处理），所以
$$
Q'^{-1} = \sqrt{\Lambda}^{-1}Q^\top.
$$

------

## 4）EoRA 的方法总结：它到底“对齐”了什么

对比朴素 SVD（式 (2)）优化的是 $\|\Delta W-BA\|_F$，EoRA 等价优化的是带权误差（式 (3)）：
$$
\mathrm{tr}\Big((\Delta W-BA)\,XX^\top\,(\Delta W-BA)^\top\Big),
$$
其中 $XX^\top$ 来自任务校准数据。
 更直观一点：如果把 $XX^\top$ 看作输入通道的重要性度量，那么 EoRA 在投影空间里会让“对应大特征值的方向”被更精细地逼近（因为它们在 $\sqrt{\Lambda}$ 缩放后被放大了），从而更能恢复任务表现。

------

## 5）Step 实现过程（严格按工程流水线）

对每个线性层（EoRA 是逐层独立、完全无反传）：

1. **计算压缩误差**

$$
\Delta W = W - \hat W.
$$

1. **收集任务校准激活并取均值（或统计）**
    得到该层校准输入激活矩阵 $X$（论文实现用校准集平均激活 $\tilde X$ 来估计统计）。
2. **构造 Gram 矩阵并做特征分解**

$$
XX^\top = Q\Lambda Q^\top.
$$

1. **构造投影矩阵**

$$
Q' = Q\sqrt{\Lambda}.
$$

1. **把误差投影到 eigenspace**

$$
\Delta W' = \Delta W Q'.
$$

1. **对 $\Delta W'$ 做 rank-$r$ 截断 SVD**

$$
\Delta W' \approx U'\Sigma'V'^\top.
$$

令
$$
B' = U'\Sigma',\quad A' = V'^\top.
$$

1. **投影回原空间（吸收到 $A$ 里）**

$$
A = A'Q'^{-1} = V'^\top \sqrt{\Lambda}^{-1}Q^\top.
$$

1. **推理时补偿（不改压缩权重本体）**
    对输入 $x$，层输出从 $\hat W x$ 变为

$$
\hat W x + B' A x.
$$

这样你可以在同一个压缩 backbone 上，按任务动态加载不同的 $B',A$（多适配器）。

------

## 6）EoRA 和常见“激活缩放 + SVD”补偿的区别（关键点）

很多 baseline 也会做 “$\Delta W S$” 的缩放再 SVD（$S$ 取对角、由通道均值/方差构造）。EoRA 的差别在于：

- 它用的是 **完整的 $XX^\top$ 的特征空间**，不仅是对角缩放；因此捕捉到输入通道间的相关性结构（eigenvectors），不是只看每个通道的幅度。
- 它给出了严格等价推导：对 $\Delta WQ\sqrt{\Lambda}$ 做 SVD，等价于最小化任务输出误差的 trace 目标。

------

## 7）一些可以继续往前推的 idea（不展开融合细节，只给方向与可落的公式）

### Idea 1：用“截断/稳定逆”的理论化处理，让 $Q'^{-1}$ 在秩亏场景更稳

当 $XX^\top$ 秩亏或特征值极小，$\sqrt{\Lambda}^{-1}$ 数值不稳。可用截断逆（只对前 $t$ 个特征值求逆）：
$$
Q'^{-1}\approx \sqrt{\Lambda_t}^{-1}Q_t^\top,
$$
并把误差目标近似成只在 top-eigenspace 上最小化（相当于只关心任务输入最常出现的子空间）。

### Idea 2：把“补偿 rank”按特征值能量自适应分配（层级动态 rank）

对每层的 $\Lambda$（或其能量覆盖率），设定 rank 与谱能量相关：
$$
r_\ell = \min\left(r_{\max},\ \arg\min_{r}\ \frac{\sum_{i>r}\lambda_i}{\sum_i\lambda_i}\le \epsilon\right).
$$
这样对“任务输入子空间更复杂”的层给更大补偿 rank，更节省总预算。

### Idea 3：把补偿目标从 hidden-state 扩展到 logits 的 KL（更贴近生成质量）

目前是层级输出误差。可以把最终目标换成校准集上的 logits KL，并把它局部近似回每层的加权目标，从而得到一个“logit-aware 的 $XX^\top$”（例如用反传得到的近似 Fisher/梯度协方差）：
$$
G \approx \mathbb{E}\left[\nabla_h \ell \ \nabla_h \ell^\top\right],
\quad
\min \mathrm{tr}\big((\Delta W-BA)\,XGX^\top\,(\Delta W-BA)^\top\big).
$$

### Idea 4：做“残差分段补偿”（类似多阶段残差 SVD，但在 EoRA 的投影空间）

先在 eigenspace 里做一次 rank-$r_1$ 逼近，再对残差做 rank-$r_2$：
$$
\Delta W' \approx B_1'A_1' + B_2'A_2',\quad r_1+r_2=r,
$$
再统一投影回原空间。这在高误差（如 3-bit/强剪枝）场景可能更稳。

### Idea 5：把多任务校准做成“可组合的 eigenspace”（多适配器共享统计）

对多个任务 $t$，各自有 $XX^\top\approx Q_t\Lambda_tQ_t^\top$。可以构造混合统计：
$$
C=\sum_t \alpha_t X_tX_t^\top,
\quad C=Q\Lambda Q^\top,
$$
从而得到“通用补偿 eigenspace”，再针对任务只存很小的差异化低秩参数。

