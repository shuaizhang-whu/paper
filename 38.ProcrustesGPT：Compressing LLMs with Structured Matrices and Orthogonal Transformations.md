# ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations

这篇的“主线”非常清晰：**直接把预训练权重投影到某个结构矩阵类 $S$（Kronecker/GS/块稀疏等）通常误差很大**；但 Transformer 存在一类“计算不变性（computational invariance）”：在层与层之间插入合适的**正交变换**并同步调整相邻权重，模型输出保持不变。于是他们把问题改成：**先找最有利于结构化压缩的旋转 $Q$，再把旋转后的权重投影到结构类 $S$**，并且全程 training-free。Grishina 等 - 2025 - ProcrustesG…

------

## 1）原始优化目标：把权重直接压到结构矩阵类 $S$

设某个线性层权重 $W\in\mathbb{R}^{d_{\text{out}}\times d_{\text{in}}}$，结构化压缩的最朴素目标是做“投影”：
$$
\min_{W'\in S}\ \|W-W'\|_F^2
$$
其中 $S$ 可以是：

- Kronecker 和的类：$\sum_{i=1}^r A_i\otimes B_i$
- GS-matrices：$P_L\big(L P R\big)P_R$（块对角 + 洗牌/置换）
- 块稀疏（例如只保留某些块/若干列为 0）

**问题**：预训练时并没有任何结构约束，直接投影到 $S$ 往往误差很大，且 training-free 情况下性能损失不可接受。

------

## 2）关键洞察：Transformer 对某些正交“旋转”不变（Rotational invariance）

他们把每个子块（MHSA/FFN）抽象成“输入线性映射 + 非线性/注意力算子 + 输出线性映射”。用统一记号写：
$$
\text{Block}(X)=\sigma(XW_{\text{in}})\,W_{\text{out}} + b
$$
其中 $\sigma$ 是 attention 或逐元素激活。

令 $Q\in\mathbb{R}^{d\times d}$ 为正交矩阵（$Q^\top Q=I$）。核心是：**如果你把 $Q$ 插到残差支路与主支路之间，并把相邻权重同步右乘/左乘 $Q$ 或 $Q^\top$，则 RMSNorm/LayerNorm 之后的归一化会“抹平”这种旋转，最终输出保持一致。**

文中给了一个关键等式（把 skip connection 与输出线性层一起看），形式上是：
$$
\Big(\frac{XW_{\text{out}}+X_{\text{skip}}}{\|XW_{\text{out}}+X_{\text{skip}}\|_F}\Big)\,W_{\text{in}}
=
\Big(\frac{XW_{\text{out}}Q+X_{\text{skip}}Q}{\|XW_{\text{out}}Q+X_{\text{skip}}Q\|_F}\Big)\,(Q^\top W_{\text{in}})
$$
直觉：

- 右边把主支路 $W_{\text{out}}$ 右乘 $Q$，同时把 skip 也乘 $Q$；
- 然后在下一层把 $W_{\text{in}}$ 替换成 $Q^\top W_{\text{in}}$；
- 归一化让尺度一致，整体函数不变。

因此：**模型允许你在层与层之间引入一组正交矩阵 $\{Q_\ell\}$，在不改变输出的情况下改变“权重的表示基底”。**

这就给了一个新自由度：

> 既然输出不变，那就选一个“让权重在结构类 $S$ 下更可压缩”的基底。

------

## 3）改进后的优化目标：选旋转 $Q_\ell$ 使结构化投影误差最小（按层独立）

### 3.1 他们最终的层级目标（核心式子）

对第 $\ell$ 个 Transformer 层（或块），设：

- $W_{\text{out}}^\ell$：块的输出线性映射权重（右乘旋转）
- $W_{\text{in}}^\ell$：块的输入线性映射权重（左乘旋转）
- $X_{\text{out}}^\ell, X_{\text{in}}^\ell$：来自校准数据的“该线性层输入特征矩阵”（用于加权）
- $W_{c,\text{out}}^\ell\in S_{\text{out}}$，$W_{c,\text{in}}^\ell\in S_{\text{in}}$：结构化后的近似

他们写成一个带权的 Frobenius 目标（可以理解为“输出对齐”的二次近似）：
$$
\min_{\substack{Q_\ell^\top Q_\ell=I\\ W_{c,\alpha}^\ell\in S_\alpha}}
\ \ \|X_{\text{out}}^\ell(W_{\text{out}}^\ell Q_\ell - W_{c,\text{out}}^\ell)\|_F^2
+
\lambda_{\text{in}}\ \|X_{\text{in}}^\ell(W_{\text{in}}^\ell - Q_\ell W_{c,\text{in}}^\ell)\|_F^2
\tag{★}
$$
它的结构非常关键：

- 第一项：选择 $Q_\ell$ 后，$W_{\text{out}}^\ell Q_\ell$ 更容易被结构类 $S_{\text{out}}$ 逼近；
- 第二项：同时要求 $Q_\ell^\top W_{\text{in}}^\ell$（等价地写成 $W_{\text{in}}^\ell \approx Q_\ell W_{c,\text{in}}^\ell$）也能在 $S_{\text{in}}$ 下逼近；
- 用 $X_{\text{out}}^\ell, X_{\text{in}}^\ell$ 做“数据加权”，让误差更对齐到校准分布。

### 3.2 从更一般目标到（★）的推导思路

更严格地，旋转会在 skip connection 上引入额外正交矩阵 $Q_{\ell-1}^\top Q_\ell$。他们先写一个更一般的目标，把 skip 也允许用结构矩阵 $W_{c,\text{skip}}^\ell$ 逼近，然后为了让各层可并行、目标可分解，近似地把：
$$
W_{c,\text{skip}}^\ell \leftarrow Q_{\ell-1}^\top Q_\ell
$$
这样就去掉了层间耦合项，得到（★）这种“每层独立可解”的形式。

另外他们发现用下面的系数平衡两项更好（而不是 $\lambda_{\text{in}}=1$）：
$$
\lambda_{\text{in}}=\frac{\|X_{\text{out}}W_{\text{out}}\|_F^2}{\|X_{\text{in}}W_{\text{in}}\|_F^2}
$$

------

## 4）如何求解（★）：交替最小化 = “投影步 + Procrustes 步”

（★）是非凸的（$Q$ 正交约束 + 结构投影），他们采用 ALS（alternating least squares）：

### 4.1 固定 $Q_\ell$：结构化投影（projection step）

- 输出侧：

$$
W_{c,\text{out}}^\ell=\arg\min_{W\in S_{\text{out}}}\ \|X_{\text{out}}^\ell(W_{\text{out}}^\ell Q_\ell - W)\|_F^2
$$

- 输入侧：

$$
W_{c,\text{in}}^\ell=\arg\min_{W\in S_{\text{in}}}\ \|X_{\text{in}}^\ell(W_{\text{in}}^\ell - Q_\ell W)\|_F^2
$$

这个“带权结构逼近”对不同 $S$ 的实现方式不同（下面第 5 节展开 Kronecker/GS）。

### 4.2 固定结构权重：求最优正交 $Q_\ell$（Procrustes step）

如果是无权的正交 Procrustes（OPP）：
$$
\min_{Q^\top Q=I}\ \|QA-B\|_F
$$
解析解：对 $BA^\top=U\Sigma V^\top$，取
$$
Q^*=UV^\top
$$
但（★）里是带权的“weighted orthogonal Procrustes problem（WOPP）”：
$$
\min_{Q^\top Q=I}
\ \|CQA-B\|_F
$$
一般没有闭式解。他们用 **Cayley transform** 参数化正交矩阵：
$$
Q=(I+K)(I-K)^{-1},\quad K^\top=-K
$$
然后用共轭梯度（conjugate gradients）在 $K$ 的参数空间里做数值优化。

------

## 5）结构矩阵类 $S$ 的投影细节：Kronecker 和 / GS matrices

### 5.1 Kronecker 和：$\sum_{i=1}^r A_i\otimes B_i$

**无权投影（Frobenius）**：
$$
\min_{A_i,B_i}\ \left\|W-\sum_{i=1}^r A_i\otimes B_i\right\|_F^2
$$
它有一个经典 SVD 解：把 $W$ 通过 reshape/rearrange 变成一个矩阵 $W_r$，对 $W_r$ 做 SVD，取前 $r$ 个分量再 reshape 回 $A_i,B_i$（论文给了伪代码 Algorithm 1）。

**带权投影（用 $X$ 加权）**：
$$
\min_{A_i,B_i}\ \left\|X\left(W-\sum_{i=1}^r A_i\otimes B_i\right)\right\|_F^2
$$
无闭式解，于是他们做交替优化：固定 $B$ 解 $A$，固定 $A$ 解 $B$。每个子问题都能化为线性最小二乘（论文附录给了把张量收缩写成矩阵方程 $CA=D$ 的推导，并给了 einsum 实现思路）。

### 5.2 GS matrices：$P_L(LPR)P_R$

GS 结构（Group-and-Shuffle）大意是：通过行列置换，把矩阵变成“由块对角低秩块”组合而成，因此乘法可更快、更省参数。

无权投影 $\min\|W-P_L(LPR)P_R\|_F^2$ 有高效 SVD 程序（引用了 GS 原论文的方法）；带权版本同样用交替最小二乘（固定 $L$ 解 $R$，固定 $R$ 解 $L$），每步都是分块 least squares。

------

## 6）两阶段求解策略：先无权初始化，再做带权精修

他们强调：直接解带权 ALS 很难、也不稳定，因此用“两阶段”：

### Stage A：Frobenius norm 下的 ALS 初始化（Algorithm 2）

初始化 $Q=I$，循环：

1. 结构投影（无权）：

$$
W_{c,\text{in}}=\arg\min_{W\in S_{\text{in}}}\ \|Q^\top W_{\text{in}}-W\|_F^2,\quad
W_{c,\text{out}}=\arg\min_{W\in S_{\text{out}}}\ \|W_{\text{out}}Q-W\|_F^2
$$

1. 把两侧拼成一个大的 OPP：

$$
\min_{Q^\top Q=I}\ \|[W_{\text{out}},W_{\text{in}}^\top]Q-[W_{c,\text{out}},W_{c,\text{in}}^\top]\|_F^2
$$

用 SVD 得到解析 $Q$。

### Stage B：把网络按 $Q_{\text{init}}$ 旋转后，在带权范数下做少量 ALS（Algorithm 3）

循环（通常只做很少轮）：

1. 带权结构投影：

$$
W_{c,\text{out}}=\arg\min_{W\in S_{\text{out}}}\ \|X_{\text{out}}(W_{\text{out}}Q-W)\|_F^2
$$

1. 用 Cayley + 共轭梯度解带权 Procrustes 得新 $Q$。

这个“先好初始化、再带权修正”的策略，是它能 training-free 稳定工作的关键。

------

## 7）工程关键技巧：用相关矩阵 $X^\top X$ 代替大激活矩阵 $X$

校准激活 $X\in\mathbb{R}^{(bs)\times d}$ 很大（batch×seq×hidden），但注意：
$$
\|X M\|_F = \|(X^\top X)^{1/2} M\|_F
$$
因此带权目标里只需要 $G=X^\top X\in\mathbb{R}^{d\times d}$。他们用分 batch 累加计算：
$$
X^\top X = \sum_i X_i^\top X_i
$$
**Embedding 特例**：embedding 输入是 one-hot，因此 $X^\top X$ 就是 token 频次对角矩阵 $D$，带权项变成：
$$
\|\sqrt{D}(W_{\text{emb}}Q-W_{c,\text{emb}})\|_F^2
$$
并且他们经验上用 $\sqrt{D+1}$（以及对 head 使用同样权重）效果更好。

------

## 8）与 SliceGPT 的关系：块稀疏结构 + 旋转 = slicing

他们指出：如果选一个“零列/零块”的结构类（非常简单的 block-sparse），并令 $\lambda_{\text{in}}=0$，那么优化会退化成：寻找一个旋转 $Q$ 使得 $W_{\text{out}}Q$ 的某些列（或 $Q_{\ell-1}^\top Q_\ell$ 的某些列）可被切掉——这与 SliceGPT 的“旋转 + 切行列”在本质上是一致的（论文给了一个命题说明等价关系）。

这点很重要：**ProcrustesGPT 是一个更一般的“旋转-再投影”框架，SliceGPT 是它在某个结构类选择下的特例。**

------

## 9）一些可延展的 idea（不展开成详细融合建议，只给方向）

1. **把“旋转提升可压缩性”迁移到 SVD 系列：先旋转再做 truncation-aware SVD**
    SVD-LLM v2/SAES-SVD 的难点是截断误差与传播。这里的 $Q$ 本质是在找一个“更适合被某类低参数结构逼近”的基底。你可以把结构类 $S$ 换成“低秩类”（或低秩+稀疏），直接优化：

$$
\min_{Q^\top Q=I,\,\mathrm{rank}(W_c)\le r}\ \|X(WQ-W_c)\|_F^2
$$

直觉上可能让奇异谱更“尖”（能量更集中），在同 rank 下丢失更少关键信息。

1. **把 SAES 的“累计误差抑制”做成旋转目标中的跨层耦合项**
    ProcrustesGPT 为了并行把 skip 相关项近似掉了。你可以保留一个弱耦合正则（例如惩罚 $Q_{\ell-1}^\top Q_\ell$ 的变化幅度）：

$$
\min \sum_\ell \mathcal{L}_\ell(Q_\ell,W_{c,\ell})\ +\ \eta\sum_\ell \|Q_{\ell-1}^\top Q_\ell-I\|_F^2
$$

用来抑制旋转在深度方向剧烈变化造成的传播不稳定。

1. **把结构类 $S$ 做成“混合结构”：Kronecker/GS + 低秩残差**
    即：

$$
W \approx W_{\text{struct}} + UV^\top
$$

其中 $W_{\text{struct}}\in S$（高效推理），$UV^\top$ 用较小 rank 补齐表达能力。旋转 $Q$ 同时服务于两部分，使 training-free 的上限更高。

1. **显式压缩/参数化 skip 中的正交矩阵 $Q_{\ell-1}^\top Q_\ell$**
    论文提到用 Cayley（存 $K$ 的上三角）减少正交矩阵存储，但仍可能是压缩瓶颈。可以进一步让 $Q_{\ell-1}^\top Q_\ell$ 也投影到某个结构正交类（例如 GS-orthogonal 或 Kronecker-orthogonal），把“旋转本身”也结构化。
2. **把“带权范数”从二阶统计升级为更任务对齐的权重（例如 Fisher）**
    他们用 $X^\top X$ 作为权重，等价于二阶输入相关性。如果换成近似 Fisher / 梯度相关性，旋转会更偏向“保持任务敏感方向可压缩”，可能更接近你目前在 SVD-LLM v2/SAES-SVD 里追求的“误差对任务影响最小”。