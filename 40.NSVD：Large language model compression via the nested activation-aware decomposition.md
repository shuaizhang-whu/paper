# NSVD：Large language model compression via the nested activation-aware decomposition

这篇的核心贡献是提出 **Nested Activation-aware Decomposition（NSVD / NID）**：用“**两段低秩**”而不是“一段低秩”来同时兼顾
 1）对校准激活分布的匹配（activation-aware），以及
 2）对原始权重矩阵本身的贴合（matrix-aware），
 从而缓解“用单一校准集做 activation-aware 压缩会对分布外任务/语言过拟合”的问题。Lu 等 - 2025 - Large language mo…

------

## 1）原始优化目标：固定秩低秩近似 + 激活感知低秩近似

### 1.1 固定秩（标准 SVD）目标

给定矩阵 $A\in\mathbb{R}^{m\times n}$（在 LLM 里通常是权重 $W$），标准 rank-$k$ 近似：
$$
\tilde A
=\arg\min_{\operatorname{rank}(B)=k}\ \|A-B\|_F
$$
Frobenius 范数可写成奇异值平方和：
$$
\|A\|_F=\sqrt{\mathrm{tr}(A^\top A)}=\sqrt{\sum_{i=1}^{r}\sigma_i^2}
$$
Eckart–Young–Mirsky 定理给出最优解是截断 SVD：
$$
A=\sum_{i=1}^{r}\sigma_i u_i v_i^\top,\quad
A_k=\sum_{i=1}^{k}\sigma_i u_i v_i^\top
$$
最小误差：
$$
\|A-A_k\|_F=\sqrt{\sum_{i=k+1}^{r}\sigma_i^2}
$$
**问题**：LLM 的输入激活 $X$ 方差很大，单纯最小化 $\|A-B\|_F$ 不对齐“输出空间误差”。

### 1.2 激活感知（activation-aware）目标

激活感知把误差放到输出上：
$$
\tilde A
=\arg\min_{\operatorname{rank}(B)=k}\ \|(A-B)X\|_F
\tag{AA}
$$
其中 $X\in\mathbb{R}^{n\times p}$ 是校准集采样得到的输入激活矩阵。

经典 ASVD-0 用一个对角缩放矩阵 $S$（例如每维激活均值幅值）做近似，把问题转成对 $AS$ 做 SVD：
$$
\tilde A_k=\arg\min_{\operatorname{rank}(B)=k}\ \|AS-B\|_F
$$
更进一步，SVD-LLM / ASVD-I 的关键思想是选取 $S$ 使得“奇异值截断与压缩损失直接对应”。令
$$
SS^\top = XX^\top
$$
若 $X$ 满秩，则对 $AS$ 做 SVD：
$$
AS=\sum_{i=1}^{r}\sigma_i u_i v_i^\top
$$
如果你把第 $j$ 个奇异分量删掉，得到 $AS$ 的“去掉第 $j$ 项”的近似，那么在 activation-aware 目标下的损失满足：
$$
\ell_j=\|(A-\tilde A)X\|_F = \sigma_j
$$
若截断到 rank-$k$，则
$$
\ell^2=\sum_{i=k+1}^{r}\sigma_i^2
$$
这就是他们所谓的 **truncation-aware data whitening**：通过对激活做“白化”构造 $S$，让“丢掉哪些奇异值”与“输出误差”一一对应。

------

## 2）这篇指出的核心痛点：activation-aware 会“对校准集激活分布过拟合”

activation-aware 的 $X$ 来自某个校准数据集（比如 WikiText-2 英文）。当你拿压缩模型去跑 **语言/任务明显不同**的数据（中文、日文、多任务指令），激活分布变化大，单纯最小化 $\|(A-B)X\|_F$ 容易出现：

- 在校准分布内：误差小、性能看起来好
- 分布外：性能断崖式下降（论文用“overfitting during post-training procedure”描述）

直觉上：你在 (AA) 里只让 $B$ 对某个 $X$ 的输出好，却没有约束 $B$ 对 $A$ 本身的“整体结构”保真；当 $X$ 变了，保留下来的方向可能不是“普适重要方向”。

------

## 3）改进后的目标：Nested Decomposition（NSVD）——把压缩拆成“激活项 + 原矩阵项”两段低秩

他们提出的“嵌套分解”把单个 rank-$k$ 的近似，改成 **两段低秩之和**，秩分别为 $k_1,k_2$，并强制
$$
k_1+k_2=k
$$
从而总体压缩预算不变。

### 3.1 Step (5a)：先做 activation-aware 低秩（解决激活方差/离群）

第一段：
$$
\tilde A^{(1)}
=\arg\min_{\operatorname{rank}(B)=k_1}\ \|(A-B)X\|_F
$$
并可用 ASVD-I/II 的 whitening 构造 $S$ 转成对 $AS$ 的截断 SVD（使得截断奇异值与损失对应）。

把它因子化为低秩乘：
$$
\tilde A^{(1)} = W_1 Z_1,\quad \operatorname{rank}(W_1)=\operatorname{rank}(Z_1)=k_1
$$

### 3.2 Step (5b)：再逼近“残差矩阵”以贴合原权重（抵消分布外偏差）

定义残差矩阵：
$$
R = A-\tilde A^{(1)}
$$
第二段不再看激活，而是直接让第二段去逼近这个残差（也就是让最终总和更接近原矩阵）：
$$
\tilde A^{(2)}
=\arg\min_{\operatorname{rank}(B)=k_2}\ \|B - R\|_F
$$
同样写成低秩因子：
$$
\tilde A^{(2)} = W_2 Z_2,\quad \operatorname{rank}(W_2)=\operatorname{rank}(Z_2)=k_2
$$

### 3.3 最终压缩权重与前向形式

最终近似是两段相加：
$$
\tilde A = \tilde A^{(1)}+\tilde A^{(2)} = W_1Z_1 + W_2Z_2
$$
对新输入激活 $X'$ 的输出：
$$
O=\tilde A X' = W_1(Z_1X') + W_2(Z_2X')
$$
复杂度方面，他们指出两段相加的 FLOPs 量级与单段 rank-$k$ 的 activation-aware 分解一致（本质上是把 rank 分成两份做两次低秩乘），因此不会把推理复杂度“翻倍到不可用”。

### 3.4 这套目标为什么能缓解分布外崩溃（从优化角度解释）

你可以把 NSVD 看成在“只对齐校准激活输出”之外又加了一个“原矩阵保真”的补偿项：

- Step (5a) 优先拟合 $(A-B)X$，对齐校准分布输出
- Step (5b) 再把 **剩余的权重结构** 用 $\|B-R\|_F$ 拉回来

因此最终 $\tilde A$ 同时满足：

- 对校准激活：靠 $\tilde A^{(1)}$ 保证
- 对未知激活：至少 $\tilde A$ 更接近 $A$（因为补了 $\tilde A^{(2)}$），减少 “只保留对校准集有效方向” 的偏置

------

## 4）NSVD 的几个理论细节：ASVD-I 与 ASVD-II 等价、以及“为什么用 SVD 版本更稳”

他们把 whitening 的构造分成两种：

- **ASVD-I（Cholesky）**：令 $SS^\top=XX^\top$ 为 Cholesky 分解
- **ASVD-II（SVD/谱分解）**：令 $XX^\top=P\Lambda P^\top$，取 $S=P\Lambda^{1/2}$

当 $X$ 满秩时，两者是等价的（可通过 $P\Lambda^{1/2}$ 的 LQ 分解与 Cholesky 唯一性证明）。但实践里 $XX^\top$ 可能半正定且有零特征值，Cholesky 需要额外处理；SVD 版本可用伪逆更自然，因此数值上更稳、更直观（它把“先旋转 $P^\top$，再按 $\Lambda^{1/2}$ 缩放”解释得很清楚）。

------

## 5）Step 实现过程（把论文方法写成可复现的工程流程）

下面按“单层单矩阵 $A$”描述，整网按层循环即可。

### Step 0：准备校准激活

用校准集跑前向，收集该线性层输入激活矩阵 $X\in\mathbb{R}^{n\times p}$。

### Step 1：构造 whitening 矩阵 $S$

计算
$$
G = XX^\top\in\mathbb{R}^{n\times n}
$$
做谱分解（或 SVD）：
$$
G=P\Lambda P^\top
$$
取
$$
S=P\Lambda^{1/2}
$$
（若有零特征值，用伪逆/截断处理）。

### Step 2：NSVD Step (5a) —— activation-aware 的 rank-$k_1$

对
$$
AS
$$
做 SVD：
$$
AS=U\Sigma V^\top
$$
截断到 $k_1$：
$$
(AS)_{k_1}=U_{1:k_1}\Sigma_{1:k_1}V_{1:k_1}^\top
$$
回到原空间：
$$
\tilde A^{(1)} = (AS)_{k_1}\,S^{-1}
$$
并因子化存储（吸收 $\Sigma$）：
$$
W_1=U_{1:k_1}\Sigma_{1:k_1},\quad Z_1=V_{1:k_1}^\top S^{-1}
$$
（或用 $\Sigma^{1/2}$ 平分到两侧也行）。

### Step 3：NSVD Step (5b) —— 对残差做 rank-$k_2$ 的矩阵近似

计算残差：
$$
R=A-\tilde A^{(1)}
$$
对 $R$ 做标准 SVD 并截断到 $k_2$：
$$
R\approx U_r\Sigma_r V_r^\top
$$
设：
$$
W_2=U_r\Sigma_r,\quad Z_2=V_r^\top
$$
得到第二段 $\tilde A^{(2)}=W_2Z_2$。

### Step 4：替换推理计算图

把原来一段 $AX'$ 替换成：
$$
\tilde A X' = W_1(Z_1X') + W_2(Z_2X')
$$
（工程实现里通常做两个低秩 GEMM 再相加）。

### Step 5：$k_1$ 的经验选择（论文实证结论可总结成规则）

他们固定总 $k$，扫 $k_1\in\{0.99k,0.95k,0.90k,0.85k,0.80k\}$ 发现：

- $k_1$ 太大：几乎退化回单一 activation-aware，分布外收益不足
- $k_1$ 太小：activation-aware 作用不足，分布内也开始受损
   一个较稳妥区间通常在 $0.90k\sim 0.95k$，而当预计任务/语言与校准集差异很大时，$k_1$ 进一步减小（比如 0.80k）可能更好。

------

## 6）NID：把第二段从 SVD 换成更经济的 ID（可选变体）

NSVD 的第二段 (5b) 也可不用 SVD，而用低秩 **Interpolative Decomposition (ID)**：
$$
R \approx C T
$$
其中 $C$ 由 $R$ 的部分列（或行）组成，$T$ 是插值系数矩阵。优点是更省计算/更省存储，但在分布差异极大的任务上，论文观察到 NID 不如 NSVD 稳（尤其中文/日文这类激活差异大场景）。

------

## 7）一些可延展的 idea（只给方向，不展开到详细融合方案）

1. **让 $k_1$ 变成“层自适应”而不是全局常数**
    论文已经展示 $k_1$ 对分布外很关键。你可以用“校准激活与目标激活的相似度”或“层输出变换强度”去决定每层 $k_{1,\ell}$（相似度低 → 减小 $k_{1,\ell}$，给矩阵保真项更多预算）。
2. **把嵌套从 2 段扩展到多段：多校准分布的 mixture**
    若你有多种校准集（英文、中文、代码、指令），可以做：

$$
\tilde A = \sum_{t=1}^{T} \tilde A^{(t)} + \tilde A^{(\text{matrix})}
$$

前 $T$ 段分别对齐不同激活分布，最后一段保真原矩阵；每段分配不同 $k_t$。

1. **把 Step(5b) 从“权重残差”换成“功能残差”的近似**
    现在 (5b) 最小化 $\|B-R\|_F$。如果把它改成对输出误差加权：

$$
\min_{\operatorname{rank}(B)=k_2}\ \|(R-B)X\|_F
$$

并且 $X$ 用“更鲁棒的激活统计”（例如多数据集混合、或去极值后的激活），可能让第二段更贴近“分布外可泛化的功能补偿”。

1. **引入“旋转/正交变换”改善可压缩性后再做嵌套**
    像 ProcrustesGPT 这种“先选基底再投影”的思想，可以作为 NSVD 的前置步骤：先找一个让 $AS$ 奇异谱更集中的旋转，再做 (5a)(5b)，可能提升同等 $k$ 下的上限。
2. **把嵌套结构与误差传播抑制结合（第三段做稳定项）**
    NSVD 的两段解决“校准对齐 + 矩阵保真”。如果你还担心层间误差累积，可以考虑再加一个很小预算的“稳定项”，专门压制误差传播敏感方向（例如只针对少数关键层/关键投影矩阵）。