# On Information Geometry and Iterative Optimization in Model Compression: Operator Factorization

这篇不是提出一个“新的 SVD-LLM 算法”，而是用**信息几何 + 迭代优化**把一堆压缩方法（SVD/FWSVD/ASVD/TRP/OIALR 等）统一成“在低算力子流形/集合上做投影”的问题，并进一步给出**带软秩约束的迭代奇异值阈值（SVT）**收敛性结论，以及基于此的更温和 rank cut 策略（IEHT/IFHT）。Shumaylov 等 - 2025 - On informa…

------

## 1）原始优化目标：训练大模型 + 投影到低算力子集

设监督学习的负对数似然（NLL）为（文中 Eq.(1)）：
$$
L(\theta)=-\sum_n\log p_\theta(y_n|x_n)
$$
其中 $\theta\in\mathcal{M}=\mathbb{R}^D$ 是全参数空间。压缩的“低算力集合”用 $\mathcal{M}_{ 表示（例如每层矩阵秩受限），文中强调“压缩算法必须回答两件事：rank selection + projection”（第 3 节）。

经典“训练后压缩”写成两阶段：
$$
\tilde\theta=\arg\min_{\theta\in\mathcal{M}}L(\theta),
\qquad
\theta_r=\operatorname{proj}_{\mathcal{M}_{<r}}(\tilde\theta),
\qquad
\tilde\theta_r=\arg\min_{\theta\in \mathcal{M}_{<r}}L(\theta)
$$
这里 $\operatorname{proj}$ 是一次性投影（train-then-sparsify），$\tilde\theta_r$ 是再微调结果。

更“理论正确”的总体压缩目标可写成双层（文中第 3 节）：
$$
\min_r \|r\|_1 \quad \text{s.t.}\quad \min_{\theta\in\mathcal{M}_{<r}} L(\theta)\le \varepsilon
$$
但 $r$ 是整数、$\mathcal{M}_{ 非凸，直接解不可行，于是实际方法都在做近似/松弛/迭代。

------

## 2）核心观点：压缩=把“投影距离”从欧氏距离换成信息距离（KL / Fisher 度量）

### 2.1 为什么 KL 投影更“对齐任务”

信息几何给出的两个关键等式（文中 Eq.(2)）：
$$
\mathrm{KL}(p_\theta\|p_{\theta+\Delta\theta})
=\frac{1}{2}\Delta\theta^\top I(\theta)\Delta\theta+O(\|\Delta\theta\|^3),
\quad
L(\theta)=\mathrm{KL}(p_{\mathrm{em}}\|p_\theta)+\text{const}
$$
其中 $I(\theta)$ 是 Fisher 信息矩阵（FIM）。

因此如果你希望压缩后保持任务分布不变，理想投影应是：
$$
\operatorname{proj}^{\mathrm{KL}}_{\mathcal{M}_{<r}}(\tilde\theta)
=\arg\min_{\theta\in\mathcal{M}_{<r}}\mathrm{KL}(p_{\tilde\theta}\|p_\theta)
$$
这比欧氏投影（只保 $\|\theta-\tilde\theta\|_2$）更贴近“保持预测分布”。

### 2.2 欧氏投影：SVD 是 $\ell_2$ 意义下的最优低秩投影

文中把传统 SVD 压缩明确表述为欧氏投影（Eq.(4)）：
$$
\operatorname{proj}^{\ell_2}_{\mathcal{M}_{<r}}(\tilde\theta)
=\arg\min_{\theta\in\mathcal{M}_{<r}}\|\theta-\tilde\theta\|_2^2
$$
对单层矩阵 $W$，这就是 Eckart–Young：截断 SVD。

### 2.3 信息投影：KL 投影≈FIM 加权二次型（FWSVD 等的理论归属）

文中将 KL 投影在 $\tilde\theta$ 处二阶展开，得到（Eq.(5)）：
$$
\operatorname{proj}^{\mathrm{KL}}_{\mathcal{M}_{<r}}(\tilde\theta)
=\arg\min_{\theta\in\mathcal{M}_{<r}}\mathrm{KL}(p_{\tilde\theta}\|p_\theta)
\approx
\arg\min_{\theta\in\mathcal{M}_{<r}}
\frac{1}{2}(\theta-\tilde\theta)^\top I(\tilde\theta)(\theta-\tilde\theta)
$$
但完整 FIM 太贵，所以出现一系列近似链条：

- TFWSVD：用对角 Fisher $I(\theta)\approx \mathrm{diag}(I)$，丢掉层内耦合；
- FWSVD：进一步把对角 Fisher做列/行聚合，使得权重更易算（但更粗）。

同理，ASVD 一类“激活匹配”的目标（文中 Eq.(7)）可视作对中间激活分布做高斯假设时的 KL 投影代理：
$$
\arg\min_{\theta\in\mathcal{M}_{<r}}\sum_i\|\theta_iX_{i-1}-\tilde\theta_iX_{i-1}\|_2^2
\equiv
\arg\min_{\theta\in\mathcal{M}_{<r}}\mathrm{KL}(p^N_\theta\|p^N_{\tilde\theta})
$$
（本质上：把每层输出当作高斯，匹配均值。）

> 这段统一视角对你写“新论文 related work / motivation”很有用：SVD-LLM v2 与 SAES-SVD 都可被解释为在“信息投影的强近似”下做更好的投影与误差控制。

------

## 3）从“单次投影”到“迭代压缩”：核心改进目标与推导（迭代硬阈值/近端法）

论文第二个重点是：**在极限压缩下，决定性能的往往不是投影用欧氏还是 Fisher，而是‘可训练性’与迭代压缩过程本身**（实验与第 5 节讨论）。

它把“训练中逐步降秩”形式化为一个带秩惩罚的优化问题（文中 Eq.(8)）：
$$
\min_{W\in\mathcal{M}}\; L(W)+\lambda\,\mathrm{rank}(W)
$$
这是非凸非连续（rank 不连续），但可以用**近端梯度**来做（核心思想：每一步先按损失走梯度，再对 rank 做近端/阈值）。

### 3.1 欧氏近端梯度：迭代奇异值硬阈值（SVHT / IHT 的本质）

经典近端梯度形式：
$$
W_{n+1}=\operatorname{prox}_{\alpha_n\lambda\,\mathrm{rank}}(W_n-\alpha_n\nabla L(W_n))
$$
而 $\operatorname{prox}_{\lambda\,\mathrm{rank}}$ 对矩阵对应“奇异值硬阈值/截断”（依赖 $\lambda$ 与步长），所以“训练时周期性做 SVD 截断”可以被解释为在解上述问题。

### 3.2 信息几何版本：把 $\ell_2$ 近端改成 Bregman（FIM）近端

论文更进一步，提出用一般的 Bregman divergence（文中 Eq.(3)）：
$$
D_\phi(\theta\|\theta')
=\phi(\theta)-\phi(\theta')-\nabla\phi(\theta')^\top(\theta-\theta')
$$
把更新写成（文中 Eq.(9)）的“镜像/广义近端梯度”：
$$
W_{n+1}\in
\arg\min_{W\in\mathcal{M}}
\Big\{
\frac{1}{\alpha_n}D_{F_n}(W,W_n)+\langle W,\nabla L(W_n)\rangle+\lambda\,\mathrm{rank}(W)
\Big\}
$$
其中 $F_n$ 可以取
$$
F_n(W)=\frac{1}{2}\|I_n^{1/2}W\|_F^2
$$
对应“用 Fisher 度量的二次型”作为距离，也就是把欧氏投影换成信息投影的局部近似。

------

## 4）关键理论结论：带软秩约束的迭代阈值法收敛（以及对最小非零奇异值的下界）

论文在第 4 节给出一组收敛性结论（Proposition 4.3），核心信息是：

在一定假设下（$L$ 下有界、梯度 Lipschitz、满足 KŁ 性质等），当步长满足
$$
0<\alpha \le \alpha_n \le \frac{\sigma_n}{L_{\nabla L}}
$$
则：

1. 目标值 $(L+\lambda\mathrm{rank})(W_n)$ 单调不增并收敛；
2. $\sum_n\|W_{n+1}-W_n\|<\infty$；
3. $W_n\to W^\*\in \mathrm{crit}(L+\lambda\mathrm{rank})$。

更有意思的是，当 $F_n(W)=\tfrac12\|I_n^{1/2}W\|_F^2$ 且 $I_n\to I\succ0$ 时，极限点的“最小非零奇异值”有下界（文中命题最后一条）：
$$
\sigma_{\min}\!\big(I^{1/2}W^\*\big)\ge \sqrt{\alpha\lambda}
\quad\Rightarrow\quad
\sigma_{\min}(W^\*)\ge \sqrt{\sigma_{\max}(I)\,\alpha\lambda}
$$
直觉：软秩惩罚 + 近端步骤会把一部分奇异值“推到 0”，而留下来的非零奇异值不会无限接近 0（避免出现一堆“几乎为 0 但又不为 0”的病态谱），这对训练稳定性是利好。

------

## 5）从理论到“改进后的方法”：为什么他们说 OIALR 的 cutoff 不好，以及 IEHT/IFHT 更温和

论文对 OIALR（“冻结 U,V 只训练 Σ，并周期性做 SVD+删奇异值”）给出形式化描述，并指出其 cutoff 规则（按最大奇异值比例阈值删）会导致**非平滑 rank reduction**：常见现象是先砍很大一刀，后面小刀补，恢复能力差（第 5 节+附录 B 的总结）。

于是他们提出把 cutoff 规则改成“按能量删”（IEHT）或“按 Fisher 加权能量删”（IFHT），使 rank reduction 更温和、更利于可训练性。

### 5.1 IEHT：按奇异值能量累计截断

对某层（或某次更新的 $S$）的奇异值 $\{\sigma_i\}$，定义总能量
$$
E_{\text{tot}}=\sum_i\sigma_i^2
$$
选择最小 $k$ 使得保留能量达到阈值 $\beta$：
$$
\sum_{i=1}^k\sigma_i^2 \ge \beta\,E_{\text{tot}}
$$
然后删掉 $i>k$ 的奇异值。相比“$\sigma_i < \beta\cdot\sigma_{\max}$”这种阈值，能量阈值通常更平滑，不容易一下子把很多“中等但有贡献”的奇异值砍掉。

### 5.2 IFHT：把“能量”换成 Fisher 加权能量（信息投影味道更浓）

他们在算法里用 $I_e$（某种可计算的 Fisher 近似）对奇异方向加权，示意写成对 $I_e S$ 做 SVD，再按加权能量做截断，最后映射回原空间（附录算法 2 的结构）：
$$
U',S',V'^\top = \mathrm{SVD}(I_e S)
$$
并用 $I_e^{-1}$ 把基变换回去，保证最终更新对应“信息度量下更合理的投影”。

这一套在概念上等价于：用 Fisher 度量定义“什么是重要方向”，再做更温和的迭代截断。

------

## 6）Step 实现过程：把论文主线落到可操作流程

这篇论文的“实现”主要体现在：如何把不同压缩法统一成投影/迭代投影，并给出 IEHT/IFHT 这两类 schedule。你可以按下面的工程流程理解与复刻：

### Step A：选择低算力子集（例如低秩流形）

对每层矩阵 $W_\ell\in\mathbb{R}^{n_\ell\times n_{\ell+1}}$，设目标秩 $r_\ell$，低算力集合：
$$
\mathcal{M}_{<r}=\{W:\ \mathrm{rank}(W_\ell)\le r_\ell,\ \forall \ell\}
$$

### Step B：选择投影距离（欧氏 or 信息）

- 欧氏：$\min\|W-\tilde W\|_F^2\Rightarrow$ 截断 SVD；
- 信息：$\min (W-\tilde W)^\top I (W-\tilde W)\Rightarrow$ 需要 Fisher 近似（FWSVD/TFWSVD）或激活代理（ASVD）。

### Step C：单次压缩（train-then-sparsify）

1. 训练全模型得 $\tilde\theta$
2. 对各层投影到 $\mathcal{M}_{（SVD 或 weighted SVD）
3. （可选）微调：$\arg\min_{\theta\in\mathcal{M}_{

### Step D：迭代压缩（sparsify-during-training / iterative thresholding）

每隔 $\nu$ 步做一次“阈值投影”，其余时间正常训练：

**OIALR 风格（基坐标冻结 + 只训 Σ）：**

1. 延迟到第 $d$ 步：把每层写成 $W=U\Sigma V^\top$
2. 冻结 $U,V$，只训练 $\Sigma$
3. 每隔 $\nu$ 步：对 $\Sigma$ 做 SVD 更新基、删奇异值（按阈值规则），调整优化器状态
4. 训练结束后 compile：把 $U\sqrt{\Sigma},\sqrt{\Sigma}V^\top$ 固化为两矩阵乘，删除 $\Sigma$ 参数

**IEHT/IFHT（把删法改成能量/加权能量）：**

- IEHT：删掉累计能量占比低于 $\beta$ 的尾部奇异值；
- IFHT：先估计 $I_e$，再在加权空间决定该删哪些奇异值，使删法更“信息投影化”。

### Step E：rank schedule（深度方向的分配）

论文实验指出：让“早层更宽松、后层更激进”（即随深度增加误差/压缩）往往更易训练——原因是早层收敛更快、在每次阈值投影后需要更大调整空间。

------

## 7）一些可延展的 idea（不展开到详细融合建议）

1. **把 SAES-SVD 的“累计误差抑制”重新解释成“信息投影的迭代误差分解”**
    论文强调 generalized Pythagorean（在信息几何里迭代投影误差可分解）。你可以尝试把 SAES 里的“累计+局部误差”写成某种 divergence 的可加项，从而把 SAES 从“启发式抑制”提升成“投影误差控制”的几何解释。
2. **把 SVD-LLM v2 的 truncation-aware 目标放进镜像近端框架**
    SVD-LLM v2 的核心是：截断误差与下游损失/激活敏感度相关。这里的镜像近端更新（用 $D_{F_n}$）提供了一个“理论容器”：你可以把 v2 的层内加权当作选择 $F_n$ 或近似 $I_n$ 的一种实现。
3. **IEHT 的“能量阈值删法”可推广为“平滑 rank 减少的控制律”**
    例如设定每次投影最多减少 $\Delta r$，或对 $r_t$ 加总变差正则 $\|r_t-r_{t-1}\|$，从优化角度就是对 rank schedule 加一个稳定项，避免 OIALR 那种“大刀”造成的不可逆损失。
4. **信息投影在 zero-shot 更关键：可把压缩目标分成两阶段**
    论文实证：zero-shot 时信息投影（FWSVD）显著更好，但 finetune 后差异缩小。一个自然想法是：先用“信息投影”拿到更好的初始化，再用“迭代阈值训练”追求更高压缩比；两者分别对应不同阶段的主矛盾（分布保持 vs 可训练性）。
5. **把“最小非零奇异值下界”用于设计稳定的阈值/学习率**
    既然理论给出 $\sigma_{\min}\ge \sqrt{\alpha\lambda}$ 这类关系，你可以反推：给定你希望保留的谱间隔，如何设 $\alpha,\lambda$ 或阈值，使得不会出现一堆“接近 0 的幽灵奇异值”导致数值不稳。