# SoLA（Soft activation sparsity + Low-rAnk decomposition）论文详细分析（理论 + Step 实现 + Ideas）

> 这篇论文的核心是：在现代 LLM 的 FFN 中发现一种“**软激活稀疏性**”（soft activation sparsity）：少量中间神经元的激活范数贡献了绝大部分能量与性能；因此**保留少量“关键神经元”不压缩**，其余“边缘神经元”再用**激活感知的低秩分解**压缩；并用一个**组件级自适应 rank 分配**在全局预算下选择各矩阵的截断位置。Huang 等 - 2025 - SoLA Leveragin…

------

## 1) 从“传统 SVD 压缩目标”到“ SoLA 改进目标”的公式链

### 1.1 传统（权重空间）低秩分解目标

给定权重矩阵 $W\in\mathbb{R}^{m\times n}$，经典 rank-$k$ 近似是：
$$
\min_{\mathrm{rank}(W')\le k}\ \|W-W'\|_F^2
\quad\Rightarrow\quad
W'\approx U_k\Sigma_kV_k^\top
$$
或写成两因子：
$$
W' = AB,\quad A\in\mathbb{R}^{m\times k},\ B\in\mathbb{R}^{k\times n}
$$
参数量从 $mn$ 变为 $(m+n)k$。

### 1.2 激活/输入感知的压缩目标（更贴近推理）

推理更关心输出误差：
$$
\min_{\mathrm{rank}(W')\le k}\ \|WX - W'X\|_F^2
$$
其中 $X$ 是校准数据得到的输入激活（或其集合）。SoLA 在低秩分解部分沿用“用输入二阶矩构造缩放/白化矩阵”的套路（与 SVD-LLM 风格一致）：对 $XX^\top$ 做 Cholesky 得到缩放矩阵 $S$，再对 $WS^{-1}$ 做 SVD，并在该空间里截断奇异值，从而使截断误差与奇异值能量直接对应。Huang 等 - 2025 - SoLA Leveragin…

------

## 2) SoLA 核心一：软激活稀疏性驱动的 FFN 细粒度分解

### 2.1 FFN 计算与“神经元”的定义

论文把两层 FFN 写为（忽略 bias）：
$$
h = \sigma(XW_{in}),\qquad
out = hW_{out}
$$
其中 $X\in\mathbb{R}^{d}$，中间维度为 $d_{ff}$，$W_{in}\in\mathbb{R}^{d\times d_{ff}},\ W_{out}\in\mathbb{R}^{d_{ff}\times d}$。第 $i$ 个神经元对应 $h_i$（中间状态的第 $i$ 维）；对应到权重上：$W_{in}$ 的第 $i$ 列与 $W_{out}$ 的第 $i$ 行共同决定该神经元的贡献。

### 2.2 软激活稀疏性：用“激活范数长尾分布”刻画 

在 SiLU/GeLU 这类软激活下，几乎不会出现大量严格为 0 的输出，但论文观察到：**按神经元排序后，激活范数呈强长尾分布**，少量神经元占据大部分累计能量；并用“移除高范数神经元会显著劣化困惑度”来证明这些神经元的重要性。

一种可复现的形式化写法是：对第 $\ell$ 层 FFN 的第 $i$ 个神经元，定义激活范数（示例）：
$$
a_i = \|h_{:,i}\|_2 \quad\text{或}\quad a_i=\|XW_{in,:,i}\|_2
$$
将 $a_i$ 从大到小排序，取“Prime Neurons（PN）”集合 $\alpha$ 与 “Marginal Neurons（MN）”集合 $\beta$，并用比例超参 $\gamma = |\alpha|/d_{ff}$ 控制 PN 的数量。论文给了经验选择：例如在某些设置下 top 15% 神经元能覆盖约 95% 的累计能量，因此默认 $\gamma=0.15$。

更一般地，你可以用累计能量阈值来定 $\gamma$：
$$
\min\ k\ \text{s.t.}\ \frac{\sum_{i=1}^{k} a_{(i)}^2}{\sum_{i=1}^{d_{ff}} a_{(i)}^2}\ge \tau,\qquad
\gamma = k/d_{ff}
$$
其中 $a_{(i)}$ 表示排序后的范数，$\tau\in(0,1)$ 是覆盖率（如 0.95）。

### 2.3 改进后的 FFN 目标：只压缩 MN，对 PN 保持“原形”

把神经元维度切分后，可把 FFN 写成两部分之和（论文给出等式形式）：
$$
FFN(X)
=\sigma(XW_{in}^{\alpha})W_{out}^{\alpha}
+\sigma(XW_{in}^{\beta})W_{out}^{\beta}
\tag{SoLA-FFN-Split}
$$
SoLA 的策略是：

- **PN 部分 $(\alpha)$**：不压缩（保留原矩阵列/行对应的通道）
- **MN 部分 $(\beta)$**：对对应的子矩阵做激活感知低秩分解（下面给出具体步骤）

因此，相比“对整块 $W_{in},W_{out}$ 统一低秩”的目标，SoLA 的隐含目标等价于在同一预算下最小化：
$$
\| \sigma(XW_{in}^{\beta})W_{out}^{\beta}
-\sigma(X\widehat{W}_{in}^{\beta})\widehat{W}_{out}^{\beta}\|_F^2
$$
并把 PN 的误差项固定为 0（因为不压缩 PN）。这就是它能在高压缩率下更稳的核心原因之一。

### 2.4 MN 子矩阵的激活感知 SVD（白化/缩放 + 截断）

对 MN 子矩阵（例如 $W_{in}^{\beta}$ 或 $W_{out}^{\beta}$）：

1. 用校准输入构造二阶矩并 Cholesky 得缩放矩阵 $S_\beta$
2. 变换后做 SVD：

$$
W_\beta S_\beta^{-1}=U_\beta \Sigma_\beta V_\beta^\top
$$

1. 截断得到 rank-$r$ 近似，再映回原空间（吸收 $S_\beta$）：

$$
\widehat{W}_\beta = (U_{\beta,r}\Sigma_{\beta,r}V_{\beta,r}^\top)S_\beta
$$

论文把这一思想写进 MN 的计算表达中（用分解后的 $U,\Sigma,V$ 替代 $W_\beta$）。Huang 等 - 2025 - SoLA Leveragin…

> 注意：SoLA 的“软激活稀疏性”主要用于 FFN（gate/up/down 等），而注意力模块本身没有非线性激活，因此 SoLA 对 attention 通常是“整矩阵低秩压缩”（实践中会排除某些敏感投影，见实现细节页）。

------

## 3) SoLA 核心二：组件级自适应截断位置（rank）分配

### 3.1 截断误差与奇异值能量（闭式可度量）

论文给出一个关键结论（写成定理形式）：在上述“缩放/白化”的 SVD 空间里，截断最小奇异值带来最小压缩损失，且压缩损失可写成被截断奇异值的平方和（能量尾和）。这使得“选 rank”可以变成可计算的能量保留问题。

因此对某个组件（某个要压缩的矩阵）$c$，设其变换后奇异值为 $\{\sigma_i\}$，取截断位置 $r_c$（保留前 $r_c$ 个），定义性能分数（能量保留率）：
$$
f(r_c)=\frac{\sum_{i=1}^{r_c}\sigma_i^2}{\sum_{i=1}^{n}\sigma_i^2}
\tag{SoLA-score}
$$
分数越高表示保留能量越多、理论上压缩误差越小。

### 3.2 把“rank 分配”写成全局预算优化（整数规划）

设：

- $r_c$：组件 $c$ 的截断位置（rank）
- $g(r_c)$：对应的显存/参数开销（论文把它当作与 rank 相关的内存占用函数）
- $B$：总内存预算（由目标压缩率给定）

则论文把问题写为：
$$
\max_{\{r_c\}}\ \sum_c f(r_c)
\quad\text{s.t.}\quad
\sum_c g(r_c)\le B
\tag{SoLA-budget}
$$
这是典型的整数规划/背包类问题，解空间巨大，不能穷举。论文采用启发式的 greedy 搜索，并为了匹配 GPU 计算习惯，把 $r$ 限制为 16 的倍数。

------

## 4) SoLA 的完整 Step-by-step 实现流程（训练自由）

下面按“你写代码/复现时的真实流水线”给一个可执行版本。

### Step 0：准备校准数据

- 采样一小批文本（论文用 WikiText2/C4 的校准样本作为示例），前向得到各层输入激活 $X$（以及 FFN 中间激活 $h$ 或其范数统计）。

### Step 1：统计 FFN 神经元激活范数，确定 PN/MN

对每层 FFN：

1. 计算每个神经元的激活范数 $a_i$
2. 按 $a_i$ 降序排序
3. 用比例 $\gamma$ 或能量阈值 $\tau$ 得到 PN 集合 $\alpha$ 与 MN 集合 $\beta$

### Step 2：对 FFN 做“按神经元切片”的结构改写

将该层 FFN 的 $W_{in},W_{out}$ 按神经元维度切片：

- $W_{in}\to (W_{in}^{\alpha},W_{in}^{\beta})$
- $W_{out}\to (W_{out}^{\alpha},W_{out}^{\beta})$

推理时用：
$$
FFN(X)=\sigma(XW_{in}^{\alpha})W_{out}^{\alpha}+\sigma(XW_{in}^{\beta})W_{out}^{\beta}
$$
其中 $\alpha$ 部分保持原矩阵。

### Step 3：对 MN 子矩阵做激活感知低秩分解

对每个 MN 子矩阵 $W_\beta$（例如 $W_{in}^{\beta}$、$W_{out}^{\beta}$ 或 gate/up/down 的 MN 切片）：

1. 构造输入二阶矩（或等价统计）并做 Cholesky 得 $S_\beta$
2. 计算 $W_\beta S_\beta^{-1}$ 的 SVD 得 $(U_\beta,\Sigma_\beta,V_\beta)$
3. 先不固定 rank，把奇异值序列存下来供 Step 5 做全局分配

### Step 4：对注意力模块做低秩分解（通常整矩阵）

对注意力模块中的各投影矩阵（q/k/v/o）：

- 同样用缩放 + SVD 得到奇异值序列
- 实践里可按论文实现细节“跳过最敏感的某些投影/层”（例如 v 或部分 o/k），以避免明显性能坍塌（属于工程策略）。

### Step 5：组件级自适应 rank 分配（全局预算）

1. 对每个组件 $c$，由 $\{\sigma_i\}$ 计算 $f(r)$ 曲线（能量保留率）
2. 给定总预算 $B$，用 greedy 在所有组件间分配 rank，使

$$
\sum_c g(r_c)\le B,\quad \sum_c f(r_c)\ \text{尽量大}
$$

并将 $r_c$ 约束为 16 的倍数（便于 kernel）

### Step 6：按选定的 $r_c$ 截断并替换权重

对每个组件 $c$：

- 截断 $W_c S_c^{-1} \approx U_{r_c}\Sigma_{r_c}V_{r_c}^\top$
- 映回原空间得到 $\widehat{W}_c$
- 在 FFN：PN 通道保持 dense；MN 通道用低秩（通常落地为两层线性）
- 在 attention：整矩阵替换为两层线性（或按策略跳过某些矩阵/层）

------

## 5) 这篇论文的关键贡献点（用“理论—机制”表达）

1. **软激活稀疏性**：即便没有 ReLU 的硬稀疏，FFN 神经元激活范数也呈长尾；少量高范数神经元对性能至关重要。
2. **细粒度压缩**：在 FFN 里“按神经元切片”，对 PN 保留 dense、对 MN 才做低秩，避免把关键通道一起压缩。
3. **组件级 rank 分配**：用“奇异值能量保留率”做可计算的分数 $f(r)$，在全局预算下分配各组件的截断位置，而不是所有矩阵用同一个 rank。

------

## 6) 一些可延展的 Ideas（不做融合细节，只给方向）

### Idea 1：$\gamma$ 从“固定比例”升级为“层自适应/数据自适应”

目前 PN 比例 $\gamma$ 主要靠经验/能量阈值。可以考虑：

- 每层用不同 $\gamma_\ell$，由该层激活长尾程度自动决定
- 甚至按输入 batch 动态决定 PN（接近“软 MoE”的 gating），对 MN 走低秩支路

### Idea 2：PN/MN 的重要性指标不只看激活范数

激活范数是启发式 proxy。可探索更“任务相关”的重要性：
$$
\text{score}_i \propto \mathbb{E}[\ |\partial \mathcal{L}/\partial h_i|\cdot |h_i|\ ]\quad\text{或 Fisher/梯度近似}
$$
仍保持训练自由（只需少量反传或近似统计）。

### Idea 3：MN 的低秩不必是统一 rank，可做“分块/分头/分通道 rank”

SoLA 的 rank 分配在“组件级”。可以进一步细到：

- FFN 的 MN 通道按 block 分组，rank 分配更细
- attention 按 head/group 分配 rank（尤其 GQA/MQA 下更可能有结构差异）

### Idea 4：rank 分配从 greedy 升级为“可证明近似”的预算优化

当前是启发式 greedy。可以考虑：

- 把 $f(r)$ 做离散边际收益 $\Delta f$ 的背包近似
- 或用拉格朗日松弛：最大化 $\sum_c (f(r_c)-\lambda g(r_c))$ 并二分 $\lambda$

### Idea 5：把“能量保留率 $f(r)$”替换为更贴近输出误差的指标

能量是很好的闭式 proxy，但仍是间接的。可考虑用校准集直接度量：
$$
\tilde f_c(r) = -\|W_cX - \widehat{W}_{c,r}X\|_F^2
$$
然后做小规模的候选 rank 搜索（保持训练自由，但更贴近真实输出误差）。