# DipSVD: Dual-importance protected SVD for efficient LLM compression

它的核心贡献是：在 SVD 压缩里同时引入 **局部重要性保护（intra-layer）** 与 **全局重要性保护（inter-layer）**，并把两者“耦合”起来做联合压缩，从而在高压缩率下比只做局部（FWSVD）或只做全局（ASVD）更稳。

------

## 1）原始优化目标：标准 SVD 只最小化整体重构误差，忽略“重要成分保护”

对一层权重矩阵 $W\in\mathbb{R}^{n\times m}$，标准截断 SVD 做低秩近似：
$$
W \approx U_k \Sigma_k V_k^\top
$$
等价于在 Frobenius 范数意义下最优：
$$
\min_{\mathrm{rank}(W')\le k}\ \|W-W'\|_F^2
$$
但这个目标是“各方向一视同仁”的：它只关心整体误差能量，不关心某些对任务更关键的方向/通道/层是否被优先保留。

DipSVD认为：LLM 中存在两种异质性——

- 局部（层内）：不同通道贡献不同，应该“保护重要通道对应的奇异方向”；
- 全局（层间）：不同层对任务敏感度不同，不应统一压缩率，否则关键层承担过多压缩误差。

------

## 2）改进总目标：双层重要性保护下的“联合压缩分配 + 重要方向优先保留”

DipSVD把问题拆成两个互补部分，并在最终流程里把它们组合：

- 局部重要性保护：让“重要通道”在 whitening 与 SVD 中被放大，从而在截断时优先保留这些通道相关的奇异方向；
- 全局重要性保护：自动为每个 Transformer layer 分配不同压缩率 $k_l$，让不重要的层承担更多压缩负担。

你可以把它的整体思想写成一个“跨层预算约束”的优化雏形：
$$
\min_{\{k_l\},\{W'_l\}}
\sum_{l=1}^{L}\ \mathcal{E}_l(W_l,W'_l;\text{local-importance})
\quad
\text{s.t.}\quad
\frac{1}{L}\sum_{l=1}^{L}k_l = k
$$
其中 $\mathcal{E}_l$ 不再是简单的 $\|W_l-W_l'\|_F^2$，而是通过“重要性加权 whitening”把误差更对齐到关键结构。

------

## 3）局部重要性保护：Channel-weighted Whitening（核心公式 + 关键推导）

DipSVD的局部模块核心是：先从校准激活 $X$ 中识别重要通道，再通过一个对角放大矩阵 $D$ 做“通道加权 whitening”。

### 3.1 通道重要性定义（结构贡献度）

设输入激活矩阵 $X\in\mathbb{R}^{m\times n}$（$m$ 个样本/Token，$n$ 个通道），第 $j$ 个通道向量为 $x_j=X_{:,j}\in\mathbb{R}^m$。

论文定义该通道的重要性为：
$$
\alpha_j=\sqrt{x_j^\top(XX^\top)x_j}
$$
直观解释：$XX^\top$ 表示样本空间的二阶结构，$\alpha_j^2$ 衡量通道方向 $x_j$ 在样本主结构上的“投影强度/对整体结构的贡献”。

### 3.2 通道放大矩阵 $D$

构造对角矩阵 $D\in\mathbb{R}^{n\times n}$：
$$
D_{jj}=
\begin{cases}
a, & \alpha_j \text{ 属于 top }p\%\\
1, & \text{otherwise}
\end{cases}
\quad (a>1)
$$
得到重加权输入：
$$
\tilde X = X D
$$
它的协方差（Gram）为：
$$
\tilde X^\top \tilde X = D^\top X^\top X D = D^\top X^\top X D
$$

### 3.3 whitening 矩阵 $S$

对 $\tilde X^\top \tilde X$ 做特征分解/SVD：
$$
\tilde X^\top \tilde X = U_{\tilde X}\Sigma_{\tilde X}U_{\tilde X}^\top
$$
定义 whitening：
$$
S=\Sigma_{\tilde X}^{-1/2}U_{\tilde X}^\top
$$
最终 whitened 输出是：
$$
\hat X = \tilde X S = X D S
$$
whitening 的关键性质是（单位协方差）：
$$
S^{-1}\tilde X\tilde X^\top (S^{-1})^\top = I
$$

### 3.4 为什么这能“把截断损失直接映射到奇异值”（DipSVD的关键理论点）

对 whitened 权重 $WS$ 做 SVD：
$$
WS = U\Sigma V^\top=\sum_{i=1}^{r}\sigma_i u_i v_i^\top
$$
若截断第 $i$ 个奇异分量（只看单个分量的影响），论文定义在输入 $\tilde X$ 上的输出差（compression loss）：
$$
L_i=\|W\tilde X - W'\tilde X\|_F
$$
在 whitened 形式下可写为（把差异写成被删掉的那一项）：
$$
L_i=\|\sigma_i u_i v_i^\top S^{-1}\tilde X\|_F
$$
利用 $u_i,v_i$ 正交归一，以及上面的 whitening 性质 $S^{-1}\tilde X\tilde X^\top(S^{-1})^\top=I$，可推出：
$$
L_i=\sigma_i
$$
进一步，如果截断多个最小奇异值 $\{\sigma_{m+1},\dots,\sigma_r\}$，总损失为：
$$
L=
\left\|\sum_{i=m+1}^{r}\sigma_i u_i v_i^\top S^{-1}\tilde X\right\|_F
=
\sqrt{\sum_{i=m+1}^{r}\sigma_i^2}
$$
意义很明确：whitening 后，“截断哪些奇异值”与“在校准输入上的输出损失”建立了直接对应关系；而通道加权 $D$ 又让重要通道在 whitening 中被强调，从而使重要结构更倾向落在前部奇异方向里，降低被截断的概率。

------

## 4）全局重要性保护：层间压缩率分配（BayesOpt 或轻量启发式）

DipSVD给出两种全局策略：算力足够用 Bayesian Optimization，算力有限用启发式（Fisher sensitivity + effective rank）。

### 4.1 Bayesian Optimization（高性能版）

以每层压缩率 $\{k_1,\dots,k_L\}$ 为搜索变量，在全局预算约束下，最大化原模型与压缩模型输出的相似度（论文写成 cosine similarity 目标）：
$$
\max_{k_1,\dots,k_L}\ \mathrm{cosine\_similarity}(f_{\text{orig}}(x), f_{\text{comp}}(x))
\quad
\text{s.t.}\quad
\frac{1}{L}\sum_{l=1}^{L}k_l = k
$$
其中 $k$ 是目标全局压缩率（或保留率的等价表述，论文在不同段落用“compression ratio / preservation ratio”切换，工程实现里通常统一成“保留比例”更直观）。

### 4.2 启发式（高效率版）：Fisher Sensitivity + Effective Rank

**(1) Fisher Sensitivity：层参数对损失的相对敏感度**

论文定义每层 Fisher sensitivity（把 attention 与 MLP 两块加总）：
$$
S_l =
\sum_{\text{Attention}}\frac{\|\nabla_\theta L\|_F}{\|\theta\|_F}
+
\sum_{\text{MLP}}\frac{\|\nabla_\theta L\|_F}{\|\theta\|_F}
$$
直觉：梯度范数越大且参数范数越小，说明该层单位参数对损失更敏感，应更少压缩。

**(2) Effective Rank：层输出信息密度/可压缩性**

取该层输出隐状态 $H_l\in\mathbb{R}^{B\times T\times D}$，reshape 成二维 $\in\mathbb{R}^{(BT)\times D}$，做 SVD 得奇异值 $\{\sigma_i\}$，定义捕获能量阈值（如 95%）下的最小秩：
$$
R_l=\min\left\{
k\ \bigg|\ \frac{\sum_{i=1}^{k}\sigma_i}{\sum_{i=1}^{r}\sigma_i}\ge \text{threshold}
\right\}
$$
（论文给的是对奇异值的累计占比阈值写法；工程上也常用 $\sigma_i^2$ 能量占比，两者都表达“低秩程度”。）

直觉：$R_l$ 越小，说明层输出本身更接近低秩、更可压缩。

**(3) 统一重要性分数 $Q_l$**
 把“重要（敏感）”与“可压缩（低有效秩）”结合，定义：
$$
Q_l = (S_l)^{\beta}\cdot (R_l)^{1-\beta}
$$
$\beta\in[0,1]$ 控制两者权重（论文实验中给了一个固定取值使其与 BayesOpt 分配相关性高）。

**(4) 在全局预算下分配每层保留率 $p_l$**
 令全局目标压缩率为 $k$，则平均保留率为 $1-k$。DipSVD用比例分配：
$$
p_l=\frac{Q_l}{\sum_{j=1}^{L}Q_j}\cdot L\cdot (1-k)
$$
于是该层压缩率（删减比例）就是：
$$
1-p_l
$$
这个形式保证 $\frac{1}{L}\sum_l p_l=1-k$，预算严格满足。

------

## 5）最终压缩：在每层用“加权 whitening + 层特定 $k_l$”做截断 SVD 并回到原空间

对第 $l$ 层：

1. 由校准数据得到 $D_l,S_l$，构造 whitened 权重 $W_lS_l$；
2. 做 SVD：

$$
W_lS_l = U_l\Sigma_l V_l^\top
$$

1. 按该层分配的 $k_l$ 截断（论文伪代码用能量阈值形式选择保留秩）：

$$
\Sigma_l'=\mathrm{Trunc}(\Sigma_l;k_l)
$$

1. 回映射得到压缩权重：

$$
W_l' = U_l\Sigma_l'V_l^\top S_l^{-1}
$$

1. 部署时将其拆成两段低秩乘（省显存/省算）：

$$
W_{u,l}=U_l(\Sigma_l')^{1/2},\qquad
W_{v,l}=(\Sigma_l')^{1/2}V_l^\top S_l^{-1}
$$

使原来一次乘 $W_lx$ 变成两次乘 $W_{u,l}(W_{v,l}x)$。

------

## 6）Step 实现过程（按工程可落地顺序）

### Step 0：校准数据

随机取一小批句子作为校准集 $C$。

### Step 1：局部重要性（对每个需压缩的线性层）

1. 前向跑校准集，收集该层输入激活 $X$；
2. 计算每个通道的重要性 $\alpha_j=\sqrt{x_j^\top(XX^\top)x_j}$；
3. 选 top $p\%$ 通道，构造对角放大 $D$（放大系数 $a>1$）；
4. 计算 $\tilde X^\top\tilde X=D^\top X^\top X D$，分解得到 whitening $S=\Sigma^{-1/2}U^\top$；
5. 缓存每层的 $S_l$（后续用于压缩与反变换）。

### Step 2：全局重要性（得到每层压缩率/保留率）

- BayesOpt：在约束 $\frac{1}{L}\sum k_l=k$ 下，搜索最大化输出余弦相似度的 $\{k_l\}$；
- 启发式：
  1. 在校准集上反向求梯度，计算每层 $S_l$（Fisher sensitivity）；
  2. 在校准集上收集每层输出隐状态，算 effective rank $R_l$；
  3. 合成 $Q_l=(S_l)^\beta(R_l)^{1-\beta}$，再用 $p_l=\frac{Q_l}{\sum Q}L(1-k)$ 分配。

### Step 3：逐层压缩与替换

对每层：

1. 形成 $W_lS_l$，做 SVD；
2. 按该层 $k_l$ 截断得到 $\Sigma_l'$；
3. 回映射 $W_l'=U_l\Sigma_l'V_l^\top S_l^{-1}$；
4. （可选）拆成 $W_{u,l},W_{v,l}$ 两矩阵以便推理加速。

------

## 7）一些可延展的 idea（只给方向，不展开成详细融合方案）

1. **把通道放大系数 $a$ 从“全局常数”变成“层自适应/通道自适应”**
    现在 $a$ 对所有层统一，可能不匹配各层统计分布。可以让 $a_l$ 与层的谱跨度、激活峰度、或 Fisher sensitivity 相关，避免某些层被过度放大导致 whitening 失衡。
2. **把“top $p\%$”的硬选择改成软权重**
    例如把 $D_{jj}$ 设计成连续函数 $D_{jj}=g(\alpha_j)$，从而避免阈值附近通道的非连续性，提高稳定性。
3. **全局分配从比例分配升级为显式优化（带平滑约束）**
    当前 $p_l$ 是简单 proportional allocation。可以写成：

$$
\min_{\{p_l\}}\ \sum_l \Phi_l(p_l)
\quad
\text{s.t.}\ \frac{1}{L}\sum_l p_l=1-k,\ p_l\in[p_{\min},p_{\max}]
$$

再加一个深度方向平滑项 $\sum_l (p_{l+1}-p_l)^2$，避免相邻层保留率跳变过大。

1. **把局部损失映射（截断损失=奇异值能量）用于“更精细的 rank 选择”**
    既然 whiten 后截断损失能直接写成 $\sqrt{\sum \sigma_i^2}$，就可以做“给定每层最大允许损失阈值 $L_l^{\max}$”的秩选择：

$$
\min k_l\ \text{s.t.}\ \sqrt{\sum_{i>k_l}\sigma_i^2}\le L_l^{\max}
$$

并用全局约束协调 $\sum \text{params}(k_l)$。

1. **把全局重要性与局部重要性做更强耦合**
    现在是“先定 $k_l$，再做局部加权 SVD”。可以反过来：局部模块能给出“若保留到秩 $k$ 的预测损失曲线 $L_l(k)$”（由奇异值累计能量直接算），然后全局模块在 $\sum \text{params}$ 预算下最小化 $\sum_l w_l L_l(k_l)$，其中 $w_l$ 由 Fisher sensitivity 决定，这样两层重要性在同一个目标里统一起来。