# LOST（Low-rank and Sparse pre-Training for LLMs）

LOST 关注的是**“从零开始预训练”**的效率：不再用全秩权重 $W$ 训练，而是直接用**低秩 + 稀疏**的结构训练。关键点不在于“低秩+稀疏”本身，而在于它的**协同设计（co-design）**：

- 低秩部分用一次 SVD 初始化，保留最大奇异值对应的主子空间；
- 稀疏部分不是随便加一个稀疏矩阵，而是从**被截断的奇异谱剩余子空间**中提取“互补信息”，并以**channel-wise（按通道/列）结构化稀疏**形式存储与训练。

------

## 1) 从基线优化目标到 LOST 目标：它到底改了什么

### 1.1 基线：全秩预训练的目标

对任意线性层权重 $W\in\mathbb{R}^{d_{\text{out}}\times d_{\text{in}}}$，预训练本质是在最小化语言模型损失（交叉熵）：
$$
\min_{\{W_\ell\}}\ \mathbb{E}_{(x,y)}\big[\mathcal{L}(f(x;\{W_\ell\}),y)\big]
$$
这里所有层的 $W_\ell$ 都是全参数、全秩可训练。

### 1.2 朴素低秩预训练：直接把 $W$ 因式分解

常见低秩参数化是：
$$
W \approx BA^\top,\quad
B\in\mathbb{R}^{d_{\text{out}}\times r},\ 
A\in\mathbb{R}^{d_{\text{in}}\times r},\ r\ll \min(d_{\text{out}},d_{\text{in}})
$$
并改为训练 $(A,B)$：
$$
\min_{A,B}\ \mathbb{E}\big[\mathcal{L}(f(x;\{BA^\top\}),y)\big]
$$
问题在于：**仅低秩会丢掉大量“非主子空间”的表达能力**，尤其在“从零开始预训练”场景里，会出现明显性能差距（训练动力学与表达受限）。

### 1.3 稀疏 + 低秩：LOST 的“改进后的目标/参数化”

LOST 不是简单做：
$$
W \approx BA^\top + S \quad(\text{随便的稀疏 } S)
$$
而是把稀疏项设计成“截断奇异谱剩余子空间的互补表达”，并用结构化稀疏便于硬件与存储。

它的“改进后目标”可以写成**同样的训练损失，但变量换成结构化参数**：
$$
\min_{\{A_\ell,B_\ell,W_{s,\ell}\}}\ \mathbb{E}\Big[\mathcal{L}\big(f(x;\{\Phi_\ell(A_\ell,B_\ell,W_{s,\ell})\}),y\big)\Big]
$$
其中 $\Phi_\ell(\cdot)$ 表示该层前向被替换为“低秩路径 + 稀疏路径”的组合（下面给出精确公式），并且稀疏路径带有固定的通道选择索引集合 $I$（初始化时确定）。

------

## 2) LOST 的理论核心：低秩保主子空间 + 稀疏补剩余子空间

### 2.1 SVD 分解与主/剩余子空间

对初始化得到的全秩矩阵 $W$ 做 SVD：
$$
W = U\Sigma V^\top = \sum_{i=1}^{D} \sigma_i u_i v_i^\top,\quad D=\min(d_{\text{out}},d_{\text{in}})
$$
取目标低秩 rank $r$，主子空间（低秩部分）对应前 $r$ 个奇异值：
$$
W_{\text{low}}=\sum_{i=1}^{r}\sigma_i u_i v_i^\top
$$
剩余子空间（互补矩阵）对应被截断的奇异谱：
$$
W_{\text{comp}}=\sum_{i=r+1}^{D}\sigma_i u_i v_i^\top
$$
LOST 的关键是：**稀疏部分不是从 $W$ 或随机噪声来，而是从 $W_{\text{comp}}$ 来**，这样它与低秩部分在奇异谱意义上是互补的（一个覆盖主子空间，一个从剩余子空间“挑重要通道”补回来）。

------

## 3) 从优化目标到“可实现的前向公式”：LOST 的两条路径

### 3.1 低秩部分（SVD 初始化 + 非线性增强表达）

先用 SVD 的 top-$r$ 构造因子（写成便于两层线性实现的形式）：
$$
A = V_r \Sigma_r^{1/2}\in\mathbb{R}^{d_{\text{in}}\times r},\quad
B = U_r \Sigma_r^{1/2}\in\mathbb{R}^{d_{\text{out}}\times r}
$$
于是 $W_{\text{low}}\approx B A^\top$。

LOST 进一步在低秩两因子之间加入激活函数（例如 SiLU），把线性低秩映射升级成“带非线性的低秩模块”：
$$
y_{\text{low}} = \sigma(xA)\,B^\top
$$
这相当于在 rank-$r$ bottleneck 上增加非线性，提高表达能力（参数不增加很多，计算开销也小）。

> 这一步可以理解为：不是逼近某个固定的线性 $W$，而是直接给“从零预训练”更强的可训练函数族。

### 3.2 稀疏部分（channel-wise 互补：从 $W_{\text{comp}}$ 选列）

LOST 做的是**按输入通道（列）选择**。对 $W_{\text{comp}}$ 的每一列 $j$ 定义通道重要性（L2 范数）：
$$
CI_j=\|W_{\text{comp}}[:,j]\|_2,\quad j=1,\dots,d_{\text{in}}
$$
给定稀疏比 $\rho$，保留 $k=\lceil \rho\cdot d_{\text{in}}\rceil$ 个通道：
$$
I=\text{TopKIndices}(CI,\ k)
$$
并用原始 $W$ 的对应列构造稀疏权重（仅存储这些列）：
$$
W_s = W[:,I]\in\mathbb{R}^{d_{\text{out}}\times k}
$$
稀疏路径前向（仅取输入的这些维度）：
$$
y_{\text{sparse}} = x[:,I]\ W_s^\top
$$
这种 channel-wise 稀疏相比元素级稀疏的优势是：

- 存储只需存 $W[:,I]$ 与索引 $I$，避免存全矩阵形状的 mask；
- 推理/训练更容易做高效的内存访问与 kernel 优化（结构化稀疏）。

### 3.3 两条路径的组合：输出级加权平均

LOST 用一个系数 $\gamma\in[0,1]$ 在输出端融合两条路径：
$$
o = \gamma\cdot y_{\text{low}} + (1-\gamma)\cdot y_{\text{sparse}}
= \gamma\cdot \sigma(xA)B^\top + (1-\gamma)\cdot x[:,I]W_s^\top
$$
这就是“改进后”的可训练层算子 $\Phi(\cdot)$。

> 注意：这不是简单的 $W=\gamma W_{\text{low}}+(1-\gamma)W_s$ 的权重融合，而是**输出融合**，并且低秩路径中间有非线性，因此整体映射更强。

------

## 4) LOST 的 Step-by-step 实现流程（从初始化到训练）

下面把论文的 Algorithm 1 过程整理成你写代码时最清晰的 pipeline（对每个线性层都做一次；也可按模块批量做）。

### Step 0：初始化一个临时全秩权重 $W$

用标准初始化（如 Kaiming）初始化全秩矩阵 $W$。**注意：SVD 只做一次，用于初始化；之后训练不再需要存 full-rank 的 $W$**（除非你想做额外对照/分析）。

### Step 1：低秩组件初始化（SVD top-$r$）

1. 对 $W$ 做 SVD：$W=U\Sigma V^\top$

2. 取 top-$r$：$U_r,\Sigma_r,V_r$

3. 构造：
   $$
   A = V_r\Sigma_r^{1/2},\quad B=U_r\Sigma_r^{1/2}
   $$

4. 在模型中把原线性层替换成低秩路径：$y_{\text{low}}=\sigma(xA)B^\top$（低秩因子之间带 SiLU）

### Step 2：互补稀疏组件初始化（从剩余奇异谱挑通道）

1. 构造互补矩阵（剩余子空间）：
   $$
   W_{\text{comp}}=\sum_{i=r+1}^{D}\sigma_i u_i v_i^\top
   $$
   工程上可以用部分 SVD + 重构近似（例如只用一段“剩余谱”来估计重要通道，减少开销）。

2. 计算每个输入通道的重要性：
   $$
   CI_j=\|W_{\text{comp}}[:,j]\|_2
   $$

3. 设 $k=\lceil \rho d_{\text{in}}\rceil$，取 $CI$ 最大的 $k$ 个通道索引 $I$。

4. 构造并存储稀疏权重：
   $$
   W_s = W[:,I]
   $$
   仅保存 $W_s$ 和索引 $I$。

### Step 3：训练阶段前向（输出融合）

对每个 batch 输入 $x$：

1. 低秩路径：
   $$
   y_{\text{low}}=\sigma(xA)B^\top
   $$

2. 稀疏路径：
   $$
   y_{\text{sparse}}=x[:,I]W_s^\top
   $$

3. 融合输出：
   $$
   o=\gamma y_{\text{low}}+(1-\gamma)y_{\text{sparse}}
   $$

把 $o$ 送入后续层与损失计算。

### Step 4：反向传播与参数更新

用标准优化器（AdamW 等）更新：

- 低秩参数：$A,B$
- 稀疏参数：$W_s$
   索引 $I$ 固定（初始化后不变）。

### Step 5：全模型

论文建议对 transformer 中的注意力线性层与 FFN 线性层统一该分解策略（即“所有 linear layers 一致处理”），形成全模型的低秩+稀疏预训练。

------

## 5) 参数量、显存与计算：为什么 LOST 更“划算”

对 $W\in\mathbb{R}^{d_{\text{out}}\times d_{\text{in}}}$：

- 原参数：$d_{\text{out}}d_{\text{in}}$
- 低秩部分参数：$r(d_{\text{out}}+d_{\text{in}})$
- 稀疏部分参数：$d_{\text{out}}k$（再加很小的索引存储开销），其中 $k=\lceil\rho d_{\text{in}}\rceil$

总参数约为：
$$
r(d_{\text{out}}+d_{\text{in}})+d_{\text{out}}k
$$
并且稀疏部分是 channel-wise 结构化，避免元素级稀疏常见的 mask/索引巨大开销。

------

## 6) 一些可延展的 Ideas（不展开结合方案，只给方向）

### Idea 1：让通道选择从“静态一次”变成“阶段性可更新”

目前 $I$ 在初始化后固定。可以尝试：

- 每隔 $T$ 步用当前训练到的 $A,B$ 估计“剩余误差”，再更新一次 $I$（少次数、低频率），实现“稀疏通道的自适应迁移”。

### Idea 2：把 $CI_j=\|W_{\text{comp}}[:,j]\|_2$ 升级为“数据驱动的重要性”

例如用校准/训练早期的输入激活统计：
$$
CI_j = \mathbb{E}_x\big[|x_j|\big]\cdot \|W_{\text{comp}}[:,j]\|_2
\quad\text{或}\quad
CI_j = \|W_{\text{comp}}[:,j]\|_2\cdot \sqrt{\mathrm{Var}(x_j)}
$$
更贴近“该通道在训练中是否真被频繁使用”。

### Idea 3：稀疏部分从“列选择”扩展为“块稀疏 / head-wise 稀疏”

对 attention 投影矩阵可以按 head 或按 block 做结构化稀疏（block indices），更利于 GPU kernel，且与模型结构一致。

### Idea 4：融合系数 $\gamma$ 的更聪明用法（但不必可学习）

论文把 $\gamma$ 设为常数主要是出于显存/效率考虑。可以做：

- 层级 $\gamma_\ell$：不同层固定不同 $\gamma$（例如浅层更依赖稀疏补偿，深层更依赖低秩主子空间）
- 或训练 schedule：早期 $\gamma$ 小一点让稀疏补偿更强，后期逐渐增大 $\gamma$ 让低秩路径主导、稳定收敛。

### Idea 5：低秩路径的非线性位置/形式可系统探索

当前是 $\sigma(xA)B^\top$。可以试：

- $\sigma$ 换成门控形式（如 SwiGLU 风格）但保持参数不变
- 或在 rank bottleneck 上加 very-small 的归一化（RMSNorm/LayerNorm）提高训练稳定性。

### Idea 6：互补矩阵 $W_{\text{comp}}$ 的“谱段选择”可做预算优化

论文做法是取“截断后剩余谱”来构造互补。你可以尝试只用某个谱段（例如中间谱）来挑通道，形成：
$$
W_{\text{seg}}=\sum_{i\in\mathcal{S}}\sigma_i u_i v_i^\top
$$
再用 $\|W_{\text{seg}}[:,j]\|_2$ 选列，可能更符合某些层的训练动力学（尤其在极低 rank 时）。