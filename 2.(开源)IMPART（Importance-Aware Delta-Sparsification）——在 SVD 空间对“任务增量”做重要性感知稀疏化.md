# 论文分析：IMPART（Importance-Aware Delta-Sparsification）——在 SVD 空间对“任务增量”做重要性感知稀疏化 （代码开源）

这篇论文关注的是 **delta compression**：给定同一 base LLM 上的多个 fine-tuned 模型，只存每个任务的 **增量参数** $\Delta W = W_{\text{ft}}-W_{\text{base}}$，再对 $\Delta W$ 做稀疏化/量化以降低存储成本。IMPART 的核心贡献是：**不在权重元素空间做随机丢弃**，而是先对 $\Delta W$ 做 SVD，并在 **奇异向量（singular vectors）层级**按重要性分配不同稀疏率，从而在高稀疏率下更好保留任务知识。Yang 等 - 2025 - ImPart Importan…

------

## 1) 从“原始目标”到“改进目标”：IMPART 的理论出发点（含公式）

### 1.1 Delta 表示与 SVD 分解（方法工作的对象）

对每个线性层（或权重矩阵）：
$$
\Delta W \in \mathbb{R}^{m\times n},\qquad \Delta W = W_{\text{ft}}-W_{\text{base}}
$$
对 $\Delta W$ 做 SVD：
$$
\Delta W = U\Sigma V^\top = \sum_{k=1}^{n}\sigma_k\,u_k v_k^\top
\tag{1}
$$
其中 $\sigma_1\ge \sigma_2\ge\cdots$ 为奇异值，$(u_k,v_k)$ 为第 $k$ 个奇异向量对。Yang 等 - 2025 - ImPart Importan…

------

### 1.2 典型基线（论文要超越的“粗粒度稀疏化/截断”）

IMPART 对比的两类常见思路（论文在引言与相关工作里指出）Yang 等 - 2025 - ImPart Importan…：

1. **元素级随机稀疏化（如 DARE）**
    在 $\Delta W$ 的元素空间随机置零：

$$
\Delta W_c = M\odot \Delta W
$$

此法忽略“参数重要性”。

1. **低秩截断（LowRank）**
    仅保留 top-$r$ 奇异分量：

$$
\Delta W_r=\sum_{k=1}^{r}\sigma_k u_k v_k^\top
$$

此法粒度太粗：要么整对奇异向量保留，要么整对丢弃。

------

### 1.3 IMPART 的改进目标：在“奇异向量对”粒度分配稀疏预算

IMPART 的核心思想是：**奇异值越大，对应奇异向量对越重要**，因此给它更低的稀疏率；反之给更高稀疏率。Yang 等 - 2025 - ImPart Importan…

它不直接写成一个显式的最优化问题（如 $\min$ 某种 loss），而是构造一种“重要性引导的随机稀疏化算子”，使得：

- 对每个奇异向量对 $k$，分配稀疏率 $p_k\in[0,1]$
- 在 $U$ 的第 $k$ 列与 $V$ 的第 $k$ 列分别按 $p_k$ 随机 mask
- 对保留下来的元素做 rescale，使得重构量在期望意义下不偏（unbiased）

这种设计等价于：在满足总体稀疏率约束的前提下，**把更多非零预算留给“大奇异值对应的方向”**。

------

## 2) IMPART 的“改进后目标公式”：重要性感知稀疏化算子（核心公式链）

### 2.1 对奇异向量做 Bernoulli mask + 重标定（论文 Eq.(2)-(6)）

对第 $k$ 个奇异向量对，独立采样 mask：Yang 等 - 2025 - ImPart Importan…
$$
\xi^{(k)}_i\sim\text{Bernoulli}(1-p_k),\quad i=1,\dots,m
\tag{2}
$$
对 $U$ 与 $V$ 的元素做稀疏化并重标定（drop-and-rescale）：
$$
\tilde U_{ik} = U_{ik}\cdot \frac{\xi^{(k)}_i}{1-p_k}
\tag{4}
$$
用稀疏后的 $\tilde U,\tilde V$ 重构稀疏 delta：
$$
\Delta W_c = \tilde U\Sigma \tilde V^\top
\tag{6}
$$
这就是 IMPART 的“改进目标公式”本体：**不是截断奇异分量，而是对每个奇异向量对按重要性做不同稀疏率的随机稀疏化**。Yang 等 - 2025 - ImPart Importan…

------

### 2.2 关键理论性质：重构输出在期望意义下匹配原输出（论文 3.3 节）

论文给了一个核心证明：对任意输入向量 $X\in\mathbb{R}^{n}$，以线性层为例（忽略 bias），fine-tuned 输出为：
$$
h = W_{\text{ft}}X = W_{\text{base}}X + \Delta W X
$$
展开第 $i$ 维（论文 Eq.(8)）：
$$
\mathbb{E}[h_i] = h^{\text{base}}_i + \sum_{j}\sum_{k}\sigma_k U_{ik}V_{kj}X_j
\tag{8}
$$
对 IMPART 稀疏化后的输出 $\tilde h = (W_{\text{base}}+\Delta W_c)X$，利用独立性与重标定（论文 Eq.(9)）：
$$
\mathbb{E}[\tilde U_{ik}] = U_{ik},\qquad \mathbb{E}[\tilde V_{kj}] = V_{kj}
$$
因此：
$$
\mathbb{E}[\tilde h_i]
= h^{\text{base}}_i + \sum_{j}\sum_{k}\sigma_k\,\mathbb{E}[\tilde U_{ik}]\,\mathbb{E}[\tilde V_{kj}]\,X_j
= h^{\text{base}}_i + \sum_{j}\sum_{k}\sigma_k U_{ik}V_{kj}X_j
$$
即
$$
\mathbb{E}[\tilde h] = h
$$
这解释了为什么 **$1/(1-p_k)$** 的 rescale 对性能很关键（论文消融也显示去掉会大幅掉点）。Yang 等 - 2025 - ImPart Importan…

> 注意：这是“期望无偏”，并不保证方差小；IMPART 的策略实质是通过让重要方向 $p_k$ 更小来降低关键分量的方差/噪声。

------

## 3) 稀疏率分配策略：从“总体稀疏率 $\alpha$”到“每个奇异向量对的 $p_k$”（核心公式）

### 3.1 总体稀疏率与压缩率定义

论文定义总体稀疏率（SR）$\alpha\in[0,1]$，压缩率（CR）为：
$$
\text{CR}=\frac{1}{1-\alpha}
$$
例如 CR=32 对应 $\alpha\approx 0.96875$。Yang 等 - 2025 - ImPart Importan…

### 3.2 重要性感知的分段函数（论文 Eq.(7)）

给定奇异值序列 $\{\sigma_k\}_{k=1}^{n}$，IMPART 用**预剪枝** + **正则化重要性映射**来生成 $p_k$：Yang 等 - 2025 - ImPart Importan…
$$
p_k=
\begin{cases}
1,& k>\lfloor n(1-\beta)\rfloor\\[4pt]
\Big(1-(\frac{\sigma_k}{\sigma_1})^{C}\Big)\cdot \gamma,& \text{otherwise}
\end{cases}
\tag{7}
$$

- $\beta$：pre-prune ratio（先把长尾小奇异值分量直接丢掉）
- $C$：正则化超参，控制“奇异值→重要性”的非线性（$C=1$ 接近线性，$C<1$ 会压平差异）
- $\gamma$：缩放系数，使得最终平均稀疏率满足目标 $\alpha$

### 3.3 如何让“平均稀疏率”精确对齐 $\alpha$（附录 Algorithm 1 逻辑）

论文给出一个可操作的过程（附录 A.1）Yang 等 - 2025 - ImPart Importan…：

- 因为同时对 $U$ 和 $V$ 稀疏化，总体稀疏率 $\alpha$ 要先转换到 $U,V$ 的目标稀疏率（论文写成近似 $\alpha \leftarrow (1+\alpha)/2$）
- 设 $r=\lfloor n(1-\beta)\rfloor$，对 $k>r$ 直接 $p_k=1$
- 先按闭式计算一个候选 $\gamma$ 并截断，避免 $p_k>1$
- 若仍达不到目标平均稀疏率，则“左移边界”：把更多靠前的 $p_k$ 置为 1，直到满足

这保证了：**总体约束优先被满足**，同时保留“重要性递增”的形状。

------

## 4) IMPART 的完整实现流程（Step-by-step，可直接落地）

下面给一个你写代码/复现实验时最需要的“最小闭环步骤”。

### Step 0：准备两套权重并计算 delta

对每个权重矩阵（线性层）：
$$
\Delta W = W_{\text{ft}} - W_{\text{base}}
$$

### Step 1：对 $\Delta W$ 做 SVD

$$
\Delta W = U\Sigma V^\top
$$

实际实现可用经济 SVD（thin SVD），并注意大矩阵的显存/CPU offload。

### Step 2：计算每个奇异向量对的稀疏率 $p_k$

输入：目标总体稀疏率 $\alpha$、超参 $\beta, C$、奇异值 $\{\sigma_k\}$

1. 先做预剪枝：$r=\lfloor n(1-\beta)\rfloor$，令 $p_k=1$ for $k>r$
2. 对 $k\le r$，用

$$
p_k=(1-(\sigma_k/\sigma_1)^C)\cdot\gamma
$$

1. 调整 $\gamma$ 与边界，确保 $\frac{1}{n}\sum_{k}p_k\approx \alpha$（论文附录 Algorithm 1）Yang 等 - 2025 - ImPart Importan…

### Step 3：对 $U,V$ 做“按列不同稀疏率”的随机 mask + 重标定

对每个 $k$：

- 为 $U$ 第 $k$ 列采样 $\xi^{(k)}$，构造
  $$
  \tilde U_{:k} = U_{:k}\odot \xi^{(k)} /(1-p_k)
  $$

- 为 $V$ 第 $k$ 列（或 $V^\top$ 第 $k$ 行）采样 $\eta^{(k)}$，构造
  $$
  \tilde V_{:k} = V_{:k}\odot \eta^{(k)} /(1-p_k)
  $$

### Step 4：重构稀疏 delta 并写回模型

$$
\Delta W_c = \tilde U\Sigma \tilde V^\top
$$

最终部署可存储稀疏 $\tilde U,\tilde V$（以及 $\Sigma$ 与 seed），推理时组合：
$$
W_{\text{deploy}} = W_{\text{base}} + \Delta W_c
$$

### Step 5（工程细节）：避免存 mask 的额外开销

论文提出用**确定性 seeding**重建 mask：用 $\sigma_k$（及 $\sigma_k+1$）作为随机种子来生成 $\xi,\eta$，从而不需要存储显式 mask。Yang 等 - 2025 - ImPart Importan…

------

## 5) 这篇论文能启发的 idea（不展开结合细节，只给方向与可写成公式的切入点）

### Idea 1：从“奇异值=重要性”升级到“任务敏感度=重要性”

IMPART 用 $\sigma_k$ 作为重要性 proxy。可以改成更直接的任务敏感度指标 $I_k$，例如基于梯度或 Fisher：
$$
I_k \;\propto\; \mathbb{E}\big[\|\nabla_{\Delta W}\mathcal{L}\cdot (u_k v_k^\top)\|\big]
\quad\text{或}\quad
I_k \;\propto\; (u_k v_k^\top)^\top F (u_k v_k^\top)
$$
然后把 $p_k$ 从 $\sigma_k$ 驱动改为 $I_k$ 驱动：
$$
p_k = f(I_k;\ \gamma,\beta,\ldots)
$$

### Idea 2：把“无偏（unbiased）”从输出期望扩展到“最小方差无偏估计”

IMPART 的 rescale 让 $\mathbb{E}[\tilde h]=h$，但不同 $p_k$ 会带来方差差异。可以考虑直接最小化方差上界（示意）：
$$
\min_{\{p_k\}}\ \mathrm{Var}(\tilde h)\quad
\text{s.t.}\ \frac{1}{n}\sum_k p_k = \alpha
$$
得到类似“按贡献分配保留率”的水位式规则（会很像重要性采样的最优分配）。

### Idea 3：将随机稀疏化替换为结构化稀疏（更友好推理加速）

现在的 mask 是元素级随机，推理加速不稳定。可改为块/行列/分组稀疏：
$$
\tilde U_{:k} = U_{:k}\odot M^{(k)}_{\text{block}}
$$
其中 $M^{(k)}_{\text{block}}$ 保证连续块、固定块大小，便于 kernel 融合与压缩存储。

### Idea 4：学习型 $p_k$（从验证集网格到可微分/自适应）

论文用验证集调 $\beta,C$ 并通过 $\gamma$ 达到目标稀疏。可把 $p_k$ 参数化为可学习函数（例如单调网络/分段线性），用少量校准步直接优化：
$$
\min_{\phi}\ \mathcal{L}\big(W_{\text{base}} + \mathrm{ImPart}(\Delta W;\ p_k(\sigma_k;\phi))\big)
\quad\text{s.t.}\ \frac{1}{n}\sum_k p_k = \alpha
$$

### Idea 5：把“pre-prune 长尾奇异分量”的规则变成自适应阈值

目前 $r=\lfloor n(1-\beta)\rfloor$ 是按比例。可改成按谱能量阈值：
$$
r=\min\Big\{t:\ \frac{\sum_{k=1}^{t}\sigma_k^2}{\sum_{k=1}^{n}\sigma_k^2}\ge \tau\Big\}
$$
这样在不同层/不同任务上更稳健。