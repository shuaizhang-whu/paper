# FLAT-LLM：Fine-grained low-rank activation space transformation for large language model compression

它的核心思想是：**不要对权重做 SVD 两矩阵分解**（会引入两次 matmul、GPU 不友好，还要存 U/V），而是利用 **注意力里 value→output 的连续计算结构**，在**激活空间**做 PCA 低维变换，并把投影基 **吸收到相邻权重里**，从而做到：训练免、校准快、结构更硬件友好。Tian 等 - 2025 - FLAT-LLM Fine-g…

------

## 1）原始优化目标：低秩压缩下尽量保持 MHA 输出，且推理结构要高效

在 MHA 中，对每个 head $h$（隐藏维 $d_{\text{hid}}$，head 维 $d_h$）：
$$
A^h=\frac{(XW_q^{h\top})(XW_k^{h\top})^\top}{\sqrt{d_h}},\quad
Y_v^h=\mathrm{Softmax}(A^h)\,XW_v^{h\top}
$$
以及输出投影：
$$
Y_o^h = Y_v^h W_o^{h\top}=\mathrm{Softmax}(A^h)\,XW_v^{h\top}W_o^{h\top}
$$
多头汇总：
$$
Y_o=\sum_{h=1}^H Y_o^h
$$
（论文 Eq.(1)(2)）Tian 等 - 2025 - FLAT-LLM Fine-g…

如果你把压缩理解为“把 $W_v^h$ 与 $W_o^h$ 变小”，一个直接目标是保持 attention 输出：
$$
\min\ \mathbb{E}_{X}\|Y_o(X)-\tilde Y_o(X)\|_F^2
$$
但常规 SVD 方案通常会把一个矩阵乘拆成两次（$UV^\top$），反而降低 GPU 效率；并且对方阵想显著省参数必须截掉 $\ge 50\%$ 奇异值，信息损失大（引言讨论）。Tian 等 - 2025 - FLAT-LLM Fine-g…

------

## 2）FLAT 的改进目标：在“value 输出激活子空间”里做低秩变换，并吸收进权重，避免 SVD 结构低效

FLAT 的核心改写是引入一个投影算子 $P^h\approx I$，但它来自 **value 输出 $Y_v^h$** 的 PCA 子空间，而不是权重的 SVD：
$$
Y_o^h
=
\mathrm{Softmax}(A^h)\,XW_v^{h\top}\,\underbrace{P^h}_{\approx I}\,W_o^{h\top}
$$
然后把 $P^h$ 截断成 rank-$r$ 的正交投影：
$$
P^h \approx \tilde Q_v^h \tilde Q_v^{h\top}
$$
并把 $\tilde Q_v^h$ 吸收进 value / output 权重里，使得新的权重维度变小，且仍是**一次 value 投影 + 一次 output 投影**（而不是 SVD 两段乘替换单段乘）。Tian 等 - 2025 - FLAT-LLM Fine-g…

------

## 3）关键方法一：Head-wise PCA-based Weight Truncation（细粒度、按 head 做 PCA）

### 3.1 PCA 的基本式

对数据矩阵 $Z\in\mathbb{R}^{N\times d}$，PCA 对协方差
$$
C=Z^\top Z = Q\Lambda Q^\top
$$
保留 top-$r$ 特征向量 $\tilde Q\in\mathbb{R}^{d\times r}$，重构为：
$$
\tilde Z = Z\tilde Q \tilde Q^\top
$$
（论文 Eq.(3)(4)）Tian 等 - 2025 - FLAT-LLM Fine-g…

### 3.2 在注意力 head 上做 PCA：对 $Y_v^h$ 的协方差分解

对每个 head $h$，收集 $M$ 个校准样本得到 value 输出 $Y_{v,m}^h$，构造协方差：
$$
C_v^h=\sum_{m=1}^{M} Y_{v,m}^{h\top}Y_{v,m}^{h}
$$
PCA 得到正交基 $Q_v^h\in\mathbb{R}^{d_h\times d_h}$，截断：
$$
\tilde Q_v^h = Q_v^h[:,1:r]\in\mathbb{R}^{d_h\times r}
$$
于是 value 输出在低维子空间的最优投影为：
$$
\tilde Y_v^h = Y_v^h \tilde Q_v^h \tilde Q_v^{h\top}
$$
（论文 4.1 节、Eq.(5)(6)(9)）Tian 等 - 2025 - FLAT-LLM Fine-g…

### 3.3 “吸收”投影基：把 $\tilde Q_v^h$ 合并进 $W_v^h,W_o^h$

从原式
$$
Y_o^h=\mathrm{Softmax}(A^h)\,XW_v^{h\top}W_o^{h\top}
$$
插入恒等分解 $I = Q_v^h Q_v^{h\top}$：
$$
Y_o^h=\mathrm{Softmax}(A^h)\,XW_v^{h\top}Q_v^h Q_v^{h\top}W_o^{h\top}
$$
然后做截断近似 $Q_v^h Q_v^{h\top}\approx \tilde Q_v^h\tilde Q_v^{h\top}$，得到：
$$
\tilde Y_o^h
=
\mathrm{Softmax}(A^h)\,XW_v^{h\top}\tilde Q_v^h\tilde Q_v^{h\top}W_o^{h\top}
$$
把两侧吸收成新权重：
$$
\tilde Y_o^h
=
\mathrm{Softmax}(A^h)\,X\tilde W_v^{h\top}\tilde W_o^{h\top}
$$
其中维度变为：
$$
\tilde W_v^{h}\in\mathbb{R}^{r\times d_{\text{hid}}},\quad
\tilde W_o^{h}\in\mathbb{R}^{d_{\text{hid}}\times r}
$$
（论文 Eq.(5)(6)）Tian 等 - 2025 - FLAT-LLM Fine-g…

这个结构的关键优点：

- 不需要存 SVD 的左/右奇异向量对（不像 $U\Sigma V^\top$）；
- 压缩率与保留 rank 比例更直接对应；
- 计算仍保持“attention head 的两次线性投影”，更贴近原生 kernel 形态（论文强调推理更快）。Tian 等 - 2025 - FLAT-LLM Fine-g…

------

## 4）关键方法二：训练免的层间 rank 分配（IPRS：Importance-Preserving Rank Selection）

FLAT 观察到不同 decoder 层的“内在维度/可压缩性”不同，因此不应统一 rank。它用 **输入/输出隐状态的夹角偏离**作为层重要性分数。

### 4.1 层重要性分数：用输入输出 cosine similarity 的角度

对第 $l$ 层 decoder，输入/输出隐状态矩阵为 $X_l,X_{l+1}$。对每个 token 行向量 $p$：
$$
c_{l,p}=\frac{X_{l,p}^\top X_{l+1,p}}{\|X_{l,p}\|_2\|X_{l+1,p}\|_2}
$$
取平均：
$$
c_l=\mathbb{E}_{X,p}[c_{l,p}]
$$
把相似度映射到“角度偏离”（归一化到 $[0,1]$）作为重要性：
$$
t_l=\frac{\arccos(c_l)}{\pi}
$$
直觉：输入输出越“不对齐”（角度偏离越大），说明该层做了更多表征变换，更“重要/不该压太狠”。（论文 4.2 节，Eq.(7) 附近）Tian 等 - 2025 - FLAT-LLM Fine-g…

### 4.2 目标：在预算约束下，让分配尽量接近比例分配且满足 $w_l\in[0,1]$

设总剩余 rank 比例预算：
$$
B=L(1-s)
$$
其中 $s$ 是全局稀疏/压缩比例（保留比例是 $1-s$），$L$ 是 decoder 层数。朴素比例分配：
$$
\hat w_l=\frac{t_l}{\sum_{j=1}^{L}t_j}\,B
$$
但可能出现 $\hat w_l>1$。于是他们把问题写成一个“带盒约束与和约束”的投影问题：
$$
\min_{w\in[0,1]^L}\ \|w-\hat w\|
\quad\text{s.t.}\quad \sum_{l=1}^{L}w_l=B
$$
（论文 Eq.(8)）Tian 等 - 2025 - FLAT-LLM Fine-g…

### 4.3 贪心重分配算法（Algorithm 1）

做法很像“water-filling / capped simplex projection”的贪心实现：

- 初始化 active set $A=\{1,\dots,L\}$，预算 $B=L(1-s)$

- 在 $A$ 内按比例重新计算临时分配
  $$
  \tilde w_l=\frac{t_l}{\sum_{j\in A}t_j}\,B,\quad l\in A
  $$

- 找到超上界集合 $S=\{l\in A\mid \tilde w_l>1\}$，把这些层钉死为 $w_l=1$，从预算里扣掉并移出 active set

- 直到没有超上界项，再把剩余的 $\tilde w_l$ 全部赋值结束。Tian 等 - 2025 - FLAT-LLM Fine-g…

最后得到每层剩余比例 $w_l$，再映射到每个 head 的保留维度 $r_l=w_l\cdot d_h$（或四舍五入到可实现的整数）。

------

## 5）关键理论：截断误差与 PCA 特征值的“直接映射”（保证方法有效）

FLAT 给了一个非常干净的误差公式：对单个 head，把 $Y_v^h$ 投影到 top-$r$ PCA 子空间的最优重构误差满足：

**定理（单 head）**
 若
$$
Y_v^{h\top}Y_v^h = Q_v^h\Lambda_v^h Q_v^{h\top},\quad \lambda_1^h\ge\cdots\ge \lambda_{d_h}^h\ge 0
$$
且 $\tilde Y_v^h=Y_v^h\tilde Q_v^h\tilde Q_v^{h\top}$，则：
$$
\|Y_v^h-\tilde Y_v^h\|_F^2=\sum_{i=r+1}^{d_h}\lambda_i^h
$$
（论文 Theorem 4.1）Tian 等 - 2025 - FLAT-LLM Fine-g…

**推论（多 head）**
 拼接所有 head 的 value 输出 $Y_v=\mathrm{concat}(Y_v^1,\dots,Y_v^H)$ 时：
$$
\|Y_v-\tilde Y_v\|_F^2=\sum_{h=1}^{H}\sum_{i=r+1}^{d_h}\lambda_i^h
$$
（论文 Corollary 4.2）Tian 等 - 2025 - FLAT-LLM Fine-g…

这说明：**丢掉的特征值之和就是你在校准激活上的最小重构损失**，因此 rank 的选择可以直接用谱能量控制（和 SVD-LLM “截断值对应重构损失”的精神很像，但对象从权重奇异值换成激活协方差特征值）。

------

## 6）Step 实现过程（从优化目标到可执行 pipeline）

### Step 0：校准数据准备

采样少量序列作为校准集（论文实验常见 128～256 条，长序列），跑前向，缓存必要的 $Y_v^h$、以及各层输入输出隐状态 $X_l,X_{l+1}$。Tian 等 - 2025 - FLAT-LLM Fine-g…

### Step 1：计算层重要性并做 IPRS 分配（得到每层保留比例 $w_l$）

1. 对每层计算 $c_l$、$t_l=\arccos(c_l)/\pi$
2. 设全局压缩比 $s$，预算 $B=L(1-s)$
3. 用 Algorithm 1 贪心投影得到 $w_l\in[0,1]$，满足 $\sum_l w_l=B$
4. 把 $w_l$ 映射到该层各 head 的 rank（通常同层 head 共享同一 $r_l$）。Tian 等 - 2025 - FLAT-LLM Fine-g…

### Step 2：对每层每个 head 做 PCA 并吸收进权重（压缩 MHA 的 $W_v,W_o$）

对层 $l$、head $h$：

1. 计算协方差 $C_v^{l,h}=\sum_m Y_{v,m}^{l,h\top}Y_{v,m}^{l,h}$

2. 特征分解 $C=Q\Lambda Q^\top$，取 $\tilde Q=Q[:,1:r_l]$

3. 更新权重：
   $$
   \tilde W_v^{l,h\top} = W_v^{l,h\top}\tilde Q,\qquad
   \tilde W_o^{l,h\top} = \tilde Q^\top W_o^{l,h\top}
   $$
   （等价于在计算图里插入 $\tilde Q\tilde Q^\top$ 再吸收）

4. 替换原 head 的 value / output 投影维度为 $r_l$。Tian 等 - 2025 - FLAT-LLM Fine-g…

### Step 3：MLP 压缩（论文用 Nyström + ridge leverage scores 的结构化近似）

这部分不是你问的“低秩 SVD 主线”，但 FLAT 也做了：对 MLP 的 up/down 投影用 Nyström 近似，并用 ridge leverage score 选通道（附录 C）。如果你后面要把它当作“补全方案”，再单独展开即可。Tian 等 - 2025 - FLAT-LLM Fine-g…

### Step 4：部署与推理优势

- 结构上减少了 value head 输出维度 $d_h\to r_l$，不仅减少参数，也减少 KV cache 激活内存；
- 没有额外 adapter（对比 SliceGPT 的 residual adapter）；
- 无恢复微调（training-free）。Tian 等 - 2025 - FLAT-LLM Fine-g…

------

## 7）与 SVD-LLM v2 / SAES-SVD 的主要理论差异（抓住“对象”与“误差”）

1. **分解对象不同**

- SVD-LLM / SAES-SVD：在权重矩阵 $W$ 上做 SVD/截断，并处理截断误差传播；
- FLAT-LLM：在激活空间（具体是 $Y_v^h$ 的协方差）做 PCA，得到投影基后“吸收进权重”，本质是**降维变换**而不是权重低秩因子化。Tian 等 - 2025 - FLAT-LLM Fine-g…

1. **误差度量不同**

- SVD-LLM v2：强调奇异值截断与重构损失/输出误差的联系（truncation-aware）；
- SAES-SVD：强调局部误差 + 累计误差抑制；
- FLAT：给出 $\|Y_v-\tilde Y_v\|_F^2=\sum \lambda_{\text{dropped}}$ 的直接映射，但这个误差是在 **value 输出激活**上定义的，并且是 head-wise 可加的。Tian 等 - 2025 - FLAT-LLM Fine-g…

1. **推理结构差异**

- SVD 权重分解通常把一个 GEMM 变两次 GEMM，GPU 未必更快；
- FLAT 通过“降 head 维度 + 吸收”保持更原生的注意力投影结构，并减少 KV cache，速度/显存更占优。Tian 等 - 2025 - FLAT-LLM Fine-g…

------

## 8）一些可延展的 idea（不展开到详细融合方案）

1. **把 SAES-SVD 的“误差传播风险”用来改进 IPRS 的重要性 $t_l$**
    FLAT 现在用输入输出夹角 $t_l=\arccos(c_l)/\pi$ 做层重要性。可以尝试把“该层压缩误差会如何向后传播”的风险（SAES 的累计误差视角）并入 $t_l$，让分配从“表征变化大就重要”变成“传播后果大就重要”。
2. **把 SVD-LLM v2 的 truncation-aware 逻辑迁移到 PCA 的特征谱上**
    既然 FLAT 的误差与 dropped eigenvalues 和直接绑定，可以在 rank 选择时引入更强的“误差预算”形式：

$$
\min_{r_l}\ \sum_l \mathrm{Cost}(r_l)
\quad \text{s.t.}\quad
\sum_{h}\sum_{i>r_l}\lambda_i^{l,h}\le \epsilon
$$

再用类似 v2 的层间权重/重要性对 $\epsilon$ 做分配。

1. **把 head-wise PCA 从“每层统一 r”提升到“每 head 自适应 r_{l,h}”**
    他们目前以 decoder-wise 为主（同层 head 同 rank）。你可以用每个 head 的特征谱衰减速度（或 dropped-eigen loss）做更细粒度分配：

$$
\min_{\{r_{l,h}\}}\sum_{l,h}\sum_{i>r_{l,h}}\lambda_i^{l,h}
\quad \text{s.t.}\quad \sum_{l,h} r_{l,h}\le R
$$

这会让“真正低秩的 head”更激进，难压的 head 保留更多维度。

1. **把投影基的估计从 PCA 升级为“带任务权重的加权 PCA”**
    当前协方差是 $\sum Y^\top Y$。可以把 token 重要性（比如 loss、梯度范数）作为权重 $\omega_m$：

$$
C=\sum_m \omega_m\,Y_{m}^\top Y_{m}
$$

让投影子空间更偏向“关键 token 分布”。

1. **将 FLAT 的激活降维用于“先降维再 SVD”**
    一个直觉组合：先用 FLAT 把某些子模块维度降下来（尤其是 attention value/out），再对剩余大矩阵做 SVD-LLM v2/SAES 的低秩，可能在相同精度下带来更高的整体压缩上限。