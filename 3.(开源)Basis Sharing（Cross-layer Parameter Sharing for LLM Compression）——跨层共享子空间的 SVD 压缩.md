# 论文分析：Basis Sharing（Cross-layer Parameter Sharing for LLM Compression）——跨层共享子空间的 SVD 压缩 （代码开源）

这篇论文做的事情可以一句话概括：**把“每层单独做 SVD 低秩近似”改为“同类型矩阵跨层拼接后做一次 SVD”，让多层共享一组 basis（基向量/基矩阵），每层只保留自己的系数矩阵，从而在同等压缩率下减小误差并提升性能**。论文同时借鉴 SVD-LLM 的思想，用输入二阶矩构造缩放/白化矩阵 $S$，让分解更贴近推理误差而不是纯权重误差。Wang 等 - 2024 - Basis sharing C…

------

## 1) 基线：逐层独立 SVD 的优化目标（从目标公式出发）

### 1.1 经典低秩逼近（权重空间）

对第 $\ell$ 层某个权重矩阵 $W^{(\ell)}\in\mathbb{R}^{d_1\times d_2}$，逐层独立的低秩分解通常等价于：
$$
\min_{\mathrm{rank}(\hat W^{(\ell)})\le k}\ \|W^{(\ell)}-\hat W^{(\ell)}\|_F^2
\quad\Rightarrow\quad
\hat W^{(\ell)} = [W^{(\ell)}]_k
$$
也可写成双因子形式：
$$
\min_{A^{(\ell)}\in\mathbb{R}^{d_1\times k},\ B^{(\ell)}\in\mathbb{R}^{k\times d_2}}
\ \|W^{(\ell)}-A^{(\ell)}B^{(\ell)}\|_F^2
\tag{B0}
$$

### 1.2 “推理关注的是 $XW$”：计算误差视角（论文图 3 的动机）

论文强调：推理误差更接近
$$
\|X(W-\hat W)\|_F
$$
而不是 $\|W-\hat W\|_F$。并给了一个示意（图 3）：即便 $\|\Delta W_1\|_F < \|\Delta W_2\|_F$，也可能出现 $\|X\Delta W_1\|_F > \|X\Delta W_2\|_F$。Wang 等 - 2024 - Basis sharing C…
 这为后续引入缩放矩阵 $S$（输入统计）做铺垫。

------

## 2) 方法一：跨层共享 basis（从“逐层”到“跨层联合”的目标变化）

### 2.1 表征：多层同类型权重共享一个 basis $B$，每层系数不同

选择 $n$ 个层的同类型矩阵（例如所有层的 $W_K$），记：
$$
W^{(1)},\dots,W^{(n)},\quad W^{(i)}\in\mathbb{R}^{d_1\times d_2}
$$
将它们横向拼接为：
$$
W = [W^{(1)}\ W^{(2)}\ \cdots\ W^{(n)}]\in\mathbb{R}^{d_1\times (nd_2)}
$$
对拼接矩阵做截断 SVD：
$$
W \approx W_k = U_k\Sigma_k V_k^\top
$$
定义
$$
B = U_k\Sigma_k\in\mathbb{R}^{d_1\times k},\qquad C = V_k^\top\in\mathbb{R}^{k\times (nd_2)}
$$
于是对任意层 $i$ 的任意列 $j$，有论文给出的“共享 basis 线性组合”形式：
$$
W^{(i)}_{:,j}\approx \sum_{m=1}^{k} B_{:,m}\,C^{(i)}_{m,j}
\tag{1}
$$
其中 $C^{(i)}\in\mathbb{R}^{k\times d_2}$ 是从 $C$ 按列切分得到的该层系数矩阵。Wang 等 - 2024 - Basis sharing C…

### 2.2 从优化角度写清楚“改进后的目标”

上述过程等价于把原来 $n$ 个独立问题 (B0) 合并成一个联合问题：
$$
\min_{\mathrm{rank}(\hat W)\le k}\ \|W-\hat W\|_F^2
\quad
\text{where } W=[W^{(1)}\cdots W^{(n)}],\ \hat W=[\hat W^{(1)}\cdots \hat W^{(n)}]
\tag{B1}
$$
对比逐层独立：
$$
\sum_{i=1}^n \min_{\mathrm{rank}(\hat W^{(i)})\le k_i}\ \|W^{(i)}-\hat W^{(i)}\|_F^2
$$
Basis Sharing 的改变在于：**强制所有层共享同一个列空间（basis 子空间）**，但允许每层系数不同，从而减少重复存储的子空间信息，并在层间相似时可能降低总误差。Wang 等 - 2024 - Basis sharing C…

------

## 3) 方法二：引入输入统计的缩放矩阵 $S$（从权重误差到计算误差的改进）

论文在 3.1 节把输入影响编码进分解：对拼接矩阵做左侧缩放（等价于在某种度量下做低秩）：
$$
W = S^{-1}(SW)
\tag{2}
$$
不是直接对 $W$ 做 SVD，而是对 $SW$ 做 SVD：
$$
SW \approx U_k'\Sigma_k' V_k'^\top = B'C'
$$
再把 $S^{-1}$ 吸收回 basis：
$$
W \approx S^{-1}B'C' = B''C'
\tag{3}
$$
其中 $B''=S^{-1}B'$ 是最终共享 basis。Wang 等 - 2024 - Basis sharing C…

### 3.1 $S$ 的构造：跨层输入激活拼接的二阶矩（借鉴 SVD-LLM）

论文直接写：借鉴 SVD-LLM 里的做法，用 Cholesky 得到：
$$
S S^\top = \mathrm{cholesky}(X^\top X)
$$
区别在于：这里的 $X$ 不是单层输入，而是把组内多层输入 $X^{(1)},\dots,X^{(n)}$ **纵向拼接**后再算 $X^\top X$，让 $S$ 反映“多层共同的输入重要方向”。Wang 等 - 2024 - Basis sharing C…
 论文还给出具体实验设置：用 WikiText-2 的 256 个样本、每个 2048 tokens 来估计 $X$（用于求 $S$）。Wang 等 - 2024 - Basis sharing C…

> 从优化角度看，这相当于把目标从 (B1) 改为一个加权度量下的低秩：

$$
\min_{\mathrm{rank}(\hat W)\le k}\ \|S(W-\hat W)\|_F^2
$$

这更接近“输入加权的误差”，在推理上更合理（与论文图 3 的动机一致）。Wang 等 - 2024 - Basis sharing C…

------

## 4) “选哪些矩阵能共享、哪些层该分组”——以 Frobenius loss 作为判据（方法落地关键）

### 4.1 不同类型矩阵是否适合共享（3.2 节）

论文按 decoder-only Transformer 中典型矩阵类型讨论：$W_K,W_Q,W_V,W_O,W_{Up},W_{Gate},W_{Down}$。Wang 等 - 2024 - Basis sharing C…

- **$W_{Down}$**：不适合共享。理由是高维→低维投影矩阵横向拼接后秩更大，同压缩率下对拼接矩阵做 SVD 会带来更大的 Frobenius loss。Wang 等 - 2024 - Basis sharing C…
- 对 **$W_K,W_Q,W_V,W_{Up},W_{Gate}$**：共享常常能降低 loss（论文用两层共享的热力图展示，例如 $W_K$ 的 9/10 层共享时 loss 小于分别压缩 loss 之和）。Wang 等 - 2024 - Basis sharing C…
- 对 **$W_O$**：共享往往增大 loss，因此不建议共享，改为逐层单独压缩。Wang 等 - 2024 - Basis sharing C…

（这部分对应论文 Fig.4 与附录热力图 Fig.7/8 的结论。）Wang 等 - 2024 - Basis sharing C…

### 4.2 共享层如何分组（3.3 节）

分组准则是：共享后的 Frobenius loss **不应大于** 不共享（分别压缩）的 loss 之和。观察到相邻层更稳，因此采用“按顺序相邻成组”（如 1-2,3-4,…）的策略。Wang 等 - 2024 - Basis sharing C…
 论文还在 4.3 节评估了组大小（2/3/4/…/32 层共享）对性能的影响，发现压缩率越大，组越大越容易崩；高压缩时 2 层共享更稳。Wang 等 - 2024 - Basis sharing C…

------

## 5) Step-by-step 实现流程（从优化目标到实际压缩管线）

下面给一套与论文描述一致的最小实现流程（以“某类矩阵 + 若干层为一组”为单位）：

### Step 0：确定共享配置

- 选矩阵类型集合：$\{W_K,W_Q,W_V,W_{Up},W_{Gate}\}$（论文建议）；$W_O$ 与 $W_{Down}$ 不做共享（或特别处理）。Wang 等 - 2024 - Basis sharing C…
- 选分组大小 $g$：例如默认 $g=2$（相邻两层一组）。Wang 等 - 2024 - Basis sharing C…

### Step 1：收集校准输入激活并构造组输入

对每个组内层 $\ell$，收集该矩阵对应的输入激活 $X^{(\ell)}$（用一小批校准数据 forward）。
 纵向拼接：
$$
X_{\text{grp}}=
\begin{bmatrix}
X^{(\ell_1)}\\
\vdots\\
X^{(\ell_g)}
\end{bmatrix}
$$

### Step 2：计算缩放矩阵 $S$

$$
SS^\top=\mathrm{chol}(X_{\text{grp}}^\top X_{\text{grp}})
$$

实现上常加 $\varepsilon I$ 以稳定 Cholesky（论文提到计算 $S$ 使用 FP64 提升精度）。Wang 等 - 2024 - Basis sharing C…

### Step 3：构造拼接权重并缩放

横向拼接组内同类型矩阵：
$$
W_{\text{grp}}=[W^{(\ell_1)}\ \cdots\ W^{(\ell_g)}]
$$
缩放：
$$
\bar W = S W_{\text{grp}}
$$

### Step 4：对 $\bar W$ 做截断 SVD 得到共享 basis 与系数

$$
\bar W \approx U_k'\Sigma_k'V_k'^\top
$$

### Step 5：还原到推理可用的共享表示

$$
B''=S^{-1}B'
$$

最终共享表示：
$$
W_{\text{grp}}\approx B''C'
$$
将 $C'$ 沿列切分回每层 $C^{(\ell)}$。

### Step 6：部署形态（两层线性替换）

对组内每层：
$$
W^{(\ell)} \approx B''C^{(\ell)}
$$
推理时：
$$
y = W^{(\ell)}x \approx B''(C^{(\ell)}x)
$$
等价把原线性层拆成两层：先 $C^{(\ell)}$（降维/变换），再共享 $B''$（升维/输出）。Wang 等 - 2024 - Basis sharing C…

### Step 7：按目标压缩率求 $k$（论文附录 A.1 给出闭式）

若组内共有 $n$ 个矩阵，每个大小 $d_1\times d_2$，压缩到原来的 $x\%$，共享后参数量为 $d_1k + kd_2n$，原始为 $nd_1d_2$，因此：
$$
d_1k + kd_2n = nd_1d_2\cdot x\%
\Rightarrow
k=\frac{nd_1d_2\cdot x\%}{d_1 + nd_2}
$$
Wang 等 - 2024 - Basis sharing C…

------

## 6) 这篇论文能启发的一些 idea（不展开结合建议）

### Idea 1：把“共享组选择”从经验相邻分组升级为“子空间相似度/谱距离”驱动的自动聚类

对每层矩阵取 top-$t$ 左奇异子空间 $U_t^{(\ell)}$，定义距离：
$$
d(\ell_a,\ell_b)=\|U_t^{(\ell_a)}U_t^{(\ell_a)\top}-U_t^{(\ell_b)}U_t^{(\ell_b)\top}\|_F
$$
用聚类得到共享组，可能比固定相邻层更好。

### Idea 2：共享不必是整矩阵：按 head / block 共享

注意力投影矩阵天然可按 head 切块。可对每个 head 子块跨层共享 basis，从而获得更细粒度的“共享强度控制”。

### Idea 3：共享 basis 的同时给每层一个极小“补偿项”提高高压缩率鲁棒性

例如：
$$
W^{(\ell)} \approx B C^{(\ell)} + \Delta^{(\ell)},\quad \mathrm{rank}(\Delta^{(\ell)})\le r_\Delta\ \text{很小}
$$
把共享造成的系统偏差集中在一个很小的自由度上。

### Idea 4：把缩放矩阵 $S$ 从“组内统一”扩展为“分块/分通道缩放”

目前 $S$ 对整个 $d_1$ 空间统一缩放。可考虑 block-diagonal 的 $S$，兼顾稳定性与表达能力：
$$
S=\mathrm{diag}(S_1,S_2,\dots,S_b)
$$
每块用对应输入二阶矩估计。

### Idea 5：将“共享收益”的判据从 $\|W-\hat W\|_F$ 迁移到近似计算误差的二阶形式

用输入二阶矩 $H=X^\top X$ 写：
$$
\|X(W-\hat W)\|_F^2
= \mathrm{tr}\big((W-\hat W)^\top H (W-\hat W)\big)
$$
直接用该量作为“是否共享/组多大”的判据，会更贴近推理性能。