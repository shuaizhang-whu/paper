# (开源)AdaSVD（Adaptive Singular Value Decomposition for Large Language Models）

AdaSVD 主要解决两件事：

1. **高压缩率下 SVD 截断误差很大**：截断后不只要“选对 rank”，还要对保留下来的 $U,V^\top$ 做**后补偿（adaComp）**；
2. **不同层重要性不同**：不应该所有层用同一个压缩率，而应做**层级自适应压缩率分配（adaCR）**。

------

## 1) 从基线目标到改进目标：AdaSVD 的理论主线（含公式）

### 1.1 基线：vanilla SVD 截断（权重空间最优）

对线性层权重 $W$，做 SVD：
$$
W = U\Sigma V^\top
$$
截断保留 top-$k$ 奇异值：
$$
W_c = U_k\Sigma_k V_k^\top
\tag{1}
$$
并将 $\Sigma_k$ 吸收进两侧（便于把线性层拆成两层）：
$$
U_k^\sigma = U_k\Sigma_k^{1/2},\quad V_k^\sigma = V_k\Sigma_k^{1/2}
\tag{2}
$$
vanilla SVD 等价于最小化 $\|U_k^\sigma(V_k^\sigma)^\top - W\|_F^2$，但它忽略了推理时真正影响误差的是 **$WX$**（输入激活驱动）。

------

### 1.2 改进目标：用“输出误差/计算误差”定义 SVD 压缩误差

AdaSVD 将压缩误差定义为：
$$
\mathcal{L}_{\text{SVD}}
=
\|WX - W_cX\|_F^2
=
\|U_k^\sigma (V_k^\sigma)^\top X - WX\|_F^2
\tag{4}
$$
并提出：截断后的 $U_k^\sigma, V_k^\sigma$ 不应保持不变，而应继续优化，使其更好地拟合 $WX$：
$$
(U_k^\sigma, (V_k^\sigma)^\top)
=
\arg\min_{U_k^\sigma,(V_k^\sigma)^\top}
\|U_k^\sigma (V_k^\sigma)^\top X - WX\|_F^2
\tag{5}
$$
这就是 adaComp 的优化起点。

------

## 2) adaComp：截断后“交替更新 $U,V^\top$”的补偿推导与稳定化

### 2.1 直接求偏导得到闭式更新（但数值不稳定）

对式(4)分别对 $U_k^\sigma$ 与 $(V_k^\sigma)^\top$ 令偏导为 0，论文给出闭式形式：
$$
\frac{\partial \mathcal{L}_{\text{SVD}}}{\partial U_k^\sigma}=0
\Rightarrow
U_k^\sigma
=
WX X^\top V_k^\sigma\Big((V_k^\sigma)^\top XX^\top V_k^\sigma\Big)^{-1}
\tag{6}
$$
问题：显式求逆容易不稳定（尤其高压缩率、校准数据少、矩阵病态时）。

------

### 2.2 稳定化策略 1：把更新 $U$ 变成最小二乘 + Moore–Penrose 伪逆

论文将更新 $U_k^\sigma$ 重写成 LSE（Least Squares Estimation）形式：Li 等 - 2025 - AdaSVD Adaptive S…

令
$$
A = X^\top V_k^\sigma,\quad B=(WX)^\top
$$
则更新 $U_k^\sigma$ 变为：
$$
U_k^\sigma = \arg\min_{U_k^\sigma}\ \|A(U_k^\sigma)^\top - B\|_F^2
\tag{8}
$$
因为 $A$ 往往非方阵/可能不满秩，先对 $A$ 做 SVD：
$$
A = U_A\Sigma_A V_A^\top
\tag{9}
$$
并用 Moore–Penrose 伪逆 $A^+ = V_A\Sigma_A^+U_A^\top$ 解：
$$
U_k^\sigma = (A^+B)^\top = (V_A\Sigma_A^+U_A^\top B)^\top
\tag{10}
$$
其中
$$
\Sigma_A^+=\mathrm{diag}(\sigma_1^{-1}\mathbf{1}_{\sigma_1\ne0},\dots,\sigma_n^{-1}\mathbf{1}_{\sigma_n\ne0})
\tag{11-12}
$$
这一步的意义：用伪逆替代“硬求逆”，缓解病态数值问题，使更新更平滑、更稳定。

------

### 2.3 稳定化策略 2：更新 $V^\top$ 也用伪逆

对固定 $U_k^\sigma$ 的情形，论文给出同样的伪逆更新：Li 等 - 2025 - AdaSVD Adaptive S…
$$
(V_k^\sigma)^\top
=
\arg\min_{(V_k^\sigma)^\top}\ \|U_k^\sigma (V_k^\sigma)^\top X - WX\|_F^2
=
\big((U_k^\sigma)^+\big)^\top W
\tag{13}
$$

------

### 2.4 交替更新（Alternating Update）形成 adaComp

用上述两条伪逆更新规则交替迭代（论文给出序列形式）：
$$
(U_k^\sigma)^{1}\rightarrow ((V_k^\sigma)^\top)^{1}
\rightarrow (U_k^\sigma)^{2}\rightarrow ((V_k^\sigma)^\top)^{2}
\rightarrow \cdots \rightarrow (U_k^\sigma)^{\tau}\rightarrow ((V_k^\sigma)^\top)^{\tau}
\tag{16}
$$
直观理解：这等价于在 rank-$k$ 因子空间上做一个“无训练的最小二乘拟合”，把截断后导致的输出偏差往回拉。

------

## 3) Stack-of-Batch（SOB）：在显存受限下“用更多校准数据”的统计重整

adaComp 的更新依赖校准激活 $X$。但论文指出在大模型上直接堆更多样本会爆显存，于是提出 SOB：把 $N$ 个样本打乱并压成 $M$ 个 bucket，每个 bucket 取均值当作新的校准样本：Li 等 - 2025 - AdaSVD Adaptive S…
$$
X_{\text{rand}} = \mathrm{Shuffle}(X)
\tag{14}
$$
其中 $\text{mini\_bsz}=\lceil N/M\rceil$。
 效果：在不增加显存的情况下，让校准统计更稳定，进而让伪逆更新更可靠。

------

## 4) adaCR：层级重要性驱动的自适应压缩率分配（从公式到可执行规则）

### 4.1 层重要性定义：输入与输出的相似度

对某层权重 $W$，输入激活 $X$，输出 $Y=WX$：
$$
Y = WX
\tag{17}
$$
论文用 cosine similarity 做 similarity（实现简单）。

### 4.2 相对重要性归一化

做均值归一，使平均重要性为 1：
$$
I_n(W)=\frac{I(W)}{\mathrm{mean}(I(W))}
\tag{19}
$$
解释：$I_n(W)>1$ 表示该层更重要，应保留更多参数。

### 4.3 从重要性映射到“层压缩率/保留率”

给定目标保留率（或目标 retention ratio）$\text{trr}$ 与最小保留率 $\text{mrr}$，论文给出线性映射：
$$
CR(W)=\text{mrr}+I_n(W)\cdot(\text{trr}-\text{mrr})
\tag{20}
$$
其中 $CR(W)$ 可理解为该层的“保留比例”（越大表示保留越多）。

### 4.4 用 $CR(W)$ 决定截断尺度（如何落到 rank）

论文用参数比例约束把 $CR(W)$ 转成实际截断：截断 $U_k^\sigma$ 与 $(V_k^\sigma)^\top$ 的列/行数（即 rank），使得：
$$
CR(W_i)=\frac{\#\text{params}(U_k^\sigma)+\#\text{params}((V_k^\sigma)^\top)}{\#\text{params}(W_i)}
\tag{21}
$$
实践上你会把 $k_i$ 当未知数，求使上式最接近 $CR(W_i)$ 的 $k_i$。

------

## 5) AdaSVD 的完整 Step 实现流程（按论文 Algorithm 1 组织）

下面把论文算法串成“你写代码时的顺序”，从目标到落地：

### Step 0：校准数据准备 + SOB

1. 从数据集中随机取校准样本 $X$
2. 设 bucket size 为 $M$，用式(14)(15) 做 $X'$（SOB），得到固定显存下更稳定的校准集

### Step 1：Whitening（可选但论文建议）

论文把 whitening 作为前置步骤（并说明 adaComp 可与数据白化结合进一步降误差）。
 实现上你可沿用 SVD-LLM 的 whitening：为每层构造白化矩阵 $S_i$，对 $W_iS_i$ 做 SVD（算法里写成 `SVD(WiSi)`）。

### Step 2：adaCR 计算每层压缩率（保留率）

对每层：

1. 取输入激活 $X_i$，算 $Y_i=W_iX_i$
2. 得到 $I(W_i)=\cos(X_i,Y_i)$
3. 均值归一得到 $I_n(W_i)$
4. 用式(20) 得到该层 $CR(W_i)$
5. 将 $CR(W_i)$ 映射为 rank $k_i$（满足式(21)）

### Step 3：SVD + 按层 rank 截断初始化

对每层权重（或其白化版本）：

1. SVD：$W_iS_i=U\Sigma V^\top$
2. 用 $k_i$ 截断得到 $U_{k_i},\Sigma_{k_i},V_{k_i}$
3. 吸收 $\Sigma$：$U_{k_i}^\sigma=U_{k_i}\Sigma_{k_i}^{1/2}$，$V_{k_i}^\sigma=V_{k_i}\Sigma_{k_i}^{1/2}$

### Step 4：adaComp 交替伪逆更新（$\tau$ 次）

对每层（或逐层循环）重复 $\tau$ 次：

1. 固定 $V_{k_i}^\sigma$：按式(8)(9)(10) 用伪逆更新 $U_{k_i}^\sigma$
2. 固定 $U_{k_i}^\sigma$：按式(13) 用伪逆更新 $(V_{k_i}^\sigma)^\top$

### Step 5：写回模型（两层线性替换）

压缩后线性层用
$$
W_{c,i}=U_{k_i}^\sigma (V_{k_i}^\sigma)^\top
$$
部署时等价拆成两层：先 $(V_{k_i}^\sigma)^\top$ 再 $U_{k_i}^\sigma$。

------

## 6) 这篇论文的关键创新点（你写 related work/贡献总结时可直接用）

1. **adaComp：截断后补偿**
    把“截断后的固定因子”升级成“在 rank-$k$ 因子空间里最小化 $\|WX-W_cX\|$ 的交替最小二乘”，并用 **Moore–Penrose 伪逆**增强数值稳定性。
2. **SOB：显存不变、校准更多**
    用均值 bucket 化稳定校准统计，使 adaComp 的更新更可靠。
3. **adaCR：按层重要性分配压缩率**
    以 $\cos(X,WX)$ 作为层重要性，线性映射到层保留率，再映射到 rank。

------

## 7) 一些后续 idea（只基于 AdaSVD 自身，不展开“结合建议”）

### Idea A：把 adaComp 的目标从 $\|WX-W_cX\|_F^2$ 扩展成“带正则的稳定 ALS”

高压缩率 + 小校准集时容易过拟合（论文也观察到迭代过多会掉点）。可考虑：
$$
\min_{U,V}\ \|UV^\top X-WX\|_F^2 + \lambda(\|U\|_F^2+\|V\|_F^2)
$$
这样伪逆解可替换为岭回归式的稳定解，减少过拟合/爆振。

### Idea B：用“早停准则”替代固定迭代数 $\tau$

例如监控校准集上的
$$
\Delta \mathcal{L}_{\text{SVD}}^{(t)} = \mathcal{L}^{(t-1)}-\mathcal{L}^{(t)}
$$
当 $\Delta \mathcal{L}$ 低于阈值或开始回升即停止，避免低压缩率场景的过优化。

### Idea C：adaCR 的重要性度量可以更“任务相关/稳健”

当前用 $\cos(X,WX)$ 很轻量，但可能受尺度/分布影响。可以尝试：

- 用 whitening 后的 $X$ 与 $WX$ 相似度
- 用 $\|WX\|_F/\|X\|_F$ 或梯度/Fisher proxy 作为重要性
   目标是让 $I(W)$ 更稳定、更可解释。

### Idea D：把 per-layer rank 的线性映射改成“全局预算优化”

论文是逐层算 $CR(W)$ 再截断。可以把 rank 分配写成预算约束：
$$
\min_{\{k_i\}}\ \sum_i \widehat{\mathcal{L}}_i(k_i)
\quad\text{s.t.}\quad \sum_i \mathrm{Cost}(k_i)\le \mathcal{B}
$$
其中 $\widehat{\mathcal{L}}_i$ 可由校准误差或奇异值能量近似给出，从而更系统地分配参数预算。

### Idea E：把 adaComp 的更新从“整层矩阵”细化到“子模块/头级别”

注意力 $W_Q/W_K/W_V$ 可以按 head 切块，对每个 head 独立做少量 adaComp 更新，可能更稳、更省算力。