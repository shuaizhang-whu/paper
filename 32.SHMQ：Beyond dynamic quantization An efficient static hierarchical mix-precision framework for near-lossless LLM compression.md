# SHMQ:Beyond dynamic quantization: An efficient static hierarchical mix-precision framework for near-lossless LLM compression

它的核心目标是：**在完全静态量化（static quantization）**的前提下，用一个“统一的敏感度度量”同时解决 **层间（inter-layer）**与**层内（intra-layer）**混合精度分配，并通过“解耦的重排（permutation）”降低静态量化的分组方差，从而做到 near-lossless 且更硬件友好。

------

## 1）原始优化目标：量化引入的损失扰动最小化（Hessian 二次型）

把量化看作对权重 $W$ 的扰动：
$$
\delta W = W - W_Q
$$
其中 $W_Q$ 是量化后的权重。论文从二阶近似出发，把量化对最终 loss 的影响写成 Hessian 二次型：
$$
\delta L \approx \frac{1}{2}\,\delta W^{\top}H\,\delta W
$$
其中 $H=\mathbb{E}\left[\frac{\partial^2 L}{\partial W^2}\right]$。

为了用于“元素级/通道级”的精度分配，进一步把它写成按层、按元素的求和形式（把每个权重的量化误差平方用 Hessian 元素加权）：
$$
\delta L \approx \frac{1}{2}\sum_{l\in\mathcal{L}}\sum_{i\in c_{out}}\sum_{j\in c_{in}}
h^{l}_{i,j}\cdot \big(w^l_{i,j}-Q(w^l_{i,j})\big)^2
$$
于是“最优量化策略”的原始形式就是：
$$
Q^{*} = \arg\min_{Q}\ \frac{1}{2}\sum_{l}\sum_{i}\sum_{j}h^{l}_{i,j}\cdot \delta w^{l\,2}_{i,j}
$$
其中 $\delta w^l_{i,j}=w^l_{i,j}-Q(w^l_{i,j})$。

**难点**：

1. 真 Hessian 在 LLM 中不可显式构造与存储；
2. 搜索空间巨大（每层/每通道/每元素选择 bit-width），直接全局求解不可行。

------

## 2）改进后的目标：把全局难题分解为“层间 + 层内”的两阶段优化（分层求解）

论文将原始目标分解为两个子问题（先粗后细），写成：
$$
\begin{cases}
Q_l^* = \arg\min_{Q_l}\ \frac{1}{2}\sum_{l\in\mathcal{L}} h^l\cdot \delta w^l \\
Q_{i,j}^* = \arg\min_{Q_{i,j}}\ \sum_{j\in c_{in}}\sum_{i\in c_{out}} h^l_{i,j}\cdot \delta w^{l\,2}_{i,j}
\quad \text{s.t. } Q_{i,j}\in Q_l^*
\end{cases}
$$
直觉是：

- **第一阶段（InterMQ）**：先决定每一层大概“高精度占比/bit 配额”（coarse-grained）。
- **第二阶段（IntraMQ）**：在层内再决定哪些通道/参数保留高精度（fine-grained）。
   并且两阶段共享同一个“量化敏感度”定义：

$$
S^l_{i,j}=\frac{1}{2}h^l_{i,j}\cdot \big(w^l_{i,j}-Q(w^l_{i,j})\big)^2
$$

------

## 3）Inter-layer：用 Fisher 近似 Hessian，推到一个“可计算的层敏感度”公式

### 3.1 Hessian 近似：$H \approx F$（经验 Fisher）

论文用 Fisher 信息矩阵近似 Hessian：
$$
H \approx F = \frac{1}{|\mathcal{D}|}\sum_{x\in\mathcal{D}} g(x)\,g(x)^{\top}
$$
其中 $g(x)=\nabla_W L(x)$，$\mathcal{D}$ 是校准集。

### 3.2 层敏感度的推导：从 $\delta W^\top F\delta W$ 到 $(g^\top\delta w)^2$

把第 $l$ 层按输出通道 $i$ 分块，令 $\delta w^l_{i,:}$ 表示该层第 $i$ 个输出通道对应的量化误差向量，则层敏感度定义为：
$$
S^l_{\text{InterMQ}}=\frac{1}{2}\sum_{i\in c_{out}}\delta w^{l\top}_{i,:}\,H\,\delta w^l_{i,:}
$$
代入 $H\approx F=\frac{1}{|\mathcal{D}|}\sum gg^\top$：
$$
S^l_{\text{InterMQ}}
\approx
\frac{1}{2}\sum_{i}\delta w^{l\top}_{i,:}\left(\frac{1}{|\mathcal{D}|}\sum_{x\in\mathcal{D}} g g^\top\right)\delta w^l_{i,:}
=
\frac{1}{2}\cdot\frac{1}{|\mathcal{D}|}\sum_{x\in\mathcal{D}}\sum_{i}\left(g^\top\delta w^l_{i,:}\right)^2
$$
这一步非常关键：**不需要显式构造 $F$**，只需要拿到校准样本的梯度向量 $g$ 并与量化误差做内积即可。

### 3.3 从层敏感度到“层高精度比例”的映射（带参数量修正）

只知道层敏感度还不够，必须把它变成“这层该给多少高精度配额”。论文给出一个 sensitivity-determined mapping：

- 先定义该层相对参数规模：

$$
r_l=\frac{c^l_{in}\cdot c^l_{out}}{\min_{l\in\mathcal{L}}(c^l_{in}\cdot c^l_{out})}
$$

- 再把敏感度与规模结合，得到该层高精度比例 $U^l$：

$$
U^l
=
\frac{S^l_{\text{InterMQ}}\cdot \sum r_l}{\sum (S^l_{\text{InterMQ}}\cdot r_l)}\cdot (U_t-U_b) + U_b
$$

其中：

- $U_t$：全模型目标高精度总体比例（例如 W4.8A8 里高精度约 20%）
- $U_b$：每层至少保底的高精度比例（例如 12.5%，保证基本 outlier 兜底）

并且论文在附录里证明：按这个映射分配后，整体高精度占比的加权平均能严格等于 $U_t$（即预算可控）。

------

## 4）Intra-layer：用 OBS 风格推导得到“元素敏感度”，再聚合成“通道敏感度”

Inter-layer 解决“每层大概给多少高精度”，但层内到底哪些通道该保留高精度，需要更细粒度。

### 4.1 从约束优化推到 OBS 形式：敏感度与 $H^{-1}$ 对角相关

论文在附录里把“量化某个权重元素”写成一个带约束的二次优化，最终得到每个元素的敏感度可写成：
$$
S^l_{i,j}
=
\frac{1}{2}\big(w^l_{i,j}-Q(w^l_{i,j})\big)^2\cdot [H_l^{-1}]_{j,j}
$$
直觉：如果输入维第 $j$ 维在 $H^{-1}$ 对角上很大，说明该方向对扰动更敏感，则同样的量化误差会造成更大损失增量。

### 4.2 计算可行化：用 $H\approx XX^\top$ + damping + Cholesky

显式 $H^{-1}$ 仍然难算。论文采取 OBC/OBS 常用的 proxy Hessian：
$$
H \approx X X^\top
$$
并加入 Levenberg–Marquardt damping（避免奇异）：
$$
\tilde H = X X^\top + \lambda\cdot \mathrm{mean}(\mathrm{diag}(XX^\top))\cdot I
$$
则元素敏感度变成：
$$
S^l_{i,j}
=
\frac{1}{2}\big(w^l_{i,j}-Q(w^l_{i,j})\big)^2\cdot [\tilde H^{-1}]_{j,j}
$$
其中 $\tilde H^{-1}$ 可以用 Cholesky 分解高效求（至少取对角）。

> 为什么 Intra-layer 不继续用 Fisher？论文给的关键理由是：Intra-layer 需要 $H^{-1}$（考虑参数耦合），而 Fisher 逆的成本更高；同时，$XX^\top$ 在层间会因深层激活幅度增大产生偏置，不适合 Inter-layer 的“公平比较”，所以两阶段用了不同 Hessian 近似。

### 4.3 从元素敏感度到通道敏感度：Manhattan/L1 聚合

为了做“通道级”混合精度，论文把同一输入通道 $j$ 上的元素敏感度按输出通道累加（L1/Manhattan norm）：
$$
S^{l}_{\text{IntraMQ}}(j)
=
\left\|S^l_{:,j}\right\|_1
=
\sum_{i\in c_{out}}|S^l_{i,j}|
$$
然后在该层选择 top-K 个最敏感输入通道作为高精度通道集合：
$$
C_{\text{sen}}=\mathrm{TopK}(S^{l}_{\text{IntraMQ}},\ K),
\qquad
K=\left\lfloor c_{in}\cdot U^l \right\rceil
$$
这一步把层间分配得到的 $U^l$（该层高精度比例）真正落到“具体哪些通道用 INT8、哪些用 INT4”。

------

## 5）关键工程点：解耦 Identification 与 Permutation，降低静态分组量化误差

静态量化通常是 group-wise（例如 group size=128）统一 scale/zero-point。混合精度如果“直接把敏感通道挑出来放一组”，会导致每组内分布差异大、方差大，静态 scale 很难兼顾，误差变大。

论文提出 **Decoupled Identification and Permutation**：

1. **Identification**：仅按敏感度把通道分成敏感簇 $C_{\text{sen}}$ 与非敏感簇 $C_{\text{insen}}$。
2. **Permutation**：在每个簇内部，再按“幅值/分布相似性”重排通道，使得**同一量化 group 内通道分布更平坦**（组内方差更小）。
3. 对重排后的权重做静态 group-wise 量化：敏感簇用高 bit，非敏感簇用低 bit。

一个额外的系统技巧是：并行线性层（如 q/k/v_proj，up/gate_proj）需要共享同一重排索引，才能把 activation permutation 融合进前一层算子（例如融合进 norm），从而避免在线重排开销。这也是它能做到“静态仍然快”的重要原因。

------

## 6）Step 实现流程（从优化目标到可落地 pipeline）

我按可复现的工程顺序写成步骤清单：

### Step 0：设定量化目标与校准数据

- 设定整体混合精度目标（如 W4A8 + 20% W8A8，即总体高精度比例 $U_t$）
- 选校准集 $\mathcal{D}$（论文示例：128 条、长度 2048）

### Step 1：Inter-layer（决定每层高精度比例 $U^l$）

对每个线性层 $l$：

1. 对该层先做一次候选静态量化得到 $W_Q$，计算 $\delta W$

2. 对校准样本做反向，拿到梯度向量 $g$

3. 计算层敏感度：
   $$
   S^l_{\text{InterMQ}}=\frac{1}{2|\mathcal{D}|}\sum_{x\in\mathcal{D}}\sum_{i}(g^\top\delta w^l_{i,:})^2
   $$

4. 用 mapping 把 $S^l_{\text{InterMQ}}$ 转成该层高精度比例 $U^l$（含保底 $U_b$ 与参数量修正 $r_l$）

输出：每层要保留的高精度通道数量 $K=\lfloor c_{in}\cdot U^l\rceil$。

### Step 2：Intra-layer（找出层内最敏感通道）

对每层：

1. 从校准集前向拿到该层输入激活矩阵 $X^l$

2. 构造 proxy Hessian：
   $$
   \tilde H = X^lX^{l\top}+\lambda\cdot \mathrm{mean}(\mathrm{diag}(X^lX^{l\top}))\cdot I
   $$

3. 取 $[\tilde H^{-1}]_{j,j}$（Cholesky 或其他方式）

4. 计算元素敏感度：
   $$
   S^l_{i,j}=\frac{1}{2}(w^l_{i,j}-Q(w^l_{i,j}))^2\cdot [\tilde H^{-1}]_{j,j}
   $$

5. 聚合成通道敏感度：
   $$
   S^l_{\text{IntraMQ}}(j)=\sum_i |S^l_{i,j}|
   $$

6. 选 top-K 通道为敏感通道集合 $C_{\text{sen}}$

### Step 3：Decoupled Permutation（降低静态 group 的分布方差）

对每层：

1. 按敏感度把通道分成敏感簇/非敏感簇
2. 在各簇内按幅值/分布相似性排序，使组内方差更小
3. 得到最终 permutation index（并确保并行层共享 index，便于融合）

### Step 4：静态混合精度量化与部署

- 对 permute 后的权重：
  - 敏感簇：INT8（或更高）
  - 非敏感簇：INT4
- 激活同样使用静态 group-wise（论文设置为对称量化）
- 把 activation permutation 融合进前序算子（如 norm）以消除在线开销

------

## 7）一些可延展的 idea（只给方向，不展开成详细融合方案）

1. **把“统一敏感度 $S$”迁移到低秩/SVD 的 rank 分配**
    这篇的核心是用二阶近似把“误差对任务损失的影响”量化出来。你可以把同样的思想用在 SVD 截断：把“丢掉第 $k$ 个奇异方向”看作参数扰动 $\delta W_k$，用
    $\delta L \approx \frac{1}{2}\delta W_k^\top H\delta W_k$
    给出更“任务对齐”的 rank/层间 rank 分配准则。
2. **Permutation 不仅能救静态量化，也可能提升低秩分解的数值稳定**
    对权重/激活先做通道重排让分布更一致，有机会让：

- 量化 scale 更稳定
- SVD/低秩近似时谱分布更“平滑”（减少极端 outlier 导致的奇异值跨度）
   从而在相同 rank 下误差更小。

1. **Inter-layer 用 Fisher，Intra-layer 用 $XX^\top$：可以进一步统一成“块对角 Fisher / KFAC”**
    它现在是“分阶段选不同近似”。你可以探索一个折中：用块对角 Fisher（或 Kronecker 近似）同时支持层间比较与层内近似逆，从而减少“两个近似体系切换”的不一致性。
2. **把“层间高精度比例映射”改成“联合约束优化”**
    当前映射是一个可控预算的启发式线性映射。可以考虑写成显式 constrained optimization：

$$
\min_{\{U^l\}}\sum_l f(S^l, U^l)\quad \text{s.t.}\quad \sum_l \omega_l U^l=U_t,\ U^l\ge U_b
$$

其中 $f$ 可以是经验拟合的误差模型或二阶上界，得到更可解释的分配规律。

1. **静态量化“近无损”之后，误差主要来自极少数层/通道：可以做更细粒度（group 内）的二级混合**
    这篇做到了层内通道级混合精度。下一步可以尝试在敏感簇内部进一步做“更小 group size / 更高精度 / 更强重排”，把代价集中到极少数最关键区域。