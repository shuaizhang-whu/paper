# SoCo：Streamlining language models via semantic basis analysis

它把一个权重矩阵的 SVD 分解项 $u_i v_i^\top$ 当作“**语义基（semantic bases）**”，然后针对目标应用（数学、代码、语言建模等）去**重新学习这些基的“重要性”（奇异值权重）**，再按重要性把不需要的基剪掉；同时允许补充一小撮“新基”来弥补目标域缺失语义。整体上属于“**基选择（basis selection）+ 低秩重参数化**”，而不是传统只做截断的 SVD。Li 等 - Streamlining language mo…

------

## 1）从标准 SVD 目标出发：把矩阵看作一组正交基的线性组合

对线性层 $y=Wx+b$，标准 SVD：
$$
W = U S V^\top,\quad
U=[u_1,\dots,u_r],\ S=\mathrm{diag}(s_1,\dots,s_r),\ V=[v_1,\dots,v_r]
$$
可写成级数形式：
$$
W=\sum_{i=1}^{r} s_i\, u_i v_i^\top
\tag{1}
$$
Basel 先证明（或至少强调）每个外积矩阵
$$
W_i=u_i v_i^\top
$$
在矩阵空间里是“正交归一基”，因为：

- Frobenius 范数：

$$
\|W_i\|_F
=\sqrt{\mathrm{tr}(W_i^\top W_i)}
=\sqrt{\mathrm{tr}(v_i u_i^\top u_i v_i^\top)}
=\sqrt{\mathrm{tr}(v_i v_i^\top)}
=\sqrt{\mathrm{tr}(v_i^\top v_i)}
=1
\tag{2}
$$

- Frobenius 内积（$i\neq j$）：

$$
\langle W_i, W_j\rangle_F
=\mathrm{tr}(W_i^\top W_j)
=\mathrm{tr}(v_i u_i^\top u_j v_j^\top)
= (u_i^\top u_j)\,(v_j^\top v_i)
=0
\tag{3}
$$

因此可以把 $\{u_i v_i^\top\}$ 当作一组“滤波器/基”，而 $\{s_i\}$ 是它们的系数（重要性）。在信号处理视角里，对输入 $x$：
$$
Wx=\sum_{i=1}^{r} s_i\,u_i (v_i^\top x)
$$
即先用 $v_i^\top x$ 测相似度，再按 $s_i$ 放大，最后沿 $u_i$ 方向叠加输出。

Basel 的“语义基”观点来自一个观察：对某些层/头的 $u_i$ 或 $v_i$ 做反嵌入（投到词表 logit 空间取 top token），会出现明显的任务相关 token（代码符号、数学符号、非英文字符等），暗示不同基承载不同语义。

------

## 2）传统低秩压缩的优化目标与不足：只“截断”，不“选基”

常规 rank-$k$ 截断等价于：
$$
\min_{\mathrm{rank}(\hat W)\le k}\ \|W-\hat W\|_F
$$
其最优解是取最大的 $k$ 个奇异值对应的项：
$$
\hat W_k=\sum_{i=1}^{k} s_i u_i v_i^\top
\tag{4}
$$
问题在于：$s_i$ 来自预训练分布（或某个 finetune 分布），并不一定对应“目标应用的基重要性”。同一个基在目标任务上可能无用甚至有害（被激活时引入错误信息），而某些目标任务需要的新语义基可能在预训练里“缺席”。

Basel 的关键改动就是：**不直接用原 $s_i$ 做剪裁依据，而是用目标数据重新学习“基权重”后再剪裁**。

------

## 3）改进后的目标：把“截断 SVD”变成“基权重重学习 + 基剪裁 + 可选新基补偿”

### 3.1 核心重参数化：冻结旧基，只学习新权重（以及少量新基）

Basel 把目标层权重写成：
$$
W_f
=\sum_{i=1}^{r} \tilde s_i\, u_i v_i^\top
+\sum_{j=1}^{\tilde r}\ \tilde u_j \tilde v_j^\top
\tag{5}
$$

- 第一项：沿用预训练 SVD 得到的基 $u_i,v_i$，**冻结**不动，只把系数从 $s_i$ 变成可学习的 $\tilde s_i$（初始化为 $s_i$）。
- 第二项：引入 $\tilde r$ 个“可学习新基”$(\tilde u_j,\tilde v_j)$，用于：
   1）补齐目标分布缺失基；
   2）补偿大量剪裁后累积损失。
   注意最终会对 $W_f$ 再做一次 SVD，如果新基与旧基有相关性，rank 会被自动合并，不一定线性增加模型大小。

### 3.2 训练目标：最小化目标任务损失（可加稀疏正则）

在目标应用训练集 $\mathcal{D}$ 上，用常规任务损失（比如交叉熵）训练：
$$
\min_{\{\tilde s_i\},\{\tilde u_j,\tilde v_j\}}
\ \mathbb{E}_{(x,y)\sim\mathcal{D}}\ \ell\big(M(W_f;x),y\big)
\tag{6}
$$
其中 $M(W_f;\cdot)$ 表示把该层替换为 $W_f$ 后的模型前向。

论文还讨论了对 $\tilde s_i$ 加 $L_1$ 以鼓励稀疏：
$$
\min\ \mathcal{L}_{\text{task}} + \lambda \sum_{i=1}^{r}|\tilde s_i|
\tag{7}
$$
但实验显示 $\lambda$ 太大会在深度压缩下伤性能（$\lambda$ 小则影响不大）。

### 3.3 剪裁规则：按“权重和占比”做逐步基剪裁（不是按固定 top-k）

Basel 的剪裁不是一次性 top-k，而是“多次小步剪”，每隔固定迭代数就对每层执行一次剪裁。

设当前保留基集合为 $\mathcal{I}$，当前权重和为：
$$
S_{\text{sum}}=\sum_{i\in\mathcal{I}} \tilde s_i
$$
给定每次剪裁要保留的比例 $\rho\in(0,1)$（KeepRatioPerPruning），剪裁选择新的集合 $\mathcal{I}'\subset\mathcal{I}$ 满足：
$$
\sum_{i\in\mathcal{I}'} \tilde s_i \ \ge\ \rho\, S_{\text{sum}}
\tag{8}
$$
并通常取满足该约束的“最小集合”（等价于：把 $\tilde s_i$ 从大到小排序，拿到权重和首次达到阈值为止），其余基直接丢弃。剪裁后继续训练让 $\tilde s_i$ 与新基适应新的容量。

这个规则比“保留固定数量基”更稳定：不同层的基分布不一样，用“权重和占比”能让剪裁幅度对每层自适应。

------

## 4）训练结束后如何真正落成“可推理的低秩结构”：对 $W_f$ 再做一次 SVD 并替换为两层线性

训练/剪裁后，每层得到一个显式矩阵（或可显式构造）$W_f$。Basel 对它做标准 SVD：
$$
W_f = U' S' V'^\top
\tag{9}
$$
取其有效 rank 为 $r'$（实际由剪裁与训练后矩阵决定，常小于原 $r$），然后用两层线性替换原层：

- 第一层权重：$S'V'^\top \in \mathbb{R}^{r'\times m}$
- 第二层权重：$U'\in \mathbb{R}^{n\times r'}$

于是参数量从原来的 $nm$ 变为：
$$
(n+m)r'
\tag{10}
$$
这一步把“基选择”产物转成标准的低秩推理形式（两次 GEMM），部署很直接。

最后再做若干 epoch 的 post finetune（全模型或部分参数）进一步恢复性能。

------

## 5）Basel 的完整 Step 实现流程（按论文 Algorithm 1 重写）

对模型中每个线性层（通常不压 embedding）：

### Step 0：SVD 分解与结构改写

1. 对原权重 $W$ 做 SVD 得到 $u_i,v_i,s_i$
2. 把该层替换为式(5)形式：冻结 $u_i,v_i$，把 $\tilde s_i$ 设为可训练（初始化 $\tilde s_i\leftarrow s_i$），额外引入 $\tilde r$ 组可训练 $(\tilde u_j,\tilde v_j)$

### Step 1：Keeping 阶段（先适配目标任务）

在 KeepingEpoch 内，只训练 $\tilde s_i$ 与新基参数，使其对目标任务拟合（式(6)）。

### Step 2：Pruning 阶段（边剪边训）

在 PruningEpoch 内持续训练，并每隔 IterationsPerPruning：

- 对每层按规则(8)剪掉小 $\tilde s_i$ 的基（实际上就是删掉对应的 $u_i,v_i$ 项）
- 剪后继续训练让模型适应容量下降

### Step 3：落地为推理结构

对每层：

1. 用式(5)构造 $W_f$
2. 对 $W_f$ 做 SVD：$W_f=U'S'V'^\top$
3. 用两层线性 $(S'V'^\top)$ 与 $U'$ 替换原层

### Step 4：Post Fine-tuning

对新模型做 PostFineTuningEpoch，进一步恢复与增强目标任务性能。

------

## 6）与 SVD-LLM v2 / SAES-SVD 的主要理论区别（抓“根本不同点”）

1. Basel 的“核心变量”不是基向量而是“基权重”

- SVD-LLM v2 / SAES-SVD：主要研究 **如何分解/如何截断/如何控制截断误差传播**（更像改分解器与误差模型）。
- Basel：把 $u_i,v_i$ 当作固定“语义字典”，核心是在目标数据上重新学习 $\tilde s_i$，再做“基选择”。它更像“在预训练语义字典上做稀疏重加权”。

1. Basel 是“目标应用驱动”的压缩（需要目标训练集）

- SVD-LLM v2、SAES-SVD 往往是 calibration set + 训练自由/极少训练（偏 post-training 压缩）。
- Basel 需要对目标任务进行训练迭代（但参数量远小于全量微调，因为冻结了大量基）。

1. Basel 自带“引入新基”的通道

- 这点与“只在已有谱里选/截断”的方法不同：它允许出现 $(\tilde u_j,\tilde v_j)$ 来补齐目标域缺失语义，相当于在压缩过程中做了一点“有约束的增量表示学习”。

------

## 7）一些可延展的 idea（只给方向，不展开详细结合方案）

1. 把“基权重重学习”改成 activation-aware 版本
    现在 Basel 用任务损失驱动 $\tilde s_i$。你可以在低成本场景下，用校准激活 $X$ 定义一个更接近 SVD-LLM 的代理目标，例如：

$$
\min_{\tilde s}\ \|(W_f(\tilde s)-W)X\|_F^2
$$

用无标签数据也能做“基重要性重估”，并与 truncation-aware 的谱解释更一致。

1. 用“误差传播风险”指导剪裁，而不是只看 $\tilde s_i$ 大小
    可以在剪裁规则里加入“该基在后续层的放大风险”或 SAES 风格的累计误差估计，让剪裁更稳，避免把“会触发误差爆炸的方向”留下来。
2. 把 Basel 的剪裁从“单层内部”推广到“跨层共享基”
    如果多层出现相似的 $u_i,v_i$ 语义方向，可以考虑构造跨层共享的基库（类似字典学习），再在每层只学系数。这样参数共享更强，适合极限压缩。
3. 新基项做结构约束（正交/低相关）以避免与旧基强耦合
    目前新基可能与旧基冗余，虽然最终 SVD 会合并，但训练期可能不稳定。可加约束：

$$
\tilde U^\top U \approx 0,\ \tilde V^\top V \approx 0
$$

或对输出做投影去相关，提高新基“补偿效率”。

1. “语义基解释”可反过来做“任务自适应层/头选择”
    既然某些层/头的基显著对应代码/数学 token，你可以用这种信号做更高层的结构决策（如只压缩某些头、或给某些层更高 rank 预算），形成“语义驱动的资源分配”。