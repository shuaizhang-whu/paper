# Dobi-SVD: DIFFERENTIABLE SVD FOR LLM COMPRESSION AND SOME NEW PERSPECTIVES

## 1）这篇论文的核心“范式切换”：从截断权重/激活感知截断权重 → 直接截断激活，再反推权重

传统 SVD 压缩一般做的是
 对权重 $W\in\mathbb{R}^{m\times n}$ 做低秩分解 $W\approx W_1W_2$（或 $U\Sigma V^\top$ 截断），优化目标偏向
$$
\min_{\mathrm{rank}(W_k)\le k}\ \|W-W_k\|_F^2
$$
或 activation-aware 版本（ASVD/SVD-LLM 系）偏向
$$
\min_{\mathrm{rank}(W_k)\le k}\ \|XW-XW_k\|_F^2
$$
但 Dobi-SVD 的关键论点是：**对于给定模块的输出激活 $A=XW$**，如果你的目标就是让输出激活尽量不变，那么**直接对 $A$ 做最优秩-$k$ 近似**更“正宗”、更接近最优（Eckart–Young–Mirsky, EYM）。

于是它把“低秩截断”的对象从 $W$ 挪到 $A$：
$$
A=XW,\qquad A_k=\arg\min_{\mathrm{rank}(\tilde A)\le k}\ \|A-\tilde A\|_F^2
$$
由 EYM 定理，若 $A=U_A\Sigma_A V_A^\top$，则
$$
A_k = U_A\Sigma_A G_k V_A^\top
$$
其中 $G_k=\mathrm{diag}(\underbrace{1,\dots,1}_{k},0,\dots,0)$。

这一步带来两个新的难点，也正好对应论文提出的三问（Q1-Q3）：

1. 每层每矩阵的 $k$ 怎么选（组合空间巨大）？
2. 已经把激活截断成 $A_k$ 了，权重 $W$ 怎么“无微调”更新成一个低秩 $W_f$ 来匹配 $A_k$？
3. 传统 SVD 的压缩比-秩 $k$ 关系是“注入式（injective）限制”，导致**要想压得动就得砍掉至少半数奇异值**（方阵时尤其严重），信息损失过大；能不能“解锁”这条限制？

------

## 2）从“优化目标”到“改进后的目标”：三块核心机制与推导

### 2.1 原始目标：离散的截断位置选择（不可导、组合爆炸）

对一个模型里很多矩阵 $W^{(\ell,j)}$（第 $\ell$ 层、第 $j$ 个线性变换），每个都有 $k^{(\ell,j)}\in\{1,\dots,n\}$。如果想在给定压缩率 $R_{\text{tar}}$ 下找到最优组合，本质是
$$
\min_{\{k^{(\ell,j)}\}}\ L_{\text{task}}(\{k^{(\ell,j)}\})\quad\text{s.t.}\quad R(\{k^{(\ell,j)}\})=R_{\text{tar}}
$$
但 $k$ 是离散变量，直接搜几乎不可能。

#### 改进目标 1：把离散 $k$ “连续化 + 可导化”

Dobi-SVD 不是让 $k$ 直接变连续，而是对奇异值做一个平滑“软截断”门控，让 $k$ 能通过梯度影响输出。

对每个激活的奇异值序列 $\{\sigma_i\}$，定义平滑截断函数：
$$
T(\sigma_i)=\sigma_i\left[0.5\tanh(\beta(k-i))+0.5\right]
$$
于是重建的“软截断激活”为
$$
\tilde A(k)=U_A\,\mathrm{diag}(T(\sigma_1),\dots,T(\sigma_n))\,V_A^\top
$$
直觉：当 $i\ll k$，$\tanh(\beta(k-i))\approx 1\Rightarrow T(\sigma_i)\approx\sigma_i$；当 $i\gg k$，$\tanh(\beta(k-i))\approx -1\Rightarrow T(\sigma_i)\approx 0$。

然后把原本的约束优化变成可训练的多目标：
$$
L(k)=L_{\text{task}}(\tilde A(k))+\gamma\left|R_{\text{now}}(k)-R_{\text{tar}}\right|
$$
训练时冻结模型其余参数，只训练所有矩阵的 $k$。

------

### 2.2 可导 SVD 的“梯度爆炸”与改进目标 2：梯度鲁棒反传（Taylor 展开）

SVD 反传里经典的数值不稳定来自项：
$$
E_{ij}=\begin{cases}
\sigma_j^2-\sigma_i^2,& i\ne j\\
1,& i=j
\end{cases}
$$
而梯度里会出现 $\frac{1}{E_{ij}}$。当 $\sigma_i\approx\sigma_j$ 或都很小，$\frac{1}{E_{ij}}$ 会爆炸。

论文给出的核心策略是：对 $\frac{1}{E_{ij}}$ 做分情况近似，避免 NaN/inf：

- 若 $\sigma_i\approx 0$ 且 $\sigma_j\approx 0$：直接令 $\frac{1}{E_{ij}}=\gamma_{\text{small}}$（一个很小常数控制上界）。
- 若 $\sigma_i\neq 0$ 但 $\sigma_i\approx \sigma_j$：对

$$
\frac{1}{\sigma_i^2-\sigma_j^2}
=\frac{1}{\sigma_i(\sigma_i+\sigma_j)}\cdot\frac{1}{1-(\sigma_j/\sigma_i)}
$$

在 $\sigma_j/\sigma_i\approx 1$ 处用几何级数（Taylor）展开：
$$
\frac{1}{1-r}\approx \sum_{t=0}^{K} r^t,\quad r=\frac{\sigma_j}{\sigma_i}
$$
于是得到稳定近似：
$$
\frac{1}{E_{ij}}\approx \frac{1}{\sigma_i(\sigma_i+\sigma_j)}\sum_{t=0}^{K}\left(\frac{\sigma_j}{\sigma_i}\right)^t
$$
这使得“训练 $k$”这个看似很轻量的过程在数值上能跑起来。

> 你如果对 SAES-SVD 的数值抑制敏感，会发现这里属于“梯度侧”的抑制，而 SAES 更偏“前向误差侧”的抑制：两者可以视作不同层面的稳定化。

------

### 2.3 截断了激活以后，权重怎么“理论最优”更新：EYM + 正交性质 → PCA/IPCA 推导

这是 Dobi-SVD 最关键的理论部分：它要解决“我直接把 $A=XW$ 截断成 $A_k$ 了，但最终模型里得换成某个低秩权重 $W_f$ 才算真正压缩”。

#### 2.3.1 从激活截断推出“投影后的权重集合”

有
$$
A_k = A V_A G_k V_A^\top
$$
代入 $A=XW$：
$$
A_k = XW V_A G_k V_A^\top
$$
于是对每个样本 $x_i$（或一批输入）对应的 $V_{A_i}$，理想的更新权重 $W_f$ 应满足
$$
\min_{W_f}\ \sum_{i=1}^{n}\|x_i W V_{A_i}G_kV_{A_i}^\top - x_i W_f\|_F
\quad \text{s.t.}\ \mathrm{rank}(W_f)=k
$$
论文把它转成“让 $W_f$ 接近一组投影权重”的问题，定义：
$$
W_{p,i}=W V_{A_i}G_kV_{A_i}^\top
$$
那么目标近似为：找一个 rank-$k$ 的 $W_f$ 去逼近 $\{W_{p,i}\}$。

#### 2.3.2 关键假设：把 $W_f$ 写成 $WVV^\top$

设 $V\in\mathbb{R}^{n\times k}$ 是某个正交基（列正交），令
$$
W_f = WVV^\top
$$
此时 $\mathrm{rank}(W_f)\le k$ 自动满足。

于是目标可写为
$$
\min_{V}\ \sum_{i=1}^{n}\|W V_iV_i^\top - W VV^\top\|_F^2
\quad (V_i \equiv V_{A_i}[:,1\!:\!k])
$$
利用 Frobenius 范数次乘性质，上界：
$$
\sum_i\|W V_iV_i^\top - W VV^\top\|_F^2
\le \|W\|_F^2\sum_i\|V_iV_i^\top - VV^\top\|_F^2
$$
所以只要最小化
$$
\min_V\ \sum_i\|V_iV_i^\top - VV^\top\|_F^2
$$
展开：
$$
\|V_iV_i^\top - VV^\top\|_F^2
= \|V_iV_i^\top\|_F^2+\|VV^\top\|_F^2-2\mathrm{tr}(V_iV_i^\top VV^\top)
$$
由于 $V_i,V$ 都列正交，$\|V_iV_i^\top\|_F^2=\|VV^\top\|_F^2=k$，所以等价于最大化
$$
\max_V\ \sum_i \mathrm{tr}(V^\top V_iV_i^\top V)
= \max_V\ \sum_i \|V^\top V_i\|_F^2
$$
这就是一个“在 Grassmann 流形上找最能对齐一组子空间的公共子空间”的问题；其标准解是对 $\{V_i\}$ 做 PCA（或等价地对 $\sum_i V_iV_i^\top$ 做特征分解取 top-$k$）。

#### 2.3.3 工程落地：IPCA（增量 PCA）避免爆内存

直接 PCA 需要把所有 $V_i$ 堆起来做大矩阵分解，内存不现实，于是用 IPCA 逐步更新主方向（论文给了一个“不断拼接-再 SVD”的增量过程），最终得到 $V$，再构造
$$
W_f = W V G_k V^\top
$$
（等价于保留子空间后再做 rank-$k$ 投影）。

> 这里和 SVD-LLM v2 / SAES-SVD 的差异非常大：它们通常在“给定 $k$”后直接用截断 SVD（或带加权）把 $W$ 变成两矩阵乘积；而 Dobi-SVD 是先在激活域选 $k$，再反推“公共子空间”去更新权重，强调“理论最优路径：Activation → Weight”。

------

### 2.4 改进目标 3：解决传统 SVD 的“压缩比-秩”注入限制（remapping + 混合精度存储）

传统 SVD 分解后存储 $m\times k$ 和 $k\times n$ 两个矩阵，总存储与原矩阵 $m\times n$ 的比值是
$$
r=\frac{k(m+n)}{mn}
$$
当 $m=n$ 时，要 $r<1$ 必须 $k<\frac{n}{2}$，意味着至少砍掉一半奇异值才能“真正变小”，这就是论文说的“injective limitation”。

Dobi-SVD 的 remapping 目标是：让压缩比和 $k$ 建立“近似双射（bijection）”，使得 $k$ 能从 $0$ 到 $\min(m,n)$ 全范围变化而仍能对应 $r\in[0,1]$。它提出把存储量从 $k(m+n)$ 改造成 $k\cdot \max(m,n)$。

做法（以 $m\ge n$ 为例）：

1. 对更新后的 rank-$k$ 权重 $W_f$ 再做一次 SVD：$W_f=U\Sigma V^\top$
2. 取 $\tilde U_k=(U\Sigma)[:,1\!:\!k]\in\mathbb{R}^{m\times k}$，$V_k=V[:,1\!:\!k]\in\mathbb{R}^{n\times k}$
3. 将 $\tilde U_k$ 的前 $n$ 行和 $V_k$ 全部做 8-bit 量化，然后把两份 8-bit 拼进一份 16-bit 的“混合精度行”，塞回 $\tilde U_k$ 的前 $n$ 行；最终**只存一个 $m\times k$ 的矩阵**。

直觉：$U,V$ 的分量更接近“集中、近似高斯”的分布，量化更友好，于是用量化把“本该单独存的 $V_k$”挤进同样的存储预算里。

------

## 3）step 实现过程：从训练 $k$ → 更新权重 → remapping 存储

我按论文的整体 pipeline 给你一个可直接复现/改写的步骤清单（不写代码）：

### Step A：准备校准数据与目标压缩率

- 选一小批样本（论文里 LLaMA-7B 用 256 条，序列长度 2048）。
- 设定目标压缩率 $R_{\text{tar}}$。

### Step B：构建“软激活截断”的可导前向

对每个被压缩矩阵对应的激活 $A$：

1. 前向拿到激活 $A=XW$
2. 做（低秩）SVD：$A=U_A\Sigma_A V_A^\top$
3. 用平滑门控 $T(\sigma_i)$ 得到 $\Sigma_k(k)$
4. 重建软截断激活 $\tilde A(k)=U_A\Sigma_k(k)V_A^\top$
5. 把 $\tilde A(k)$ 喂回网络后续层（其余权重冻结）

### Step C：训练阶段只更新 $k$

- 参数：所有矩阵的 $k$（或论文里叫“truncation value”）。
- Loss：

$$
L=L_{\text{task}}+\gamma|R_{\text{now}}(k)-R_{\text{tar}}|
$$

- 关键：SVD 反传用“Taylor/截断近似”的鲁棒策略，避免梯度爆炸。

输出：每个矩阵的最优 $k^{(\ell,j)}$。

### Step D：权重更新（Activation → Weight）

对每个矩阵 $W$（固定其目标 rank $k$）：

1. 用同一批数据跑前向，收集每个样本对应的 $V_{A_i}$（只取前 $k$ 列形成 $V_i$）
2. 用 IPCA 求公共子空间 $V\approx \mathrm{PCA}(\{V_i\})$
3. 构造更新权重

$$
W_f = W V G_k V^\top \quad (\text{或等价形式})
$$

1. 再把 $W_f$ 以低秩形式落到推理图中（两矩阵乘，或后续配合 remapping）

### Step E：remapping（混合精度存储，解锁注入限制）

对每个 $W_f$：

1. SVD 得到 $\tilde U_k=(U\Sigma)[:,1\!:\!k]$、$V_k$
2. 量化并拼接，把 $V_k$ “塞进” $\tilde U_k$ 的部分行
3. 最终存一个 $\max(m,n)\times k$ 的矩阵结构，实现 $r\leftrightarrow k$ 的更顺滑映射

------

## 4）与 SVD-LLM v2 / SAES-SVD 的主要理论区别（抓最核心的）

1. **截断对象不同**

- SVD-LLM v2 / SAES-SVD：主线仍是“在权重域做低秩分解”，用激活统计/误差抑制去决定截断策略与补偿。
- Dobi-SVD：强调“最优秩-$k$ 近似应该发生在激活 $A$ 上”，再反推权重。

1. **“选 $k$”的方式不同**

- SVD-LLM v2 / SAES-SVD：多是启发式、逐层策略、或通过误差模型推 $k$/rank 分配。
- Dobi-SVD：把每个矩阵的 $k$ 变成可训练参数，用任务损失直接驱动（并加压缩率约束项）。

1. **权重更新的理论路线不同**

- SVD-LLM v2：更多是“对 $W$ 做截断/白化/重加权”，在权重域一次到位。
- SAES-SVD：在权重域压缩，同时显式建模局部/累计误差并抑制。
- Dobi-SVD：先构造“投影权重集合” $W_{p,i}=WV_{A_i}G_kV_{A_i}^\top$，再求公共子空间 $V$（PCA/IPCA）得到 $W_f=WVV^\top$ 的理论最优形态。

1. **对“传统 SVD 的硬限制”的处理不同**

- SVD-LLM/SAES：通常接受 $k(m+n)/(mn)$ 这条存储规律，低压缩比时确实被迫砍大半奇异值。
- Dobi-SVD：通过 remapping + 量化，把存储结构改写为 $k\cdot\max(m,n)$，从而让 $k$ 的可用范围更大。

------

## 5）一些可延展的 idea（只给方向，不写细融合方案）

### idea 1：把“可训练的 $k$”从 tanh 门控升级为“更贴近秩约束”的连续松弛

tanh 门控本质是在奇异值谱上做软 mask。你可以考虑用：

- Hard-concrete / L0-style 门控（更稀疏、更接近离散）
- 或直接把目标写成 $\sum_i s_i$ 的预算（$s_i\in[0,1]$ 是连续保留系数），再加上排序/单调约束，逼近“前 $k$ 个保留”的结构。

### idea 2：把 SAES-SVD 的“累计误差抑制”引入到 $k$ 的训练目标里

Dobi-SVD 的训练目标是 $L_{\text{task}}+\gamma|R-R_{\text{tar}}|$。
 你可以把“误差传播风险”作为第三项正则，例如用一个可计算的代理（比如层间敏感度、或某种二阶近似）约束“k 的分配不要让误差在后段爆炸”。

### idea 3：把“Activation → Weight”的 PCA 子空间，做成 attention-aware / layerwise 分布漂移鲁棒

Dobi-SVD 用 $\{V_{A_i}\}$ 做 PCA 得公共子空间。一个潜在问题是：校准样本分布变了，子空间可能漂移。
 可以做：

- 分层/分 token 段的加权 PCA（对重要 token 权重大）
- 或用子空间对齐的鲁棒估计（例如对 $V_iV_i^\top$ 的几何中值而不是算术平均）

### idea 4：remapping 的“量化塞入”是很强的工程点，但也可能变成理论点

它隐含了一个假设：$U,V$ 分布近似“高斯且集中”，量化误差可控。你可以尝试更理论化地刻画：

- 量化误差对最终 $W_f$ 的谱扰动上界（比如 $\|\Delta W\|_2$ 上界）
- 以及它与任务损失之间的联系（用 Lipschitz 或局部二阶项）。

### idea 5：Dobi-SVD 的“激活截断最优”结论，可推广到 LatentLLM 那种“模块组合量”上

Dobi-SVD 强调对 $A$ 截断更优；LatentLLM 强调对 attention map / 组合矩阵截断更贴近功能。
 一个自然方向是：把 Dobi 的可导 $k$ 训练，搬到“组合量的谱空间”里（比如对 $QK^\top$ 或某个模块输出张量做可导截断），再做对应的权重反演/公共子空间更新。

