# MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression

（核心：把“低秩 + 混合精度量化”统一成一个约束优化问题；用二阶近似把全局问题拆成“层内候选生成 + 层间全局分配”；最后用一个“低秩感知的 AdaRound 变体”做误差收敛）。Gordon 等 - 2025 - MLoRQ Bridgin…

------

## 1）统一问题：把“量化”与“低秩”写成同一类可选操作

对第 $ℓ$ 个全连接层：

- 输入输出：
  $$
  y_ℓ = W_ℓ x_ℓ + b_ℓ
  $$

  $$
  W_ℓ ∈ R^{d^y_ℓ × d^x_ℓ}
  $$

  

MLoRQ 将压缩后的权重选项定义为两类集合的并集：

1）仅量化（Quantization-only）：

- 给定位宽 $b^W_ℓ ∈ B$（候选集合如 {2,3,4,6,8}）
- 量化算子 $Q(W, ϕ, b)（ϕ=(s,z)$ 为 scale/zero-point）
- 得到：$W^f_ℓ = Q(W_ℓ, ϕ_{W_ℓ}(b^W_ℓ), b^W_ℓ)$

2）低秩 + 分解后分别量化（Low-rank + Quantization）：
 先做 $rank r_ℓ$ 分解：$W_ℓ ≈ A^{(r)}_ℓ B^{(r)}_ℓ$
 其中 $A^{(r)}_ℓ ∈ R^{d^y_ℓ × r_ℓ}, B^{(r)}_ℓ ∈ R^{r_ℓ × d^x_ℓ}$

再对 A、B 分别量化（位宽可不同）：

- $Ã^{(r)}_ℓ(b^A_ℓ)=Q(A^{(r)}*ℓ, ϕ^{(r)}*{A_ℓ}(b^A_ℓ), b^A_ℓ)$
- $B̃^{(r)}_ℓ(b^B_ℓ)=Q(B^{(r)}*ℓ, ϕ^{(r)}*{B_ℓ}(b^B_ℓ), b^B_ℓ)$

最终低秩量化组合权重：

- $W^f_ℓ = Ã^{(r)}_ℓ · B̃^{(r)}_ℓ$

因此每层的“可选压缩方案集合”是：
$$
W_ℓ^{options} = W_ℓ^Q ∪ W_ℓ^{LQ}
$$
这一步把“只量化”和“低秩+量化”都统一成同一个离散选择问题。

------

## 2）原始全局优化目标：在权重内存约束下，最小化任务损失退化

论文把整个网络的压缩配置写成：
 $S = (W^f_1, …, W^f_L)$，其中 $W^f_ℓ ∈ W_ℓ^{options}$

给定权重内存约束 $ψ_WMS$​，目标是：
$$
min_S  L(w + Δw(S))
$$

$$
s.t.  WeightsMemory(S) ≤ ψ_WMS
$$

其中 $Δw(S)$ 表示由压缩引入的权重扰动（把所有层的 $ΔW_ℓ$ 向量化拼起来）。

这个问题直接求解几乎不可能：搜索空间巨大（每层 $rank×(b_A,b_B)$ + 量化-only 的 $b_W$）。

------

## 3）从全局目标到可解形式：二阶泰勒近似 + 层独立假设（把问题拆开）

### 3.1 二阶近似：用 Hessian 把“精度退化”变成可计算的二次型

对任务损失做二阶展开（以权重向量 $w$​ 为变量）：
$$
ΔL = g^T Δw + Δw^T H_w Δw + O(‖Δw‖^3)
$$
假设在预训练模型附近 $g≈0$（常见 PTQ 假设），则：
$$
ΔL ≈ Δw^T H_w Δw
$$


### 3.2 层独立近似：把全局误差拆成逐层和

进一步假设各层压缩误差贡献近似独立（类似很多 Hessian-aware 量化/剪枝工作），则：
 $ΔL ≈ Σ_{ℓ=1..L}  Δw_ℓ^T H_{w_ℓ} Δw_ℓ$

这一步是 MLoRQ 的关键“可分解”桥梁：
 全局目标 → 层内误差度量（local loss） + 层间资源分配（memory budget）。

并且 Hessian 采用 Label-Free Hessian（无需标签的 Hessian 估计）来获得对角近似。

------

## 4）改进后的层内目标：Hessian 对角加权的局部误差 Λ_ℓ（用于生成候选）

MLoRQ 在层内阶段只用 Hessian 的对角项（把 H_{w_ℓ} 当对角矩阵），得到层内局部损失：
$$
Λ_ℓ(W^f_ℓ) = ‖ D_ℓ ⊙ (W_ℓ − W^f_ℓ) ‖_F^2
$$
其中：

- ⊙ 为逐元素乘
- $D_ℓ(i,j) = sqrt( H_{w_ℓ}[i+n·j, i+n·j] )$（即 Hessian 对角开方后 reshape 成矩阵）
- 这使得对“更敏感”的权重元素，误差惩罚更大

层内阶段的目标可以写成：
 对所有 $(r_ℓ, b^A_ℓ, b^B_ℓ) $或 $(b^W_ℓ)$：
$$
min Λ_ℓ(W^f_ℓ)
$$
注意：这还不是最终全局配置，只是为每层找一小撮“可能最优”的候选集合（Pareto front），以便后面做层间优化。

------

## 5）层内关键：Hessian-aware SVD（从难解的加权低秩到一个可做的上界 + SVD）

如果只考虑低秩、不考虑量化，层内低秩子问题是：
$$
min_{A^{(r)}_ℓ, B^{(r)}_ℓ}  Λ_ℓ(A^{(r)}_ℓ B^{(r)}_ℓ)
$$
对所有 rank r

但这个“逐元素 Hessian 加权”的低秩问题一般没有直接闭式解。

论文做法：用一个更粗但可解的上界，把逐元素权重 $D_ℓ$ 压缩成“按输出通道缩放”的矩阵 $Q_ℓ$：

定义对角矩阵 $Q_ℓ$，使：
 $[Q_ℓ]*{i,i} = Σ_j [D_ℓ]*{i,j}$

（即把第 i 个输出通道对应行上的 Hessian 权重求和，得到每个输出通道一个缩放因子。）

然后给出上界形式：
$$
Λ_ℓ(A^{(r)}_ℓ B^{(r)}_ℓ) ≤ ‖ Q_ℓ (W_ℓ − A^{(r)}_ℓ B^{(r)}_ℓ) ‖_F^2
$$
于是改为最小化这个上界：
$$
min_{A^{(r)}_ℓ, B^{(r)}_ℓ} ‖ Q_ℓ W_ℓ − Q_ℓ A^{(r)}_ℓ B^{(r)}_ℓ ‖_F^2
$$
这时就能用“加权低秩分解”的经典技巧：对 $Q_ℓ W_ℓ$ 做普通 SVD：
$$
Q_ℓ W_ℓ = U_ℓ Σ_ℓ V_ℓ^T
$$
构造低秩因子（论文选择一种“利于按输出通道量化”的缩放分配）：
$$
A_ℓ = Q_ℓ^{-1} U_ℓ
$$

$$
B_ℓ = Σ_ℓ V_ℓ^T
$$

再取前 r 列/行得到 rank-r 版本。

这里有个很实用的工程动机：权重量化是“按输出通道”设 scale，$Q_ℓ$ 的通道缩放可以被吸收到量化 scale 里，从而尽量不引入额外量化噪声。

------

## 6）层内量化参数选择：分别为 W / A / B 选 ϕ（scale、zero-point）

### 6.1 量化-only（$W_ℓ$）

对每个候选位宽 b，选使 Hessian 加权 MSE 最小的 $ϕ$：
$$
ϕ_{W_ℓ}(b) = argmin_ϕ ‖ D_ℓ ⊙ ( W_ℓ − Q(W_ℓ, ϕ, b) ) ‖_F^2
$$


### 6.2 低秩分解后（$A_ℓ、B_ℓ$）

目标是让最终 $Ŵ_ℓ = Q(A_ℓ,ϕ_A,b_A) · Q(B_ℓ,ϕ_B,b_B)$ 的 Hessian 加权误差尽量小。

直接联合最小化代价很高，所以拆成两步：

1）对 A：直接最小化 Hessian 加权误差（B 先固定）：
$$
ϕ_{A_ℓ}(b) = argmin_ϕ  Λ_ℓ( Q(A_ℓ,ϕ,b) · B_ℓ )
$$
2）对 B：用一个上界把目标简化为“逼近 B 本身”的量化误差（降低搜索成本）：
$$
Λ_ℓ( A_ℓ · Q(B_ℓ,ϕ,b) ) ≤ ‖ B_ℓ − Q(B_ℓ,ϕ,b) ‖*F^2
$$
 于是：
$$
ϕ*{B_ℓ}(b) = argmin_ϕ ‖ B_ℓ − Q(B_ℓ,ϕ,b) ‖_F^2
$$
实现上用网格/线搜索（并配合 outlier percentile 搜索）来选最优$ ϕ$。

------

## 7）层内 Pareto 集：把“误差-内存”做多目标筛选，极大缩小搜索空间

对每个候选压缩方案 $W^f_ℓ$，计算两件事：

1）局部误差：$Λ_ℓ(W^f_ℓ)$
 2）内存代价：$M_ℓ(W^f_ℓ)$

内存的闭式写法很关键（MLoRQ 明确把低秩+量化也算进权重内存）：

若是低秩+量化：
$$
M_ℓ = r_ℓ ( d^y_ℓ · b^A_ℓ + d^x_ℓ · b^B_ℓ )
$$
若是量化-only：
$$
M_ℓ = d^y_ℓ · d^x_ℓ · b^W_ℓ
$$
然后定义支配关系：方案 W’ 支配 W 若

- $Λ_ℓ(W’) ≤ Λ_ℓ(W)$ 且 $M_ℓ(W’) ≤ M_ℓ(W)$
- 且至少一个严格更好

保留下来的非支配集合就是 Pareto 集 $P_ℓ$。
 这一步把“每层成千上万组合”的候选，缩成几十/几百个“可能全局最优”的点，为层间 ILP 做准备。

------

## 8）层间全局优化：在总内存约束下，选每层一个候选，使整体误差最小

### 8.1 用 SQNR 做层间目标（比纯 Hessian 累加更稳定）

论文层间阶段倾向用 SQNR（信噪比）来衡量某层压缩的影响：
$$
Ψ_ℓ(W^f_ℓ) =  ‖ f_ℓ(W_ℓ) ‖_2^2  /  ‖ f_ℓ(W_ℓ) − f_ℓ(W^f_ℓ) ‖_2^2
$$
其中 $f_ℓ$ 表示：只压缩第 $ℓ$ 层，其余层保持浮点时的网络输出（或层输出），用校准集估计。

层间问题写成：
$$
max_{W^f_1∈P_1,…,W^f_L∈P_L}  Σ_ℓ Ψ_ℓ(W^f_ℓ)
$$
 $s.t.  Σ_ℓ M_ℓ(W^f_ℓ) ≤ ψ_WMS$

实现上为了变成标准 ILP，论文用等价的最小化形式：
$$
Φ_ℓ(W^f_ℓ) = 1 / Ψ_ℓ(W^f_ℓ)
$$
并用指示变量 $I_ℓ(W^f_ℓ)∈{0,1}$ 表示第 $ℓ$ 层选了哪个候选，约束每层必须选一个：
$$
Σ_{W^f_ℓ∈P_ℓ} I_ℓ(W^f_ℓ) = 1
$$
最终 ILP：
$$
min Σ_ℓ Σ_{W^f_ℓ∈P_ℓ} I_ℓ(W^f_ℓ) · Φ_ℓ(W^f_ℓ)
$$
 
$$
s.t. Σ_ℓ Σ_{W^f_ℓ∈P_ℓ} I_ℓ(W^f_ℓ) · M_ℓ(W^f_ℓ) ≤ ψ_WMS
$$

$$
I_ℓ(·)∈{0,1}
$$



### 8.2 一个很实用的加速：层间/层内指标插值，减少整网推理次数

如果 $P_ℓ$ 很大，逐候选跑完整推理代价太高。论文提出对指标做线性插值：

先对每个$ (b^A_ℓ,b^B_ℓ) $预算只评估 $k_inf$ 个点，形成稀疏网格 $G_ℓ(b^A,b^B)$。其余点通过相邻两个网格点插值：
$$
Φ_ℓ(W^f) ≈ α Φ_ℓ(W^{f(l)}) + β Φ_ℓ(W^{f(h)})
$$
 其中 $β、α=1-β$ 由局部误差 $Λ_ℓ$ 在两点之间的位置决定。

其动机来自一阶近似：若权重扰动足够小，
 $‖ f(w_ℓ) − f(w_ℓ + Δw_ℓ) ‖_F^2 $近似与$ ‖Δw_ℓ‖_F^2$ 成正比，
 因此指标随“扰动幅度/局部误差”变化相对平滑，插值误差可控。

------

## 9）最终误差收敛：LoRAda（Low-Rank aware Adaptive Rounding）

前面两阶段决定了每层用“量化-only”还是“低秩+量化”，以及对应位宽与 rank。但在超低比特时，rounding（向上/向下取整）造成的误差会非常大，因此引入 LoRAda：一个考虑“分解后两块矩阵 A、B”的 AdaRound 变体。

### 9.1 软量化（soft quantizer）

标准量化$ Q(W) $里面有 round。AdaRound 用一个可学习的$ h(V)$​ 替代 hard rounding，定义：
$$
Q_S(W, V, ϕ, b) = s( clip( floor(W/s) + h(V) + z, 0, 2^b−1 ) − z )
$$

$$
h(V) = clip( σ(V)(ξ−γ)+γ, 0, 1 )
$$

 其中 $σ$ 是 $sigmoid，ξ=1.1、γ=−0.1$（让 $h$ 更容易收敛到 0 或 1）。

### 9.2 LoRAda 的层内优化目标（把 A、B 的 rounding 一起调）

设 $W ≈ AB$（这里 A,B 是已确定 rank 的低秩因子），定义可学习变量 $V_A、V_B$，对应 A、B 的 rounding。

令
$$
Ã = Q_S(A, V_A, ϕ_A, b_A)
$$

$$
B̃ = Q_S(B, V_B, ϕ_B, b_B)
$$

优化：
$$
min_{V_A, V_B}  ‖ W x − Ã B̃ x ‖_F^2  + λ( Ω(V_A) + Ω(V_B) )
$$
其中 $Ω(·)$ 是促使$ h(V)$ 变成 0/1 的正则（用 annealing 逐步加强），本质就是“把 rounding 决策学出来”，但仍属于 PTQ 范畴（只用校准数据、无需标签）。

------

## 10）Step 实现过程：按论文流程拆成可落地的步骤

下面按“你要复现/移植到 LLM”那种颗粒度写：

### Stage 0：校准准备

1. 取小校准集 D（图像/文本都行，论文里图像常用 32～1024）
2. 计算/估计每层权重的 Label-Free Hessian 对角（得到 D_ℓ 或其通道压缩版 Q_ℓ）

### Stage 1：层内候选生成（对每一层 ℓ）

1. 构造通道加权矩阵 $Q_ℓ$（按输出通道聚合 Hessian 权重）
2. Hessian-aware SVD：对$ Q_ℓ W_ℓ$ 做 SVD 得 $U,Σ,V$
3. 组装因子：$A_ℓ=Q_ℓ^{-1}U，B_ℓ=ΣV^T$，并枚举 rank r（取前 $r$）
4. 对每个候选位宽 b：
   - 量化-only：用 Hessian-MSE 搜索 $ϕ_{W_ℓ}(b)$
   - 低秩：分别搜索 $ϕ_{A_ℓ}(b_A)、ϕ_{B_ℓ}(b_B)$
5. 枚举所有压缩选项$ W^f_ℓ ∈ W_ℓ^{options}$，计算：
   - 局部误差 $Λ_ℓ(W^f_ℓ)$
   - 内存代价 $M_ℓ(W^f_ℓ)$
6. 取 Pareto 集 $P_ℓ$（非支配解集合）

### Stage 2：层间全局分配

1. 对每层每个候选 $W^f_ℓ ∈ P_ℓ$，评估 SQNR（或 NMSE）指标 $Φ_ℓ$
   - 若候选多，用插值策略减少推理次数
2. 解 ILP：在总内存约束 $ψ_WMS$ 下，为每层选一个候选，得到全局方案 $S$

### Stage 3（可选）：LoRAda rounding 精修

1. 对采用量化的层（包含量化-only 与低秩+量化），按层顺序做 LoRAda：

- 固定 scale/zero-point
- 优化 $V$（或 $V_A,V_B$），最小化重构误差 + rounding 正则

1. 如果你还叠加了其它激活量化修正（如 RepQ/ERQ），通常顺序是：

- 先做激活侧修正（重参数化、ridge 回归校正等）
- 再重新估计 Hessian / 重新做 LoRAda（论文也强调会重算）

------

## 11）一些可以延展的 idea（先给方向，不展开细融合建议）

1）把“rank 与位宽”的离散分配，从“纯内存约束”扩展到“内存 + 时延/吞吐”双约束
 把 $M_ℓ$ 换成同时包含：

- 权重内存（HBM/Flash）
- 计算代价（FLOPs 或 kernel latency 预测）
   形成多约束 ILP / 多目标 ILP，更贴近端侧部署。

2）把层内误差 $Λ_ℓ$ 从“权重域 Hessian 加权”升级到“激活域/输出域”的混合度量
 现在 $Λ_ℓ$ 是 $D_ℓ ⊙ (W−W^f) $的加权 Fro 误差，本质更像“权重扰动的二阶代理”。你可以构造：
 $Λ̃_ℓ = ‖ (W−W^f) X_ℓ ‖_F^2 $ 或  $‖ (W−W^f) H_ℓ^{1/2} ‖_F^2$
 把激活协方差 $H_ℓ=E[X X^T] $引入，使 rank 选择更“数据感知”，并与低秩方法（尤其是 activation-aware / truncation-aware SVD）更自然对齐。

3）LoRAda 可以做“结构化 rounding”：针对低秩 A、B 的谱结构加先验
 例如对 A、B 的 rounding 变量加一个“保持奇异值谱形状”的约束/正则，使得 rounding 更像“谱域微调”而不是逐元素二值决策，可能减少极低 bit 的不稳定。

4）把 Pareto 生成从“穷举所有 $r×(b_A,b_B)$”改成“连续松弛 + 投影”
 比如先在连续域优化一个代理（把 rank 用核范数或谱截断连续化、把 bit 用可微 proxy），得到少量候选，再投影回离散集合，这样能进一步降低 layer 内搜索成本。