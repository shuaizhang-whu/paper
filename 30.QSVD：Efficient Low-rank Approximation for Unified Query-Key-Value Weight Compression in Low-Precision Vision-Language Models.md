# QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models

## 1）问题与原始优化目标：VLM 推理的瓶颈在 QKV 计算与 KV cache

在 VLM（图像 token + 文本 token 拼接后进 LLM）中，自回归解码阶段每步都要用注意力，而注意力的主要开销来自：

- Q/K/V 线性投影权重：$W_q,W_k,W_v\in\mathbb{R}^{E\times E}$
- KV cache：存储历史 token 的 $K,V$，大小 $\eta_{\text{fp}}=2LE$（$L$ 为上下文长度）
- 计算：每层做 $XW_q, XW_k, XW_v$，若 $X\in\mathbb{R}^{L\times E}$，则 FLOPs（或 MACs）级别 $\gamma_{\text{fp}}=3LE^2$
- 权重参数量：$\alpha_{\text{fp}}=3E^2$

从“压缩”的角度，一个很自然的原始目标是：在给定硬件预算（权重/算力/cache）下降低任务损失或输出误差：
$$
\min_{\text{compression}} \ \mathbb{E}_{(x,y)}[\mathcal{L}(f_{\text{compressed}}(x),y)]
\quad \text{s.t.}\quad
(\alpha,\gamma,\eta)\le (\bar\alpha,\bar\gamma,\bar\eta)
$$
传统做法是对 $W_k,W_v$（有时不动 $W_q$）分别做低秩分解并截断 rank $r$。

------

## 2）基线：分别对 Q/K/V 做 SVD（或只对 K/V）

对单个矩阵 $W\in\mathbb{R}^{E\times E}$，SVD 截断 rank $r$：
$$
W \approx U_r\Sigma_rV_r^\top \equiv W_dW_u
$$
其中一种常见写法是把 $\Sigma_r$ 分到两边（便于量化/数值稳定）：
$$
W_d = U_r\Sigma_r^\beta,\quad
W_u = \Sigma_r^{1-\beta}V_r^\top,\quad 0\le \beta\le 1
$$
如果对 $W_q,W_k,W_v$ 分别做（图中称 ind scheme），则：

- 权重规模：$\alpha_{\text{ind}}=6rE$
- 计算：$\gamma_{\text{ind}}=6LrE$
- KV 中间缓存（为重算 KV 或存低秩中间量）：$\eta_{\text{ind}}=2rL$

问题：三次 down-proj（或两次）重复读 $X$，且 cache 仍然要为 K 与 V 分开存。

------

## 3）QSVD 的核心改进 1：把 QKV 拼接后做一次联合 SVD，实现“共享 down-projection + latent cache”

### 3.1 从“分别分解”到“联合分解”的目标变化

QSVD 的关键是把 QKV 的三个权重拼在一起：
$$
W_{\text{concat}}=[W_q,\ W_k,\ W_v]\in\mathbb{R}^{E\times 3E}
$$
对这个大矩阵做 rank-$r$ 截断 SVD（论文 Eq.2）：
$$
[W_q,W_k,W_v]=W_{\text{concat}}
\approx W_d^{(r)}\ \Sigma_r\ (W_u^{(r)})
$$
然后继续用 $\beta$ 把奇异值分配到两边（论文 Eq.3–4）：
$$
W_{d,qkv}=W_d^{(r)}\Sigma_r^\beta,\qquad
W_{u,qkv}=\Sigma_r^{1-\beta}W_u^{(r)}
$$
并把 $W_{u,qkv}$ 切成三块：
$$
W_{u,qkv}=[W_{u,q},\ W_{u,k},\ W_{u,v}],\quad
W_{u,q},W_{u,k},W_{u,v}\in\mathbb{R}^{r\times E}
$$
于是得到（论文 Eq.4）：
$$
[W_q,W_k,W_v]\approx W_{d,qkv}\ [W_{u,q},W_{u,k},W_{u,v}]
$$
这一步的“优化直觉”是：在同一注意力层里，Q/K/V 的投影子空间高度相关，联合做低秩比三次独立做更能复用主子空间，从而在同等 rank 下误差更小、且硬件更省。

### 3.2 推理时的 latent cache：存 $C_{qkv}$ 而不是存 $K,V$

对输入 $X\in\mathbb{R}^{L\times E}$，先做一次共享 down-projection：
$$
C_{qkv}=XW_{d,qkv}\in\mathbb{R}^{L\times r}
$$
之后分别重建：
$$
Q=C_{qkv}W_{u,q},\quad
K=C_{qkv}W_{u,k},\quad
V=C_{qkv}W_{u,v}
$$
解码阶段缓存的不是 $K,V$，而是 $C_{qkv}$（论文 Eq.5）。

因此硬件指标变为：

- 权重：$\alpha_{\text{qsvd}}=4rE$
- QKV 生成计算：$\gamma_{\text{qsvd}}=4LrE$（一次 down + 一次 up（拼接））
- cache：$\eta_{\text{qsvd}}=rL$（比 FP16 的 $2LE$ 小很多）

这解释了论文中强调的点：在满足 $r<0.75E$ 时，权重与计算都严格比原始方案省。

------

## 4）QSVD 的核心改进 2：跨层 rank 分配，用“奇异值重要性”做全局截断（从目标到可计算公式的推导）

联合 SVD 后，关键变成：每层该保留多少奇异值（rank），以及跨层如何分配总预算 $k$（论文称 rank budget）。

### 4.1 原始形式：截断某个奇异值对 loss 的影响（泰勒一阶）

对某层权重 $W$ 的 SVD：
$$
W=\sum_{i=1}^{E}\sigma_i u_iv_i^\top
$$
若“截断第 $i$ 个奇异值”（令 $\sigma_i=0$），则权重变化为（论文 Eq.6）：
$$
\Delta W_{\sigma_i}=W-W'_{\sigma_i}=\sigma_i u_iv_i^\top
$$
令 $G_W=\frac{\partial \mathcal{L}}{\partial W}$，用一阶近似（论文 Eq.7–8）：
$$
\Delta \mathcal{L}_{\sigma_i}
\approx \langle \Delta W_{\sigma_i},\ G_W\rangle_F
$$
为了得到“重要性”，论文用校准集上 $(\Delta \mathcal{L})^2$ 的期望作为经验 Fisher 对角近似（论文 Eq.9）：
$$
\hat I_{\sigma_i}
=\mathbb{E}_{x\sim \mathcal{D}}\big[(\Delta \mathcal{L}_{\sigma_i})^2\big]
\approx \frac{1}{N}\sum_{n=1}^N
\Big(\langle \Delta W_{\sigma_i},\ G_W^{(n)}\rangle_F\Big)^2
$$
这就是“理论目标”：保留那些 **截断会让 loss 上升最大的奇异值**。

### 4.2 计算瓶颈与改进：从 $O(E^3)$ 内存到 $O(E^2)$ 的等价公式

直接算 $\Delta W_{\sigma_i}$ 是全矩阵，所有 $i$ 都存会爆内存（每层 $O(E^3)$）。QSVD 把内积化简：
$$
\langle \sigma_i u_iv_i^\top,\ G\rangle_F
=\sigma_i\ \mathrm{tr}((u_iv_i^\top)^\top G)
=\sigma_i\ \mathrm{tr}(v_iu_i^\top G)
=\sigma_i\ (u_i^\top G v_i)
$$
而如果把奇异向量堆成矩阵 $U=[u_1,\dots],V=[v_1,\dots]$，则
$$
[U^\top G V]_{ii}=u_i^\top G v_i
$$
因此得到论文 Eq.10 的可计算形式：
$$
\hat I_{\sigma_i}
=\frac{1}{N}\sum_{n=1}^N
\sigma_i^2\ \big([U^\top G_W^{(n)} V]_{ii}\big)^2
$$
这一步非常关键：只需要存 $G_W$（或其批量统计）与 $U,V$，整体内存降到 $O(E^2)$。

如果 SVD 前做了白化/旋转（论文附录给了更一般形式），相当于在变换后的空间做分解，则梯度也要做相应的逆变换：
$$
\hat I_{\sigma_i}
=\frac{1}{N}\sum_{n=1}^N
\sigma_i^2\ \big([U^\top G_W^{(n)} S^{-T} V]_{ii}\big)^2
$$

### 4.3 跨层 rank 分配（全局截断）

计算出所有层、所有奇异值的 $\hat I_{\sigma_i}$ 后：

1. 把所有奇异值按 $\hat I$ 全局排序
2. 只保留 top-$k$（rank budget），其余置零
3. 每层的有效 rank 就由“它在 top-$k$ 中占了多少”自动决定

这与 SVD-LLM v2 的主要区别是：这里是 **基于校准集梯度的敏感度/Fisher 近似** 来分配 rank，而不是用解析的 truncation loss 估计去做最优截断（两者的信号来源不同：一个是经验梯度，一个是理论近似损失模型）。

------

## 5）QSVD 的核心改进 3：低秩结构下的 PTQ + 双旋转去 outlier，并学习 $\beta$ 控制 $C_{qkv}$ 的离群值

QSVD 的目标是做低比特推理（W8A8/W8A4/W4A4），但低秩结构引入了新的 outlier 问题，特别是缓存的 $C_{qkv}$。

### 5.1 低秩注意力线性部分

在 QSVD 结构中，一段线性可写为（论文 Eq.11 附近）：
$$
Y = XW_{d,qkv}W_{u,qkv}=C_{qkv}W_{u,qkv}
$$

### 5.2 双正交旋转：同时平滑 $X$ 与 $C_{qkv}$

引入两个正交矩阵 $H_1,H_2$（满足 $H^\top H=I$），重写为：
$$
Y=(XH_1^\top)\ (H_1W_{d,qkv}H_2^\top)\ (H_2W_{u,qkv})
$$
量化执行时（符号 $Q(\cdot)$ 表示量化算子），他们把量化落在 $C_{qkv}$ 与上投影上，并给出近似计算（论文 Eq.11–12）：
$$
Y' = Q(C_{qkv})\ Q(H_2W_{u,qkv})
$$
直觉：$H_1$ 负责抹平输入激活 $X$ 的通道离群值，$H_2$ 负责抹平中间表示（以及上投影相关）的离群值，让 W4A4 等极端量化也能跑得稳。

### 5.3 为什么还会有 $C_{qkv}$ outlier：$\Sigma_r^\beta$ 的“放大效应”与 $\beta$ 学习目标

即使有旋转，$C_{qkv}$ 仍会出现强 outlier。原因在于：
$$
W_{d,qkv}=W_d^{(r)}\Sigma_r^\beta,\quad
\Sigma_r^\beta=\mathrm{diag}(\sigma_1^\beta,\dots,\sigma_r^\beta)
$$
因此
$$
C_{qkv}=XW_d^{(r)}\Sigma_r^\beta
= [\sigma_1^\beta (XW_d^{(r)})_1,\ \dots,\ \sigma_r^\beta (XW_d^{(r)})_r]
$$
如果奇异值跨度很大，$\sigma_i^\beta$ 会把某些通道放得更极端，从而在 $C_{qkv}$ 里制造通道级 outlier（论文 Eq.13 的推导就是这个意思）。

所以 QSVD 提出：**不要固定 $\beta$**，而是在校准集上为每层学习最优 $\beta$ 来最小化量化前后输出差异（论文 Eq.14）：
$$
\min_{\beta}\ \sum_{d\in\mathcal{D}}\ \|Y_d - Y'_d\|_2^2
\quad\text{s.t. } 0\le \beta\le 1
$$
这一步本质上是在控制“奇异值能量分配到 down-proj 的比例”，从而在“数值稳定（少 outlier）”与“表示能力（低秩逼近精度）”之间找平衡。

------

## 6）Step 实现流程（可直接对应工程落地）

按论文整体 pipeline（joint SVD → rank allocation → PTQ）拆成可执行步骤：

### Step 1：联合 QKV SVD（逐层）

对每个注意力层：

1. 拼接 $W_{\text{concat}}=[W_q,W_k,W_v]\in\mathbb{R}^{E\times 3E}$
2. 做 rank-$r$ SVD：$W_{\text{concat}}\approx W_d^{(r)}\Sigma_r W_u^{(r)}$
3. 形成共享下投影与三个上投影：

$$
W_{d,qkv}=W_d^{(r)}\Sigma_r^\beta,\quad
[W_{u,q},W_{u,k},W_{u,v}]=\Sigma_r^{1-\beta}W_u^{(r)}
$$

### Step 2：计算奇异值重要性分数（校准集）

对每层每个 $\sigma_i$：

1. 用校准集样本前向+反向得到 $G_W^{(n)}$
2. 计算

$$
\hat I_{\sigma_i}=\frac{1}{N}\sum_{n=1}^N
\sigma_i^2\big([U^\top G_W^{(n)}V]_{ii}\big)^2
$$

### Step 3：跨层全局 rank 分配

1. 收集全模型所有 $\hat I_{\sigma_i}$
2. 全局排序取 top-$k$
3. 其余奇异值置零，相当于对各层自动分配不同有效 rank

### Step 4：推理结构（KV cache 改为 latent cache）

prefill / decode 时：

1. 计算并缓存

$$
C_{qkv}=XW_{d,qkv}\in\mathbb{R}^{L\times r}
$$

1. 需要 $K,V$ 时重算：

$$
K=C_{qkv}W_{u,k},\quad V=C_{qkv}W_{u,v}
$$

### Step 5：量化（PTQ）与旋转平滑

1. 为激活/权重引入旋转 $H_1,H_2$，把等价变换吸收到权重里以降低额外开销
2. 对权重做 per-channel 量化，对激活做 per-token 对称量化（论文实验设置）
3. 在低秩结构下按 Eq.11–12 的方式执行量化版计算

### Step 6：逐层学习 $\beta$

对每层用校准集优化：
$$
\min_{\beta}\sum_{d}\|Y_d-Y'_d\|_2^2
$$
得到 layer-wise $\beta$，用于最终部署。

------

## 7）一些可延展的 idea（先给方向，不展开到详细融合方案）

1. **把“联合 QKV”推广到“联合更多矩阵”**：例如把注意力输出投影 $W_o$ 也纳入拼接（形成 $[W_q,W_k,W_v,W_o]$），看共享子空间是否进一步提升“同等 rank 下的误差—cache—算力”折中。
2. **重要性分数从一阶扩展到二阶近似**：当前用 $(\Delta\mathcal{L})^2$ 近似对角 Fisher。可以尝试引入更接近 SVD-LLM v2 的 truncation loss 估计信号，或做块对角/低秩二阶近似，使 rank 分配更“理论可控”。
3. **学习“逐奇异值的 $\beta_i$”而不是单个 $\beta$**：现在每层一个 $\beta$。更细粒度可以设

$$
W_{d,qkv}=W_d^{(r)}\mathrm{diag}(\sigma_1^{\beta_1},\dots,\sigma_r^{\beta_r})
$$

让 outlier 控制更精确，但要注意参数与稳定性（可加 $\beta_i$ 的平滑正则）。

1. **把 latent cache 的误差当作“可控噪声/正则”来分析**：论文观察到某些设置下准确率甚至超过 FP16（并且 hallucination 指标变好）。可以围绕

$$
K,V \text{由 } C_{qkv} \text{重建的误差}
$$

建立“噪声抑制幻觉/正则化”的解释模型，并指导更稳定的 rank/量化策略。

1. **低秩 + 量化的共同设计**：当前流程是“先低秩再 PTQ + 旋转 + 学 $\beta$”。可以考虑把 rank 选择与 $\beta$ 学习放到同一目标里（多目标：精度 + outlier + cache），得到更统一的优化框架。

