# ResSVD：Residual Compensated SVD for Large Language Model Compression

这篇工作的两条主线非常清晰：

1. **Residual Compensation（残差补偿）**：不把截断后的残差 $R=W-W_r$ 当“垃圾”丢掉，而是再用一次低秩去逼近残差，把同样的 rank 预算分成两段用，从而理论上保证更小的截断误差。
2. **Partial-layer Compression（部分层压缩）**：在固定“整体压缩率”预算下，不压所有层，而是只压最后 $k$ 层，显著缓解误差从前往后累积传播的问题，并用“最终层误差最小”来选 $k$。

------

## 1）从基线目标开始：SVD-LLM 类方法在最小化什么

对任意线性层权重矩阵 $W\in\mathbb{R}^{m\times n}$，给定校准数据得到该层输入激活 $X$（把 batch/token 展开），很多后训练 SVD 压缩方法的目标可以写成“输出重构误差”：
$$
\widehat W_r = \arg\min_{W_r}\ \|WX - W_rX\|_F
$$
等价地，
$$
L = \| (W-W_r)X \|_F
$$
如果进一步采用“白化/缩放（whitening）”，可以把激活分布的影响吸收到一个缩放矩阵 $S$ 上，让截断奇异值与输出误差更直接对应（这与 SVD-LLM 的 truncation-aware whitening 思想同源）。于是你可以把问题近似理解为：在合适的缩放空间里做最优 rank-$r$ 近似。

ResSVD 在理论部分先做了一个关键重写：由于 $X$ 在实践里是固定校准集统计，优化 $\| (W-W_r)X\|_F$ 的核心仍是让 $W_r$ 尽量逼近 $W$（尤其在白化后这种关系更直接），因此把目标简化成权重逼近形式：
$$
\widehat W_r = \arg\min_{W_r}\ \|W - W_r\|
$$
这里的范数可以理解为 Frobenius 范数（论文后续用 Eckart–Young–Mirsky 定理对应的就是 Frobenius 截断误差）。

------

## 2）核心改进 1：Residual Compensation（两阶段截断）从“单次截断”变成“残差再截断”

### 2.1 直接截断（baseline）

标准截断 SVD：
$$
W = U\Sigma V^\top,\qquad
W_r = U_r\Sigma_r V_r^\top
$$
直接截断的误差（Eckart–Young–Mirsky）：
$$
\|W-W_r\|_F^2 = \sum_{i=r+1}^{\min(m,n)} \sigma_i^2(W)
$$
这就是“丢掉小奇异值”的损失。

### 2.2 ResSVD：把 rank 预算拆成两段 $r_1+r_2=r$

ResSVD 令目标总 rank 为 $r$，拆成：
$$
r=r_1+r_2
$$
**第一阶段**：对 $W$ 截断到 $r_1$：
$$
W_{r_1}=U_{r_1}\Sigma_{r_1}V_{r_1}^\top
$$
计算残差：
$$
R = W - W_{r_1}
$$
**第二阶段**：对残差 $R$ 再截断到 $r_2$：
$$
R_{r_2} = \arg\min_{\mathrm{rank}(A)\le r_2}\|R-A\|_F
$$
最终组合得到“补偿后的压缩权重”：
$$
\widehat W_r = W_{r_1}+R_{r_2}
$$
直觉上：第一段低秩抓住“主结构”，第二段低秩专门抓第一段没抓住但依然重要的残差结构——你可以把它理解为“用两组基去覆盖谱空间”，而不是把所有预算押注在一次截断上。

### 2.3 为什么它一定不差：关键不等式

定义：

- 直接截断损失：

$$
L_D=\|W-W_r\|_F^2=\sum_{i=r+1}\sigma_i^2(W)
$$

- 残差补偿损失：

$$
L_C=\|W-\widehat W_r\|_F^2
= \|R-R_{r_2}\|_F^2
= \sum_{i=r_2+1}^{\mathrm{rank}(R)} \sigma_i^2(R)
$$

ResSVD 用了一个关于残差奇异值的性质（可理解为：把 $W$ 的前 $r_1$ 个奇异方向拿走后，残差 $R$ 的第 $i$ 个奇异值不会超过 $W$ 的第 $r_1+i$ 个奇异值）：
$$
\sigma_i(R)\le \sigma_{r_1+i}(W)
$$
代入可得上界：
$$
L_C=\sum_{i=r_2+1}\sigma_i^2(R)
\le \sum_{i=r_2+1}\sigma_{r_1+i}^2(W)
=\sum_{i=r+1}\sigma_i^2(W)=L_D
$$
因此：
$$
L_C \le L_D
$$
这就是它“理论上保证更接近原矩阵”的核心论证：**同样的总 rank $r$，两阶段截断 + 组合的 Frobenius 误差不大于一次截断**。

------

## 3）核心改进 2：Partial-layer Compression（只压最后 $k$ 层）从“均匀压全层”变成“避免误差传播”

Transformer 是链式结构，层输出作为下一层输入。若第 $i$ 层压缩引入误差 $e_i$，后续层等价于在“带噪输入”上继续计算，误差会累积放大。ResSVD 的经验观察是：**压缩前面若干层**会导致“从很早就开始带误差”，最终层误差很大；而**只压最后几层**，虽然被选中的层要承担更高的单层压缩率，但误差传播路径更短，整体更稳。

### 3.1 固定整体压缩率 $R_o$ 时，单层压缩率如何换算

设模型总层数 $N$，选择压缩最后 $k$ 层。为了满足固定整体压缩率 $R_o$，被压缩的层需要承担更高的“层内压缩率” $R_l$（论文用下面关系式）：
$$
R_l = \frac{N\cdot R_o}{k}
$$
约束是 $R_l<1$（否则层内压缩率超过 100% 不可能）。

这一步很重要：它明确了“只压一部分层”不是白嫖预算，而是把预算集中到少数层上。

### 3.2 用“最终层误差最小”选择 $k$

论文进一步观察到：最终层误差与下游 zero-shot accuracy 强相关（图里给了 Kendall 相关性非常高且为负，意味着误差越小准确率越高）。

于是 Partial-layer Compression 的选择策略是：在候选集合里扫描 $k$，对每个 $k$ 做一次“临时压缩”，并计算压缩模型与原模型在校准集上**最后一层输出误差**（例如 $\|h_N-h_N'\|$），选最小者：
$$
k^\*=\arg\min_{k\in\mathcal{K}} \mathrm{Err}_{\text{final}}(k)
$$
其中 $\mathcal{K}$ 通常按步长 $s$ 取值：$k\in\{s,2s,\dots,N-s\}$。

------

## 4）把它写成可复现的 Step 实现流程（对应它的伪代码结构）

### Step 0：校准数据

随机采样校准样本（例如 256 条、长序列），用于：

- whitening / 缩放矩阵估计
- 评估候选 $k$ 的最终层误差
- （可选）做压缩后轻量 update（论文在总算法里有 UPDATE 步）

### Step 1：Partial-layer Compression 搜索 $k^\*$

对每个候选 $k$：

1. 计算层内压缩率
   $$
   R_l=\frac{N R_o}{k}
   $$

2. 复制一份模型，对最后 $k$ 层做“残差补偿压缩”（见 Step 2）

3. 用校准集前向，计算最后一层输出误差 $\mathrm{Err}_{\text{final}}(k)$

4. 记录误差最小的 $k^\*,R_l^\*$

### Step 2：Residual Compensation 压缩每个被选中的层（最后 $k^\*$ 层）

对每个要压缩的权重矩阵 $W$（论文实现里通常是对 block 内线性层统一处理）：

1. whitening：得到缩放矩阵 $S$，先在缩放空间处理
   $$
   \widetilde W = WS
   $$

2. 由压缩率 $R_l^\*$ 计算该矩阵对应的目标 rank $r$。论文给了从参数量角度推出来的换算：
   $$
   R_l = 1 - \frac{(m+n)r}{mn}
   \quad\Rightarrow\quad
   r=\frac{mn(1-R_l)}{m+n}
   $$

3. 设定中间 rank $r_1$（超参/固定值），得到 $r_2=r-r_1$

4. 第一阶段截断：
   $$
   \widetilde W_{r_1}=\mathrm{SVDTrunc}(\widetilde W,r_1)
   $$

5. 残差：
   $$
   \widetilde R=\widetilde W-\widetilde W_{r_1}
   $$

6. 第二阶段截断：
   $$
   \widetilde R_{r_2}=\mathrm{SVDTrunc}(\widetilde R,r_2)
   $$

7. 组合：
   $$
   \widehat{\widetilde W}_r=\widetilde W_{r_1}+\widetilde R_{r_2}
   $$

8. 把奇异值吸收进左右因子，存成两矩阵乘形式（部署更方便）：
   $$
   \widetilde W_{r_1}=U_{r_1}\Sigma_{r_1}V_{r_1}^\top
   \Rightarrow
   \hat U_{r_1}=U_{r_1}\Sigma_{r_1}^{1/2},\ 
   \hat V_{r_1}=\Sigma_{r_1}^{1/2}V_{r_1}^\top
   $$
   对 $r_2$ 同理，然后拼接 $[\hat U_{r_1};\hat U_{r_2}]$、$[\hat V_{r_1},\hat V_{r_2}]$ 得到总 rank $r$ 的两因子表示。

9. 反 whitening（如果实现里需要回到原空间）或直接在推理时把缩放融合进相邻层（不同代码实现会有差异）。

### Step 3：替换模型权重

- 前 $N-k^\*$ 层不变
- 最后 $k^\*$ 层用上述低秩因子替换线性层（两次 matmul）

------

## 5）这篇方法的“主要区别点”怎么理解（从方法论角度）

- 与 **SVD-LLM/ASVD**：它们的核心在于“如何让一次截断的奇异值更对应 truncation loss”（通过 whitening/缩放/映射），但**仍是一次截断**；ResSVD 在此之上强调：**哪怕你的一次截断已经最优，残差依旧有结构可挖**，用第二次截断把残差的可压缩部分再吃掉。
- 与 **AdaSVD**：AdaSVD 更像“迭代更新奇异矩阵/补偿”，ResSVD 的补偿非常直接：**补偿对象就是截断残差矩阵 $R$**，并且有简单的谱不等式保证 $L_C\le L_D$（在 Frobenius 意义下）。
- 与“层选择/层重要性分配”类：很多方法会给每层不同压缩率，但仍压全层。ResSVD 更激进：在固定整体预算下，**直接不压前面的层**，用结构性的方式减少误差传播路径。

------

## 6）一些可以往前推的 idea（不展开成具体融合建议，只给可写成新方法的方向）

### Idea 1：把 $r_1:r_2$ 从固定超参变成“按残差谱自适应分配”

现在 ResSVD 需要给定 $r_1$。你可以用第一阶段后残差的能量曲线决定 $r_2$ 的“边际收益”，例如令：
$$
g(t)=\sum_{i=1}^{t}\sigma_i^2(\widetilde R)
$$
选择 $r_2$ 使得 $g(r_2)$ 达到某个能量覆盖率，或最大化单位 rank 的收益：
$$
\Delta(t)=\sigma_t^2(\widetilde R)
$$
当 $\Delta(t)$ 下降到阈值就停止，把剩余预算回流给 $r_1$。

### Idea 2：把“残差补偿”推广到多阶段（K-stage residual SVD）

二阶段本质是：
$$
W \approx \sum_{j=1}^{2} A^{(j)},\quad \mathrm{rank}(A^{(j)})=r_j
$$
可以推广为：
$$
W \approx \sum_{j=1}^{K} A^{(j)},\quad \sum_j r_j=r
$$
每次对当前残差做截断。理论上每加一阶段，F 范数误差不增（类似逐步逼近）。工程上你可以用很小的 $K$（比如 3）在高压缩率下更稳。

### Idea 3：Partial-layer Compression 的目标从“最后层误差”升级为“任务敏感 token/位置加权误差”

现在用：
$$
\mathrm{Err}_{\text{final}}=\|h_N-h_N'\|
$$
可以改成对关键位置（例如最后若干 token、或特定 prompt 区间）加权：
$$
\mathrm{Err}=\sum_{t} w_t \|h_{N,t}-h'_{N,t}\|_2^2
$$
甚至对 logits 做 KL：
$$
\mathrm{Err}=\sum_t \mathrm{KL}(p_t\ \|\ p'_t)
$$
这样选出来的 $k^\*$ 更贴近生成质量/下游任务而不仅是隐藏态距离。

### Idea 4：模块级 partial compression（只压最后 $k$ 层中的某些子模块）

ResSVD 自己也提到目前对 block 内矩阵用了统一压缩率。你可以进一步在最后 $k$ 层里对 MLP/Attention 采用不同 $R_l$，优化成一个预算约束问题：
$$
\min \sum_{\ell\in\text{last-}k}\Big(\alpha\,\mathrm{Err}_{\ell,\text{attn}}+\beta\,\mathrm{Err}_{\ell,\text{mlp}}\Big)
\quad
\text{s.t.}\ \sum \mathrm{Cost}(r_{\ell,\cdot})\le B
$$
这会把“只压最后几层”的思想推进到“只压最后几层里最合适的部分”。

### Idea 5：把残差补偿与误差传播显式联动：用“残差驱动的层选择”

既然最终层误差与性能强相关，可以把每个候选 $k$ 的评估从“只看最终层误差”扩展为“最终层误差 + 残差能量”：
$$
J(k)=\mathrm{Err}_{\text{final}}(k)+\lambda\sum_{\ell\in\text{last-}k}\sum_{i>r_1}\sigma_i^2(\widetilde R_\ell)
$$
直觉：当残差仍很“可压缩”（能量集中）时，ResSVD 的二阶段更赚；当残差谱很平，继续压缩可能更伤。