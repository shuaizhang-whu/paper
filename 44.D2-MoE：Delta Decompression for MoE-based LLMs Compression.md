# $D^2$-MoE：Delta Decompression for MoE-based LLMs Compression

它的核心是把 MoE 的“专家差异”从“必须完整保留的整块权重”改写成“**共享基座权重 + 低秩 delta 权重**”，并且全流程 **train-free**：先用 Fisher 信息做更靠谱的 expert merge 得到基座 $W_b$，再把每个 expert 的差异 $\Delta W_i$ 用 **truncation-aware SVD（沿用 SVD-LLM 的 scale/whitening 思路）**压成低秩，最后对基座做一种“半动态结构化剪枝”（先静态、再按 batch 动态）进一步压缩与加速。Gu 等 - Delta decompression for …

------

## 1）MoE 计算形式与压缩目标

### 1.1 MoE-FFN 的前向

一个 MoE-FFN 层对输入 $x$ 的输出：
$$
y=\sum_{i=1}^{N} G(x)_i \cdot E_i(x)
$$
其中 $N$ 是专家数，$G(x)\in\mathbb{R}^N$ 是 router 的 gating 权重，$E_i(\cdot)$ 是第 $i$ 个专家（一般就是 FFN 的几层线性）。

router 通常是 top-k 稀疏选择：
$$
G(x) := \mathrm{Softmax}\big(\mathrm{TopK}(x W_g)\big)
$$
所以每个 token 只激活少数几个专家（稀疏计算是 MoE 推理效率的关键）。

### 1.2 MoE 压缩的“根矛盾”

已有两大类方法：pruning 或 merging。pruning 会不可逆丢掉专家知识；merging 会把不同专家强行揉在一起，牺牲 diversity（论文在第 1–2 页用 CKA 相似度说明专家间只有中等相似，说明“能合并的共享部分存在，但差异也很关键”）。

D2-MoE 的出发点就是：**不要把差异丢掉，也不要把差异硬塞进一个 merged expert 里，而是把差异变成“可压缩的 delta”**。

------

## 2）核心改写：专家权重的 Delta Decomposition

对每个专家的某个权重矩阵（论文记为 $W_i\in\mathbb{R}^{m\times n}$）：
$$
W_i = W_b + \Delta W_i
$$

- $W_b$：所有专家共享的 base weight，承载“共性”
- $\Delta W_i$：专家特有差异，承载“多样性/专门化”

这一步本身并没有减少参数量（还多了一个 $W_b$），所以关键在于：
 1）怎么让 $W_b$ 尽可能“代表共性”，使得 $\Delta W_i$ 更“干净、更低秩”；
 2）怎么高效压缩 $\Delta W_i$；
 3）怎么进一步压缩 $W_b$。

------

## 3）改进目标 1：用 Fisher 加权的“最优 merge”来构造 $W_b$

### 3.1 朴素平均的隐含目标

若用简单平均：
$$
W_b=\frac{1}{K}\sum_{i\in\mathcal{K}} W_i
$$
它等价于在 Frobenius 范数下最小化
$$
\min_{W_b}\ \sum_{i\in\mathcal{K}} \|W_i - W_b\|_F^2
$$
但它假设每个专家“重要性一样”，这在 MoE 不成立：路由频率、对损失敏感度都不同。

### 3.2 Fisher 加权 merge 的目标函数

论文引入 Fisher 信息作为每个专家（或其参数）的重要性权重。对第 $i$ 个专家：
$$
F_i = \mathbb{E}_{x\sim D_i}\mathbb{E}_{y\sim p_\theta(y|x)}
\Big[\big\|\nabla_{\theta_i}\log p_\theta(y|x)\big\|^2\Big]
$$
直觉：梯度越大，说明该专家参数对似然更敏感、更“关键”。

于是 base weight 用 Fisher 加权平均：
$$
W_b=\frac{\sum_{i\in\mathcal{K}} F_i W_i}{\sum_{i\in\mathcal{K}} F_i}
$$
它对应的加权最小二乘形式是：
$$
\min_{W_b}\ \sum_{i\in\mathcal{K}} F_i \|W_i - W_b\|_F^2
$$
对 $W_b$ 求导为 0 就得到上面的闭式解。

**关键点**：这样构造的 $W_b$ 会更偏向“重要专家的共性”，使得
$$
\Delta W_i = W_i - W_b
$$
更像“真正的差异”，而不是把一些关键共性误分配到 delta 里，导致 delta 更难压缩。

------

## 4）改进目标 2：对 delta 权重做 truncation-aware SVD（激活/scale 感知）

这部分直接借鉴 SVD-LLM/ASVD 的思想：用激活统计构造 scale（或 whitening）矩阵，让奇异值截断更对应真实输出误差，并缓解 activation outlier 的影响。

### 4.1 activation-scale 的构造与目标

对每个专家 $i$，收集其激活矩阵 $X_i$（通常是该线性层输入的 batch/token 采样，维度 $n\times b$）。构造 Gram：
$$
G_i = X_i X_i^\top
$$
然后取一个 scale 矩阵 $S_i$（论文写的是对 $G_i$ 做 Cholesky 得到 $S_i$）并构造加权矩阵：
$$
W_{\text{scale}} = \Delta W_i S_i
$$

### 4.2 对 $W_{\text{scale}}$ 做 SVD 并截断

$$
W_{\text{scale}} = U\Sigma V^\top
$$

截断保留前 $k$ 个奇异值（记为 $\mathrm{Trunc}(\Sigma)$），并把奇异值“吸收”进左右因子，得到低秩表示：
$$
\Delta U_i = U\sqrt{\mathrm{Trunc}(\Sigma)}
$$
于是
$$
\Delta W_i \approx \Delta U_i \Delta V_i
$$
这就是它所谓“delta decompression”：推理时不需要存完整 $\Delta W_i$，只要存 $(\Delta U_i,\Delta V_i)$ 并用两次 GEMM 还原其作用。

------

## 5）改进目标 3：对 base weight 做“半动态结构化剪枝”（列剪枝）

论文观察到 $W_b$ 有两类冗余：

- 静态冗余：某些列不管输入怎么变都几乎没贡献；
- 动态冗余：某些列重要性随输入 batch 改变。

于是提出“两阶段列剪枝”：先静态剪一半目标稀疏，再在推理时按 batch 动态剪剩下的一半。

### 5.1 列重要性度量（静态与动态共用同一公式）

对 $W_b\in\mathbb{R}^{m\times n}$，输入激活 $X\in\mathbb{R}^{n\times b}$，第 $j$ 列的指标：
$$
C_j = \|W_b[:,j]\|_2\cdot \|X[j,:]\|_2
$$
把列按 $C_j$ 升序排序，优先剪掉小的。

### 5.2 两阶段剪枝

- 静态阶段：用校准集统计一次 $C_j$，剪掉达到目标稀疏度一半的列；
- 动态阶段：推理时对每个 batch 重新计算保留列的 $C_j$，再剪掉剩下一半目标稀疏度对应的列（输入自适应）。

注意这是一种“结构化剪枝”（列剪），更容易带来可加速的稀疏/子矩阵计算收益，而不是不规则的 unstructured mask。

------

## 6）整体推理形式：共享 base + 稀疏激活的低秩 delta

对每个 token/batch，router 选 top-k 专家（记为集合 $\mathcal{T}(x)$）。前向可以写成：
$$
y = W_b x + \sum_{i\in\mathcal{T}(x)} G(x)_i \cdot \big(\Delta U_i\Delta V_i x\big)
$$
其中 $W_b x$ 可以先算一次（并在列剪枝后变成更小的有效矩阵乘），而 delta 部分只对被选中的少数专家做两次低秩乘，再按 gating 加权求和。

这也解释了它的“效率直觉”：

- 原 MoE：需要存每个专家的完整权重；
- D2-MoE：存一个 $W_b$（还可剪枝变稀疏）+ 每个专家的低秩 delta（更省存储），并且推理时只算 top-k 个 delta。

------

## 7）Step 实现流程（按工程可落地写法）

1）校准数据准备
 选一小批校准样本（论文实验用 WikiText-2 512 samples），跑 MoE 模型，记录：

- 每个专家在各层的输入激活 $X_i$（用于 $S_i$ 与剪枝度量）
- 计算 Fisher 信息所需的梯度统计（用于 $F_i$）

2）Fisher merge 得到 base weight
 对每个 MoE-FFN 的专家层（每个线性矩阵都可以独立做）：

- 计算 $F_i$
- 计算 $W_b=\frac{\sum_i F_iW_i}{\sum_i F_i}$

3）构造 delta 并做 truncation-aware SVD
 对每个专家：

- $\Delta W_i=W_i-W_b$
- 由 $X_i$ 得到 $S_i$（例如 Cholesky/谱分解）
- $W_{\text{scale}}=\Delta W_i S_i$
- SVD + 截断到 $k$，得 $\Delta U_i,\Delta V_i$

4）对 base weight 做两阶段列剪枝

- 静态：用校准激活计算 $C_j$，剪掉一半目标稀疏
- 动态：部署时对每个 batch 再算一次 $C_j$，剪掉另一半目标稀疏

5）部署推理

- 每个 batch：先走 router 得 top-k expert mask
- 计算（剪枝后）$W_b x$
- 对每个被选中的专家：计算 $\Delta U_i(\Delta V_i x)$，乘 $G(x)_i$，累加

------

## 8）它的“优化目标演化”可以怎样概括（从公式角度串起来）

你可以把 D2-MoE 的整体目标理解为：在不训练的约束下，做一个“结构化的近似 MoE”：

- 用一个共享 $W_b$ 去解释所有专家权重的公共部分；
- 用低秩 $\Delta U_i\Delta V_i$ 去解释每个专家的差异部分；
- 并对 $W_b$ 施加列稀疏约束以换取推理速度。

抽象成一个（概念）联合目标大致是：
$$
\min_{W_b,\{\Delta U_i,\Delta V_i\}}
\sum_i \underbrace{\|W_i-(W_b+\Delta U_i\Delta V_i)\|_{\star}}_{\text{重建误差}}
\quad +\quad
\lambda \underbrace{\|W_b\|_{\text{col-0}}}_{\text{列稀疏}}
$$
其中 $\|\cdot\|_{\star}$ 在他们实现中被分解为：

- $W_b$ 用 Fisher 加权的 $\ell_2$（Frobenius）闭式；
- $\Delta W_i$ 用 activation/scale 加权后的截断 SVD；
- 稀疏部分用列重要性指标做启发式两阶段剪枝。

------

## 9）一些可以延展的 idea（只给方向，不展开成详细融合方案）

1）把 Fisher merge 从“专家级标量权重 $F_i$”细化到“逐列/逐块 Fisher”
 现在是 $W_b=\frac{\sum_i F_iW_i}{\sum_i F_i}$。如果把 $F_i$ 变成与参数同形状的对角近似（或按列聚合），可以得到：
$$
W_b[j] = \frac{\sum_i F_{i,j} W_i[j]}{\sum_i F_{i,j}}
$$
更细粒度地抽取“哪些列/通道是跨专家共享的关键共性”。

2）delta 的低秩不是每个 expert 独立：可以做“共享子空间”
 论文在附录提到 delta 的 $V$ 之间 CKA 很高（接近共享），这提示可把 $\Delta V_i$ 部分共享：
$$
\Delta W_i \approx \Delta U_i \Delta V_{\text{shared}}
$$
或用分组共享（按路由频率/相似度聚类）在几乎不伤性能的情况下进一步省存储。

3）让 delta 的 rank 随层/随 expert 自适应（用路由频率或敏感度分配预算）
 如果某些专家几乎很少被路由到，它们的 delta 其实可以更激进压缩；反之高频专家保留更高 rank。一个简单的预算分配可以是：
$$
k_i \propto \mathrm{freq}(i)^\alpha \cdot \mathrm{sensitivity}(i)^\beta
$$
把总参数预算固定住做分配。

4）base 的“半动态剪枝”可以用更贴近输出误差的度量
 目前 $C_j=\|W_b[:,j]\|_2\|X[j,:]\|_2$ 是幅值型指标。可以用更“二阶/误差传播友好”的指标替代，例如把激活协方差或 Hessian/Fisher 的对角近似并入，得到更稳定的列选择。

5）把 delta decompression 与“delta trimming（裁剪部分专家 delta）”结合成多级可伸缩推理
 论文展示了逐步 trim delta 可以把压缩率推到更高，同时性能可控。你可以把它做成部署策略：根据延迟/显存动态决定保留多少个专家的 delta（甚至对不同请求不同策略）。