# LatentLLM: Attention-Aware Joint Tensor Compression

## 1. 这篇论文想解决的核心问题

传统 SVD/ASVD 类方法多是“**逐层、逐矩阵**”做低秩近似：对每个线性层 $W$ 找到 $W\approx BA$。但在 Transformer 里，很多关键模块（尤其 attention）真正决定行为的是**组合后的函数**，比如：

- QK 组合决定注意力打分：$M_i = X^\top W_{q,i}^\top W_{k,i} X$
- VO 组合决定注意力输出：$\sum_i W_{o,i}W_{v,i}(\cdot)$
- MLP 的两层之间有非线性，逐层拟合不等价于拟合整块输出

LatentLLM 的主线是：
 1）把局部 activation-aware SVD 的理论做“更严格的最优预条件”；
 2）提出“junction matrix（连接矩阵）”自由度，并用它做 **block-identity** 来减少参数/FLOPs；
 3）把压缩目标从“拟合单个矩阵/激活”升级为“拟合 attention map 或模块组合”，用 **joint / high-order（Tucker/HOSVD）** 的形式一次压多个矩阵。

------

## 2. 从朴素目标到 activation-aware（ASVD范式）再到“最优预条件”推导

### 2.1 朴素（weight-based）低秩目标

给定权重 $W\in\mathbb{R}^{d'\times d}$，做 rank-$r$ 近似 $\hat W=BA$，经典目标：
$$
L_0=\|W-BA\|_F^2
$$
最优解就是对 $W$ 做截断 SVD（取前 $r$ 个奇异值/向量）。

### 2.2 activation-aware 目标（ASVD式）

给定校准输入激活 $X\in\mathbb{R}^{d\times l}$，希望保持输出 $WX$：
$$
L_1=\mathbb{E}_X\|WX- BAX\|_F^2
$$
把期望写成二阶统计（自相关/协方差）形式，令
$$
C=\mathbb{E}[XX^\top]\in\mathbb{R}^{d\times d}
$$
则：
$$
\begin{aligned}
L_1
&=\mathbb{E}\|(W-BA)X\|_F^2 \\
&=\mathrm{tr}\Big((W-BA)\,\mathbb{E}[XX^\top]\,(W-BA)^\top\Big) \\
&=\mathrm{tr}\big((W-BA)C(W-BA)^\top\big)
\end{aligned}
$$
进一步把它等价改写成“权重乘一个矩阵”的 Frobenius 范数（关键一步）：
$$
L_1=\|WC^{1/2}-BA\,C^{1/2}\|_F^2
$$
于是令预条件（pre-conditioning）：
$$
P=C^{1/2}
$$
那么 minimizing $L_1$ 等价于对“白化后的权重”做截断 SVD：
$$
BA\,P = \mathrm{svd}_r(WP)=\mathrm{svd}_r(WC^{1/2})
$$
**LatentLLM 的点**：它强调 $P=C^{1/2}$ 是从目标严格推导出来的“最优预条件”，不是经验选择（对角近似、Hessian对角、$\ell_1$/$\ell_2$ 对角等都属于近似/次优）。

> 这一步对你后面要融合 SVD-LLM v2 / SAES-SVD 很重要：因为它提供了一个“理论上更干净”的统计加权方式，后续你如果要做误差传播/误差抑制，也可以在这个加权空间里统一度量。

------

## 3. junction matrix：从“分解不唯一”到“block-identity 降参/降算”

### 3.1 分解的不唯一性

对 $WP$ 做截断 SVD：
$$
USV=\mathrm{svd}_r(WP)
$$
此时 $U\in\mathbb{R}^{d'\times r}, S\in\mathbb{R}^{r\times r}, V\in\mathbb{R}^{r\times d}$。

通常会取 $B=US$, $A=VP^+$（或把 $S^{1/2}$ 平分到两边）。LatentLLM 说：你可以插入任意可逆的“junction matrix” $J\in\mathbb{R}^{r\times r}$：
$$
B=USJ,\qquad A=J^+VP^+
$$
只要满足 $SJJ^+=S$，误差不变（本质上就是在秩-$r$ 子空间里做可逆变换，重参数化）。

### 3.2 利用 junction 让 A 出现“恒等块”（block identity）

把 $VP^+$ 分块：
$$
VP^+ = [V_1\;V_2],\quad V_1\in\mathbb{R}^{r\times r}
$$
若 $V_1$ 可逆，取 $J=V_1$，则：
$$
A = J^+VP^+ = V_1^+[V_1\;V_2]=[I\;\;V_1^+V_2]
$$
即 $A$ 的左侧是恒等映射，不需要计算，对参数量与 FLOPs 都能直接省掉 $r^2$ 级别的项。

**这点和你熟悉的 SVD-LLM/SAES-SVD 的差异**：
 SVD-LLM v1/v2、SAES-SVD 主要在“如何选 rank / 如何补偿 truncation 误差 / 如何抑制误差传播”，但它们通常不把“分解不唯一”当成可优化自由度来系统利用；LatentLLM 把它显式化，并把“恒等块”当作一个结构化加速点。

------

## 4. 从“激活拟合”升级为“注意力拟合”：Joint QK 的目标与推导（核心）

这里是全文最关键、也最“和 SVD-LLM 系列不同”的部分：它不再仅拟合 $W_qX$ 和 $W_kX$，而是直接拟合 **attention map**（softmax 前）：

### 4.1 原始注意力打分

第 $i$ 个 head：
$$
M_i = X^\top W_{q,i}^\top W_{k,i}X
$$

### 4.2 Joint QK 的压缩形式（把 MHA 变 MLA 的思路）

设共享压缩矩阵（latent plane）：

- $A_q\in\mathbb{R}^{r_q\times d}$, $A_k\in\mathbb{R}^{r_k\times d}$

每个 head 各自的解压矩阵：

- $B_{q,i}\in\mathbb{R}^{d_h\times r_q}$, $B_{k,i}\in\mathbb{R}^{d_h\times r_k}$

则近似的 attention map：
$$
\hat M_i = X^\top A_q^\top B_{q,i}^\top B_{k,i}A_k X
$$
记
$$
G_i = W_{q,i}^\top W_{k,i}\in\mathbb{R}^{d\times d},\qquad
H_i = B_{q,i}^\top B_{k,i}\in\mathbb{R}^{r_q\times r_k}
$$
则 $\hat M_i$ 的核心是用低秩张量结构拟合 $G_i$：
$$
G_i \approx A_q^\top H_i A_k
$$

### 4.3 目标函数：attention-map 误差

论文给出的形式是：
$$
L_2=\sum_{i=1}^{h}\|M_i-\hat M_i\|_F^2
$$
为了把 $X$ 的统计纳入（attention-aware），它把误差写成“加权后的 $G_i$”的 Frobenius 范数。思路和前面 activation-aware 类似，但这次左右两侧都会出现 $C^{1/2}$（因为 $M_i$ 里有 $X^\top (\cdot) X$）：

令 $C=\mathbb{E}[XX^\top]$，则可得到等价的加权形式（直觉：$X^\top \Delta X$ 的二阶矩会引入左右各一个 $C$）：
$$
L_2 \equiv \sum_i \|C^{1/2}G_iC^{1/2} - C^{1/2}A_q^\top H_i A_k C^{1/2}\|_F^2
$$
再定义：
$$
G'_i = C^{1/2}G_iC^{1/2},\quad
A'_q = A_qC^{1/2},\quad A'_k = A_kC^{1/2}
$$
则：
$$
L_2 = \sum_i \|G'_i - A_q'^\top H_i A'_k\|_F^2
$$
这就是一个典型 **Tucker / HOSVD** 形态：

- $\{G'_i\}_{i=1}^h$ 是一个 3-mode 张量（head 维 + 两个 $d$ 维）
- $A'_q$ 和 $A'_k$ 是两个 mode 的低维子空间（tensor planes）
- $\{H_i\}$ 是 core（每个 head 一个 slice）

### 4.4 交替优化（Algorithm 1 的本质）

它用交替更新得到 $A_q, A_k$（每一步都是一个“对称矩阵求前 $r$ 个特征向量/奇异向量”）：

初始化：
$$
A_q \leftarrow \mathrm{TopRightSingular}_{r_q}\left(\sum_i G_iG_i^\top\right)
$$
迭代：
$$
A_k \leftarrow \mathrm{TopRightSingular}_{r_k}\left(\sum_i G_i^\top A_q^\top A_q G_i\right)
$$
得到 $A_q,A_k$ 后，再回推每个 head 的解压矩阵 $B_{q,i},B_{k,i}$。其中仍然保留每个 head 的 junction matrix $J_i$ 自由度，用于进一步做 block-identity/降参。

------

## 5. Joint VO 与 Joint UD（MLP）——为什么它们“有推导但效果不一定最好”

### 5.1 Joint VO（value+output）

它尝试最小化（忽略 softmax 的非线性影响或把其当作权重）：
$$
L_3=\sum_i \|W_{o,i}W_{v,i}X - \hat W_{o,i}\hat W_{v,i}X\|_F^2
$$
形式上也能写成类似 $G_i\approx B_o H_i A_v$ 的 HOSVD 问题，并给出两种计算顺序的复杂度对比（决定推理时先做 $A_vX$ 还是先做 head 内 contraction）。

但它也明确提到：**因为 attention 的非线性与权重依赖性，这个 joint VO 在实践里不如拆开压 V/O 稳定**（论文把更有效的变体放在附录里，比如更 attention-aware 的统计 $C_v, C_o$ 等）。

### 5.2 Joint UD（MLP：up+down）

这里它借鉴 SparseLLM 的“引入辅助变量把非线性解耦”的思路。两层 MLP：
$$
Z = W_uX,\quad Z'=\sigma(Z),\quad Y=W_dZ'
$$
引入辅助变量 $Z,Z'$，优化一个可分的 surrogate：
$$
L_4=\alpha\|W_uX-Z\|_F^2+\beta\|Z'-\sigma(Z)\|_F^2+\gamma\|W_dZ'-Y\|_F^2
$$
固定其他变量时有闭式解：

- $Z'$ 的更新是 ridge regression：

$$
Z'=(\gamma W_d^\top W_d+\beta I)^+\left(\beta\sigma(Z)+\gamma W_d^\top Y\right)
$$

- 若激活是 ReLU，可对 $Z$ 给出分段闭式更新（按正负号分情况）：

$$
Z^- = W_uX,\qquad
Z^+ = \frac{1}{\alpha+\beta}(\alpha Z^-+\beta Z')
$$

然后在给定 $Z$ 或 $Z'$ 时，再对 $W_u$ 或 $W_d$ 做 activation-aware SVD 更新（本质是对“有效权重”做 $C^{1/2}$ 加权的截断 SVD），交替几轮收敛。

------

## 6. 和 SVD-LLM v2 / SAES-SVD 的主要理论区别（抓大点）

我按“优化对象、误差度量、自由度、全局性”四个维度对比：

### 6.1 优化对象不同：LatentLLM直接拟合“模块组合量”（尤其 attention map）

- **SVD-LLM v1/v2**：核心仍是对单层线性映射 $WX$（或其局部误差传播版本）做 truncation-aware 处理，本质是“层级（layerwise）压缩 + 控制截断误差”。
- **SAES-SVD**：重点是“误差抑制/误差协同”，把累计误差、局部误差引入一个自适应参数求解框架，仍然主要围绕“分层压缩 + 误差项调度/抑制”。
- **LatentLLM**：在 QK 上改为拟合 $W_q^\top W_k$ 诱导的 attention map（softmax 前），属于更结构化、更贴近功能的目标；并用 HOSVD/Tucker 把多头一起压。

### 6.2 误差度量的统计加权不同：更强调 $C^{1/2}$ 的“最优性”，且在 attention 中出现双边加权

- SVD-LLM/SAES 的加权通常围绕激活统计、逐层误差传播来构造；
- LatentLLM 在 attention-map 误差里天然出现 $C^{1/2}\Delta C^{1/2}$ 这种双边结构，更像在“二次型空间”里拟合。

### 6.3 “分解不唯一”在 LatentLLM 里是可用的结构自由度（junction → block identity）

- SVD-LLM/SAES 一般把 $USV$ 固定拆成 $BA$（或固定规则分配奇异值），较少把“等价重参数化”用来减少 FLOPs/参数；
- LatentLLM 把 junction 系统化，并给出一个非常工程化的收益点：block-identity。

### 6.4 全局性：LatentLLM 的“joint”是张量分解意义上的 joint；SAES/SVD-LLMv2 的“global”更偏误差传播意义

- LatentLLM 的 joint：通过 Tucker/HOSVD 让多个矩阵共享子空间（如 $A_q,A_k$），属于“结构共享”的全局；
- SAES/SVD-LLMv2：更像“顺序压缩下误差如何不爆”的全局（误差链条/累积误差建模与抑制）。

------

## 7. step 实现过程（你如果要复现/对接到你现有框架，关键步骤长什么样）

我按模块给“最小闭环”的实现步骤（不写代码，只写流程与必要公式）：

### 7.1 准备：统计量与预条件

1. 采样校准数据，收集每层输入 $X$（或 attention 输入 token）
2. 计算（带 damping）：

$$
C = XX^\top + \lambda I,\qquad P=C^{1/2}
$$

1. 后续所有 SVD 类更新用 $WP$ 或 attention-map 目标里用 $C^{1/2}$ 双边加权

### 7.2 单层 activation-aware 压缩（local）

对每个线性层 $W$：

1. 形成 $W' = WP$
2. 截断 SVD：$U,S,V=\mathrm{svd}_r(W')$
3. 选 junction $J$，构造：

$$
B=USJ,\quad A=J^+VP^+
$$

1. 若做 block-identity：从 $VP^+$ 找合适子块 $V_1$ 并令 $J=V_1$（必要时配合 pivoting）

### 7.3 Joint QK（最重要）

对每层注意力的 Q、K：

1. 每个 head 形成 $G_i=W_{q,i}^\top W_{k,i}$（注意如果要 attention-aware，可把等价形式并入更新公式中）
2. 初始化 $A_q$
3. 交替更新 $A_k$、$A_q$（每步求 top-$r$ 特征/奇异向量）
4. 回推每 head 的 $B_{q,i},B_{k,i}$，并可对 head 级 junction $J_i$ 做 block-identity/稀疏结构化

### 7.4 Joint UD（MLP块）

对每层 MLP：

1. 用校准数据得到原始输出 $Y$
2. 初始化 $Z=W_uX$，$Z'=\sigma(Z)$
3. 交替更新 $Z'$（ridge）、更新 $Z$（ReLU 分段闭式）
4. 给定 $Z$ 更新 $W_u$ 的低秩（activation-aware SVD）；给定 $Z'$ 更新 $W_d$ 的低秩
5. 循环若干轮

------

## 8. 这篇论文能启发的 idea（先给“方向”，不展开成具体融合方案）

下面这些点，我刻意挑“能和你现有的 SAES-SVD / SVD-LLMv2 思路发生化学反应”的：

### idea 1：把“误差抑制/截断补偿”从 $WX$ 空间迁移到 **attention-map 空间**

LatentLLM 的 QK 目标本质是拟合 $G_i=W_{q,i}^\top W_{k,i}$。
 你可以想象：SAES-SVD 里对误差的自适应抑制，是在层输出空间做的；而 attention-map 的误差会更直接影响 token mixing。
 一个自然方向是：构造“attention-map 误差传播”的版本，把抑制项加在
$$
\sum_i \|C^{1/2}(G_i-\hat G_i)C^{1/2}\|_F^2
$$
上，而不是只在 $\|WX-\hat W X\|$ 上。

### idea 2：把 junction matrix 当成“可优化变量”，用 SAES 的思想做自适应选择

LatentLLM 里 junction 主要用于 block-identity（工程上很香），但它其实是一个“保持误差不变的等价变换自由度”。
 你可以考虑：在不改变局部低秩误差的前提下，用 junction 去优化某个“全局误差代理”（比如后续层的误差放大率、或 SAES 里累计误差项的上界）。这会把“结构自由度”引入误差抑制体系。

### idea 3：rank 分配从“层级”升级为“张量 plane 级”（$r_q,r_k$ 非对称、甚至 head-adaptive）

LatentLLM 明确允许 $r_q\neq r_k$。
 你可以进一步让 $r_q,r_k$ 由统计量（比如 $\sum_i G_iG_i^\top$ 的谱衰减）自适应决定，并把它接到 SVD-LLMv2 那类“截断敏感度度量”上：不是对 $W$ 的奇异值，而是对 joint 目标里的“聚合矩阵”的特征值做截断决策。

### idea 4：把 VO 的“效果不稳”变成一个研究点：用 SAES 的累计误差建模解释并修复

论文承认 joint VO 不如拆分稳定，本质原因是 attention 权重、token 相关性、非线性带来的统计失配。
 你可以尝试把它形式化成“误差项分解”：

- 统计失配误差（$C$ vs 真正的 $C_v$）
- 乘积结构误差（$W_oW_v$ 的可压缩性）
- 注意力权重引入的输入分布漂移
   然后用 SAES 的“局部+累计”误差框架去抑制其中某一类（比如以 $C_v$ 的估计来校正预条件，或对 VO 的误差做 layer-to-layer 的补偿）。

### idea 5：RoPE-aware 的 joint QK（窗口化）可以和“截断感知”结合成新的截断准则

它在附录讨论了 RoPE 情况下，理论上目标会变成对多个相对位移 $\Theta_{n-m}$ 的求和。论文给了“窗口化”近似。
 你可以把“窗口大小/位移分布”引入截断准则：
 不同位移对应不同 $\Theta$ 下的有效 $G$ 谱结构，可能导致“同样 rank 在不同位移下误差不同”。截断感知如果能把这一点编码进去，会比只看 $WP$ 的谱更贴近真实推理场景。