# FLAR-SVD: Fast and Latency-Aware Singular Value Decomposition for Model Compression

核心目标很明确：在“数据感知（activation-aware）的 Cholesky-SVD/白化SVD”基础上，解决两个痛点——(1) 协方差估计不稳导致 Cholesky 失败或分解质量差；(2) 逐层 rank 搜索太慢且不等价于真实硬件时延收益。它提出三块改进：**协方差收缩稳定化 + 单调性驱动的二分 rank 搜索 + 硬件时延感知的 rank 调整**。Thoma 等 - CVPR 2025 open access…

------

## 1）从“原始优化目标”开始：为什么要数据感知的低秩分解

考虑一层线性（或卷积/投影展开后的矩阵）权重 $W\in\mathbb{R}^{m\times n}$，输入激活（校准样本拼接）为 $X\in\mathbb{R}^{n\times N}$，原层输出为
$$
Y = WX.
$$

### 1.1 仅做权重重构（vanilla SVD）的目标

最常见的低秩压缩是直接最小化权重的 Frobenius 误差：
$$
\min_{\mathrm{rank}(\hat W)\le r}\ \|W-\hat W\|_F^2,
\quad \hat W = U_r\Sigma_r V_r^\top.
$$
这会优先拟合“权重本身的能量”，但并不保证对任务相关的特征输出 $Y$ 误差小。

### 1.2 数据感知目标：最小化输出（特征）误差

更贴近任务的是最小化输出误差：
$$
\min_{\mathrm{rank}(\hat W)\le r}\ \|(W-\hat W)X\|_F^2.
$$
令输入二阶矩（协方差/Gram）为
$$
H = XX^\top\in\mathbb{R}^{n\times n},
$$
则上式等价于一个 $H$-加权的矩阵逼近：
$$
\|(W-\hat W)X\|_F^2
= \mathrm{tr}\!\left((W-\hat W)H(W-\hat W)^\top\right).
$$
这就是“为什么要把激活统计引入分解”的根源。

------

## 2）从原目标到改进目标：Cholesky 白化把加权问题变成普通 SVD

如果我们能取一个白化因子 $S$ 使得
$$
H = SS^\top,\quad S \text{ 可逆},
$$
则可以把加权误差转成普通 Fro 范数误差。因为
$$
\mathrm{tr}\!\left((W-\hat W)H(W-\hat W)^\top\right)
= \|(W-\hat W)S\|_F^2.
$$
令
$$
\tilde W = WS,\quad \tilde{\hat W} = \hat W S,
$$
则目标变成
$$
\min_{\mathrm{rank}(\tilde{\hat W})\le r}\ \|\tilde W-\tilde{\hat W}\|_F^2.
$$
此时最优解就是对 $\tilde W$ 做截断 SVD：
$$
\tilde W = U\Sigma V^\top,\quad
\tilde{\hat W}_r = U_r\Sigma_r V_r^\top.
$$
再“去白化”得到压缩权重：
$$
\hat W_r = \tilde{\hat W}_r S^{-1}.
$$
这就是 Cholesky-SVD / 数据增强 SVD 的理论主干：把“激活加权误差”严格映射到“白化后矩阵的奇异值截断误差”。

------

## 3）FLAR-SVD 的第一个关键改进：用 Ledoit–Wolf 收缩让协方差更稳、更可 Cholesky

实际问题是：用有限样本估计的 $H$ 可能病态、甚至非正定（数值上），导致 Cholesky 不稳/失败，或者 $S^{-1}$ 极不稳定。FLAR-SVD 的解决方式是：对样本协方差做 **收缩（shrinkage）**，把极端特征值往“更温和”的谱分布拉回。

### 3.1 样本协方差估计（论文写成 raw scaling）

对 batch 内样本，先计算（以特征为维度的）样本协方差：
$$
S_{\text{raw}}=\frac{(X-\bar X)^\top(X-\bar X)}{n}.
$$
其中 $\bar X$ 是按样本均值中心化（实现中通常是 token 维度聚合后的均值）。

### 3.2 选择收缩目标：缩放后的单位阵

他们取目标矩阵为“按平均方差缩放的单位阵”：
$$
T = I\cdot \overline{\mathrm{diag}}(S_{\text{raw}}).
$$
直觉：假设特征间相关性估计不可靠时，先回退到“各向同性但尺度匹配”的估计。

### 3.3 收缩强度（shrinkage intensity）

论文给出一个强度形式（记为 $\lambda$）：
$$
\lambda
=\frac{\mathrm{Tr}(S)-\mathrm{Tr}(T)}{\|S-T\|_F^2}.
$$
（实现里通常还会对 $\lambda$ 做裁剪到 $[0,1]$，以避免过度收缩。）

### 3.4 收缩后的协方差

$$
S_{\text{shrink}}
=(1-\lambda)S+\lambda T.
$$

这样做的效果是：把协方差的极端特征值收缩向中间，显著提升正定性与条件数，从而：

- Cholesky 更容易成功：$S_{\text{shrink}}\succ 0$
- 白化与去白化更稳定：$S^{-1}$ 不会爆炸
- 在样本数较少、维度较高时尤其有效（这是论文强调的场景）

最后用
$$
S=\mathrm{chol}(S_{\text{shrink}})
$$
作为白化因子进入上一节的 Cholesky-SVD 流程。

------

## 4）第二个关键改进：利用“误差随 rank 单调”做二分 rank 搜索

逐层选 rank 是大头成本。很多方法要么统一压缩率、要么做代价高的贪心/梯度搜索。FLAR-SVD 用的是“二分搜索”，其成立依赖一个关键性质：**对同一矩阵，最优 rank-$k$ 近似的重构误差随 $k$ 单调不增**。

### 4.1 单调性为什么成立（核心数学理由）

对任意矩阵 $A$ 的 SVD：$A=U\Sigma V^\top$，最优 rank-$k$ 重构误差为
$$
\|A-U_k\Sigma_k V_k^\top\|_F^2
=\sum_{i>k}\sigma_i^2,
$$
显然 $k$ 越大，尾部项越少，误差单调下降。

在 Cholesky-SVD 的语境里，实际分解对象是白化后的 $\tilde W=WS$，因此对 $\tilde W$ 的截断同样满足这种单调性。论文图里展示“特征退化/重构误差 vs 压缩率”近似单调，从而二分可用。

### 4.2 搜索空间上界：等参数 rank（equivalent rank）

为了避免 rank 太大失去压缩意义，他们定义“等参数”的上限 rank：
$$
k_{\text{equiv}}=\frac{mn}{m+n}.
$$
因为分解为两个矩阵 $U_k\in\mathbb{R}^{m\times k}$、$V_k\in\mathbb{R}^{k\times n}$ 的参数量为 $k(m+n)$，令其等于原参数量 $mn$ 得到上式。搜索范围就是 $k\in[1,k_{\text{equiv}}]$。

------

## 5）rank 搜索的“目标函数/判据”从粗到细两套

FLAR-SVD 给了两种判据：一种非常快（不需要额外校准前向），一种更准确（需要前向对齐特征/输出）。

### 5.1 快速判据：权重重构的余弦相似度

对候选 rank $k$，构造 $A_{\text{rec}}(k)=U_k\Sigma_kV_k^\top$，用
$$
\text{sim}(k)=\cos\big(A,\ A_{\text{rec}}(k)\big)
$$
并要求
$$
\text{sim}(k)\ge \tau_{\text{sim}}.
$$
它的优点是完全不需要跑模型，缺点是只关心“参数像不像”，不一定等价于“特征/准确率影响”。

### 5.2 精确判据：用末端特征（或 logits）的 MSE 归一化层重要性

他们用一个全局统一的“末端特征差异”作为判据：把某层压缩后造成的误差传播到网络末端，用末端特征的 MSE 来衡量，从而不同层都能用同一个阈值比较。

设全精度末端特征为 $F$，压缩候选为 $F^{(k)}$，则
$$
\text{MSE}(k)=\|F^{(k)}-F\|_2^2,
$$
要求
$$
\text{MSE}(k)\le \tau_{\text{feat}}.
$$
这相当于把“层的局部改动”映射到“全局可比较的末端损害”，从而更适合做跨层的 rank 选择。

------

## 6）二分 rank 搜索的实现 step（逐层执行）

下面给一个你照着就能实现的流程（按论文 Algorithm 1 的精神展开）：

1）确定待压缩层集合（例如 ViT 的 QKV、Proj、FC1、FC2，或 CNN 的 conv/linear）并确定每层矩阵形状 $m\times n$。

2）校准数据采样：取少量 batch，hook 每层输入激活 $X$，形成 $X\in\mathbb{R}^{n\times N}$。

3）计算样本协方差：
$$
S_{\text{raw}}=\frac{(X-\bar X)^\top(X-\bar X)}{n}.
$$
4）Ledoit–Wolf 收缩：
$$
T = I\cdot \overline{\mathrm{diag}}(S_{\text{raw}}),\quad
S_{\text{shrink}}=(1-\lambda)S_{\text{raw}}+\lambda T.
$$
5）Cholesky 得白化因子：
$$
S=\mathrm{chol}(S_{\text{shrink}}).
$$
6）构造白化权重并做 SVD：
$$
\tilde W = WS,\quad \tilde W = U\Sigma V^\top.
$$
7）二分搜索 rank：

- low=1，high=$k_{\text{equiv}}$

- mid=$\lfloor (low+high)/2\rfloor$

- 构造候选重构
  $$
  \tilde W_{\text{rec}}(mid)=U_{mid}\Sigma_{mid}V_{mid}^\top
  $$
  去白化得到
  $$
  W_{\text{rec}}(mid)=\tilde W_{\text{rec}}(mid)S^{-1}.
  $$

- 用判据（余弦相似度或末端特征 MSE）计算 $v(mid)$

- 若不满足阈值（例如相似度太低 / MSE 太大），说明 rank 太小：low=mid+1
   否则 high=mid-1

- 循环直到收敛，得到满足阈值的最小 rank。

8）最终将该层替换为低秩两层（或直接存 $U_k\Sigma_k$ 与 $V_k$ 的分解形式），并继续下一层。

------

## 7）第三个关键改进：Latency-aware 的 rank 调整（让“压缩”真的变“更快”）

论文强调一个现实：FLOPs/参数降了，不一定 latency 降，甚至可能更慢（kernel 启动开销、shape 不友好、硬件对齐要求等）。因此在二分过程中插入一个“时延预测过滤器”。

### 7.1 用硬件专用回归器预测时延恢复比

对候选 rank mid，预测一个时延比值：
$$
\text{lat\_recov} = f_{\text{hw}}(\text{input\_shape},\ A,\ \text{mid})
$$

- 若 $\text{lat\_recov} > 1$：压缩后比原来更慢，直接丢弃这一侧（倾向更小 rank 或跳过该层压缩）
- 若 $\text{lat\_recov} \le 1$：再继续做相似度/MSE 的精确评估

### 7.2 rank 对齐到硬件友好粒度

很多硬件/编译器对 rank 是 8 的倍数（或 16、32）更友好。于是把 mid 映射为
$$
\text{mid} \leftarrow \text{round\_to\_multiple}(\text{mid}, 8).
$$
这一步看似工程，但对“压缩却变慢”的现象非常关键。

------

## 8）这篇论文的几个“可延展 idea”（先给方向，不展开细结合建议）

1）把“协方差收缩”推广到更复杂的统计量
 如果你后续做的是带误差补偿/对齐项的目标（例如需要 $H=XX^\top$ 以及某种 cross-term），那么不仅 $H$ 可以收缩，交叉统计（或块协方差）也可以做结构化收缩，以提升小样本下的稳定性与可复现性。

2）把“二分搜索”从“rank 单调”升级到“误差预算单调”
 现在二分的前提是：指标随 rank 单调。你可以考虑构造一个更“理论单调”的指标，比如用截断奇异值尾和来界定误差上界：
$$
E(k)=\sum_{i>k}\sigma_i^2
$$
并把特征 MSE 阈值映射为一个层内可计算的上界预算，减少前向次数。

3）把“时延预测”写成约束优化的形式
 当前是先过滤再评估。更一般可以写成：
$$
\min_{\{k_\ell\}}\ \sum_\ell \text{Degrade}_\ell(k_\ell)
\quad \text{s.t.}\quad \sum_\ell \text{Latency}_\ell(k_\ell)\le L_{\max}
$$
其中 $\text{Degrade}_\ell$ 可取末端特征 MSE 或其代理，$\text{Latency}_\ell$ 由预测器给出。这样你能把“找一组 ranks”变成一个清晰的 constrained search 问题（更像一篇方法论文的形态）。

4）用“末端特征 MSE”做层间统一标尺这一点，其实可以继续深化
 论文用末端特征帮助跨层可比。你可以把它扩展成：用多个位置的特征（例如 block 输出、head 输入、logits）组成一个加权误差向量，形成更稳的层重要性估计：
$$
\text{Score}(k)=\sum_j w_j \|F_j^{(k)}-F_j\|^2
$$
并观察它是否仍保持近似单调，从而依旧可二分。