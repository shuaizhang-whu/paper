 # NBL：Efficient Large Language Model Inference with Neural Block Linearization

**Neural Block Linearization (NBL)**（把自注意力层用线性层替换，以加速推理），按你要的结构：从“原始优化目标”到“改进后的目标/判据”，给出关键公式推导，并把实现流程拆成可落地的 step；最后给一些 idea（不做很细的融合方案展开）。 Erdogan 等 - 2025 - Efficient la…

------

## 1. 问题设定：把“注意力层”视作待逼近的黑盒算子

对第 $k$ 个 self-attention 层（或更一般的网络 block）：

- 输入激活（token 维度展开后）：

$$
X \in \mathbb{R}^{h_{\text{in}}\times N},\quad N=s\cdot t
$$

其中 $s$ 是校准序列数，$t$ 是上下文长度。

- 原注意力层输出激活：

$$
Y = A_k(X) \in \mathbb{R}^{h_{\text{out}}\times N}
$$

NBL 的核心：**用一个线性层** $\hat Y = WX+b$ 直接拟合 $A_k(\cdot)$ 的输入输出关系，从而把注意力的 $O(n^2)$ 推理复杂度替换为线性的 $O(n)$。

------

## 2. 原始优化目标：LMMSE 线性最小均方误差拟合

### 2.1 目标函数（原始优化目标）

把每列 token 表示当作随机变量采样，目标是最小化输出误差的均方：
$$
\min_{W,b}\ \text{MSE}(Y,\hat Y)
= \mathbb{E}\left[\|Y-(WX+b)\|_2^2\right]
$$
其中
$$
\hat Y = WX + b
$$
这就是标准 **LMMSE（Linear MMSE）** 形式。

------

### 2.2 闭式解（从目标到解）

记均值：
$$
\mu_X=\mathbb{E}[X],\quad \mu_Y=\mathbb{E}[Y]
$$
协方差与互协方差：
$$
C_{XX}=\mathbb{E}[(X-\mu_X)(X-\mu_X)^\top]
$$
则 LMMSE 的最优解为：
$$
W^\star = C_{YX} C_{XX}^{-1}
$$
这一步的含义很关键：NBL **不需要梯度训练**，只要在校准集上统计二阶矩，就能算出替换层参数。

------

### 2.3 误差的“可计算解析式”（为后续 bound 铺垫）

将最优 $W^\star$ 代入，MSE 可以写成（典型 LMMSE 结论）：
$$
\text{MSE}(Y,\hat Y)=\text{Tr}\left(C_{YY}-C_{YX}C_{XX}^{-1}C_{XY}\right)
$$
其中
$$
C_{YY}=\mathbb{E}[(Y-\mu_Y)(Y-\mu_Y)^\top]
$$
这个式子说明：线性可逼近性取决于 $Y$ 是否能被 $X$ 的线性子空间解释（通过互协方差项）。

------

## 3. 改进后的目标/判据：用 CCA 给“可替换性”上界，并用于选层

上面 MSE 只能告诉你“某层拟合得如何”，但论文进一步做了两件事：

1. 把误差 **归一化** 成 NMSE，便于跨层比较
2. 用 **CCA（Canonical Correlation Analysis）** 把 NMSE 的上界写成“按模态分解”的形式，得到稳定、可排序的替换准则

------

### 3.1 归一化误差 NMSE（从 MSE 改进为可比较指标）

定义：
$$
\text{NMSE}(Y,\hat Y)=\frac{\text{MSE}(Y,\hat Y)}{\text{Tr}(C_{YY})}
$$
直觉：用输出能量 $\text{Tr}(C_{YY})$ 做归一化，避免不同层输出尺度不同导致比较失真。

------

### 3.2 CCA：把“输入-输出线性相关性”分解为一组 canonical correlations

CCA 的标准形式：寻找方向 $a,b$ 使投影后相关系数最大：
$$
\max_{a,b}\ 
\rho=\frac{a^\top C_{YX} b}{\sqrt{a^\top C_{YY} a}\sqrt{b^\top C_{XX} b}}
$$
通过白化可得到“标准化互相关矩阵”：
$$
C_W = C_{YY}^{-1/2}\, C_{YX}\, C_{XX}^{-1/2}
$$
对其做 SVD：
$$
C_W = U\Sigma V^\top
$$
则奇异值
$$
\Sigma=\text{diag}(\rho_1,\rho_2,\dots,\rho_r),\quad r=\min(h_{\text{out}},h_{\text{in}})
$$
这些 $\rho_i$ 就是 canonical correlations，衡量每个模态上 $Y$ 可被 $X$ 线性预测的程度。

------

### 3.3 改进后的“判据公式”：NMSE 的 CCA 上界

论文给出（关键定理形式）：
$$
\text{NMSE}(Y,\hat Y)\ \le\ (h_{\text{out}}-r)\ +\ \sum_{i=1}^{r}(1-\rho_i^2)
$$
在大多数自注意力里常有 $h_{\text{out}}=h_{\text{in}}\Rightarrow r=h$，因此常见简化是：
$$
\text{NMSE}(Y,\hat Y)\ \le\ \sum_{i=1}^{h}(1-\rho_i^2)
$$
直觉非常清楚：

- 若某层输入输出几乎线性可预测：$\rho_i\approx 1\Rightarrow 1-\rho_i^2\approx 0$，上界很小，说明 **适合线性化替换**
- 若 $\rho_i$ 很小，上界大，替换风险大

------

### 3.4 一个实现细节：用残差输出做 bound，更贴合 block 可替换性

论文实现里常用：
$$
Y^+ = Y + X
$$
用 $Y^+$ 与 $X$ 做 CCA 计算 bound（因为真实 Transformer 子层有残差路径，替换层的影响应在残差输出尺度上评估），但线性层 $W,b$ 仍从注意力输出 $Y$ 来拟合（残差在网络结构中保留）。

------

## 4. Step 实现过程：从校准数据到“替换若干层”

下面按“你真要复现/实现”角度拆解（对应论文算法结构）。

### Step 0：准备校准集与 hooking

- 取校准数据集 $D=\{S^{(i)}\}_{i=1}^s$，每条长度 $t$
- 让样本通过模型前 $k-1$ 个 block 得到该层输入：

$$
X_k^{(i)} = f_{k-1}\circ\cdots\circ f_1(S^{(i)})
$$

并取第 $k$ 个 attention 输出：
$$
Y_k^{(i)}=A_k(X_k^{(i)})
$$

- 叠成矩阵（按 token 列拼接）：

$$
X=[X_k^{(1)},\dots,X_k^{(s)}]\in\mathbb{R}^{h\times (st)},\ 
Y=[Y_k^{(1)},\dots,Y_k^{(s)}]\in\mathbb{R}^{h\times (st)}
$$

------

### Step 1：统计均值与协方差

$$
\mu_X=\text{mean}(X),\quad \mu_Y=\text{mean}(Y)
$$

若用残差评估 bound，还需：
$$
Y^+=Y+X,\quad C_{Y^+Y^+}=\text{Cov}(Y^+,Y^+),\quad C_{Y^+X}=\text{Cov}(Y^+,X)
$$

------

### Step 2：算 CCA bound（用于“选哪些层替换”）

先做白化（一般用特征分解求 $C^{-1/2}$）：
$$
C_{XX}=V_X\Lambda_X V_X^\top\Rightarrow C_{XX}^{-1/2}=V_X\Lambda_X^{-1/2}V_X^\top
$$
标准化互相关矩阵：
$$
C_W = C_{Y^+Y^+}^{-1/2}\, C_{Y^+X}\, C_{XX}^{-1/2}
$$
SVD：
$$
C_W=U\Sigma V^\top,\quad \rho_i=\Sigma_{ii}
$$
bound 分数（常用简化）：
$$
\text{Bound}_k=\sum_{i=1}^{h}(1-\rho_i^2)
$$

------

### Step 3：按 bound 排序，选 $m$ 个最可替换层

$$
\mathcal{A}_{\text{low}}=\arg\min_{\text{choose }m}\ \{\text{Bound}_k\}
$$

------

### Step 4：对选中层计算 LMMSE 参数 $W,b$ 并替换

对每个被选中层 $k$：
$$
W_k=C_{Y_kX_k}C_{X_kX_k}^{-1},\quad
b_k=\mu_{Y_k}-W_k\mu_{X_k}
$$
然后把注意力子层替换为线性层 $\hat Y=WX+b$（残差仍保留）。

------

### Step 5：推理复杂度收益（为什么能显著加速）

如果模型共有 $K$ 个注意力层，序列长度 $n$，hidden size $d$：

- 原 attention 预填充复杂度约：

$$
O(K\cdot n^2 d)
$$

- 替换 $m$ 层后：

$$
O((K-m)\cdot n^2 d\ +\ m\cdot n d)
$$

因此 $n$ 越长、$m$ 越大，加速越明显。

KV-cache 也按被替换层数线性减少（只剩 $K-m$ 层需要存 K/V）：
$$
\text{KV}_{\text{new}}=\text{KV}_{\text{old}}\cdot\frac{K-m}{K}
$$

------

## 5. 与 SVD-LLM v2 / SAES-SVD 的主要理论区别（抓“目标、对象、误差形式、收益来源”）

### 5.1 压缩对象不同：函数近似 vs 权重矩阵低秩

- **NBL**：近似的是“注意力层这个算子”在数据分布下的输入输出映射
  $$
  A_k(\cdot)\approx (x\mapsto Wx+b)
  $$
  关键是 **激活分布上的二阶统计**。

- **SVD-LLM v1/v2 / SAES-SVD**：核心是对权重矩阵（主要是线性层）做 SVD/截断/误差补偿，本质是
  $$
  W \approx U_r \Sigma_r V_r^\top
  $$
  重点在 **权重谱结构、截断误差传播、逐层补偿/抑制误差**。

结论：NBL 不做“低秩分解”也能加速，因为它直接把 $n^2$ 的注意力计算替换掉；而 SVD-LLM 即便把 Q/K/V/O 线性层低秩化，注意力的 $QK^\top$ 仍是 $n^2$。

------

### 5.2 优化目标与判据不同：MSE/CCA bound vs 截断感知与误差抑制

- **NBL** 的优化目标明确是：
  $$
  \min_{W,b}\ \mathbb{E}\|Y-(WX+b)\|^2
  $$
  改进后的“选层判据”是：
  $$
  \text{Bound}_k=\sum_i(1-\rho_{k,i}^2)
  $$
  属于“线性可预测性/可线性化程度”的统计判别。

- **SVD-LLM v2 / SAES-SVD** 更强调“截断误差如何在网络里累计”以及“如何让低秩近似的误差可控/可抑制”（例如通过某种自适应抑制/协同误差压制策略），它们的判据通常会显式考虑：

  - 截断后残差能量（与奇异值尾部相关）
  - 层间误差传播（accumulated/local error）
  - 对下游输出敏感度（某些近似 Hessian/activation-aware 的思想）

------

### 5.3 计算收益来源不同：降低序列复杂度 vs 降低矩阵乘 FLOPs/参数量

- **NBL** 的收益来自把注意力层从 $O(n^2)$ 变 $O(n)$，对长上下文尤其明显。
- **SVD-LLM/SAES-SVD** 主要降低线性层 matmul 的 $O(d^2)$ 常数或变为 $O(dr)$，更偏“参数/算子”层面压缩。

------

## 6. 一些可写成“新论文方向”的 idea（先给方向，不展开细融合步骤）

1. **“CCA-驱动的截断感知”统一框架**
    用 NBL 的 canonical correlations $\rho_i$ 来估计“某层输出在数据分布下的有效线性子空间”，把它作为 SVD-LLM v2 的 rank 选择/截断惩罚项的外部信号：

   - 让 rank $r$ 不是只看权重谱 $\sigma(W)$，还看激活可解释性 $\rho$
   - 直观：权重低秩 ≠ 输出低维可解释；$\rho$ 可以补齐这一点

2. **“局部线性化 + 低秩化”两级压缩**
    先用 NBL 把注意力层替换成线性层 $W_{\text{NBL}}$，再对这个线性层做 SVD-LLM v2/SAES-SVD 式的低秩分解与误差抑制：
   $$
   W_{\text{NBL}}\approx U_r\Sigma_rV_r^\top
   $$
   这样同时拿到：

   - 序列复杂度降维（来自 NBL）
   - 线性层算子进一步降 FLOPs（来自低秩）

3. **“误差抑制思想”迁移到 NBL 的跨层选择**
    SAES-SVD 强调 accumulated/local error 的协同抑制。可把这种思想用于 NBL 的“选层”策略：不是一次性按 $\text{Bound}_k$ 排序，而是引入一个“累计误差预算”或“层间耦合惩罚”，优化类似：
   $$
   \min_{\mathcal{S},\{W_k\}}\ \sum_{k\in\mathcal{S}} \text{Bound}_k\ +\ \lambda\cdot \text{AccErr}(\mathcal{S})
   $$
   其中 $\text{AccErr}$ 可以建模为被线性化层的误差在残差链路中的放大/叠加。

4. **“按 head / 子空间”细粒度线性化**
    NBL 目前多是“整层 attention 替换”。可以引入 CCA 的模态分解，把“高 $\rho_i$”子空间线性化，“低 $\rho_i$”子空间保留原注意力或保留少数 head，实现混合算子：
   $$
   Y \approx P_{\text{lin}}(WX) + P_{\text{nonlin}}(A(X))
   $$
   这会更接近“可解释的结构化替换”。