# Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert Merging

## 1）问题背景：MoE 压缩的“专家冲突”来自参数空间不对齐

MoE 一层的输出（简化写法）是多个专家输出的加权和。对输入 token $x\in\mathbb{R}^d$，路由器给出每个专家的权重 $G_i(x)$，则
$$
y=\sum_{i=1}^{n}G_i(x)\,E_i(x)
$$
常见 MoE-FFN 专家可以写成（论文给了一个门控 FFN 形式）：
$$
E(x)=\big(\sigma(xW_{\text{gate}})\odot (xW_{\text{up}})\big)W_{\text{down}}
$$
MoE 压缩常见两条路：**专家剪枝**（删专家）和 **专家合并**（把多个专家融合成一个）。这篇专注“合并”。

关键难点是：MoE 专家由路由机制驱动在不同输入子分布上专门化，导致不同专家权重矩阵 $W^{(i)}$ 处于不同“参数表示空间/基底”里。于是最朴素的合并
$$
W_{\text{merged}}=\sum_{i=1}^{n}\alpha_i W^{(i)}
$$
很容易出现 **parameter conflict**：本质上是把不同基底下的向量硬相加，导致功能相互抵消或干扰，性能崩掉。

------

## 2）原始优化目标：在不微调/不搜索的前提下，尽量保持 MoE 层的输出

如果把“合并”理解为：用一个新的单专家 $W_{\text{merged}}$ 近似原来一个专家集合 $\{W^{(i)}\}$，那么一个自然目标是（对 token 分布取期望）：
$$
\min_{W_{\text{merged}}}\ \mathbb{E}_{x}\left\|\sum_{i\in\mathcal{Q}}G_i(x)\,W^{(i)}x - W_{\text{merged}}x\right\|_2^2
$$
其中 $\mathcal{Q}$ 是准备合并的那一组专家。

但直接在权重空间学一个 $W_{\text{merged}}$（尤其不允许训练）会遇到冲突：不同 $W^{(i)}$ 的“左奇异子空间/输入变换空间”不一致。

------

## 3）Sub-MoE 的改进：把“合并”从参数空间的加法，改成**共享子空间对齐后的合并**

### 3.1 改进直觉：先对齐表示空间（共享基底），再合并专家特有成分

Sub-MoE 的核心 insight：对同一组专家的权重做“联合分解”，把它们映射到一个共同子空间（共享 $U$），再只在“专家特有的投影系数”（$V$ 部分）上做融合，这样冲突会显著减少。

------

## 4）改进后的目标公式：从“直接平均”到“共享 $U$ 的联合 SVD + 频率加权合并 $V$”

### 4.1 专家功能相似性 → 先做聚类，保证“可合并性”

因为专家可能很不相似，Sub-MoE 先把专家分组：对一批校准 token 集合 $X=\{x_1,\dots,x_m\}$，计算每个专家的输出集合
$$
Y_i=\{E_i(x_1),\dots,E_i(x_m)\}
$$
用平均余弦相似度度量专家 $E_i,E_j$ 的功能相似性：
$$
\mathrm{Sim}(E_i,E_j)=\frac{1}{m}\sum_{\ell=1}^{m}
\frac{E_i(x_\ell)\cdot E_j(x_\ell)}{\|E_i(x_\ell)\|\ \|E_j(x_\ell)\|}
$$
然后用 K-means 做聚类，最小化经典 K-means 目标：
$$
J=\sum_{t=1}^{k}\sum_{E_j\in Q_t}\|Y_j-C_t\|_2^2
$$
得到每一层的专家分组 $Q_t$（论文还强调可做 multi-layer 的联合分配：不同层的“分组数”可不同，以满足整体压缩率）。

> 这一阶段的作用是把“能对齐的专家”放一起，减少后面 SVD 对齐的难度。

------

### 4.2 子空间对齐：Experts Union Decomposition（联合 SVD）

对同一组 $Q$ 内的 $n$ 个专家权重矩阵（同形状）：
$$
W^{(1)},W^{(2)},\dots,W^{(n)}\in\mathbb{R}^{O\times I}
$$
把它们按行拼接（vertical concatenation）成一个“联合矩阵”：
$$
W_{\cup}=
\begin{bmatrix}
W^{(1)}\\
W^{(2)}\\
\vdots\\
W^{(n)}
\end{bmatrix}
\in\mathbb{R}^{(nO)\times I}
$$
对 $W_{\cup}$ 做 SVD（论文式 (5) 的结构表达）：
$$
\mathrm{SVD}(W_{\cup}) = U\Sigma V^\top
$$
并把 $V^\top$（或 $V$）按专家切分成 $n$ 个块（记作 $V^{(i)}$）：
$$
V^\top=
\begin{bmatrix}
V^{(1)\top}\\
V^{(2)\top}\\
\vdots\\
V^{(n)\top}
\end{bmatrix}
$$
于是每个专家都被表示在同一组共享基底 $U$（以及共享的尺度 $\Sigma$）下，只是系数 $V^{(i)}$ 不同。

你可以把这看成一种“强约束的共同子空间拟合”：强制所有专家共享同一个输入侧的正交基，使得它们在统一坐标系里可比较、可融合。

------

### 4.3 频率加权合并：把路由器的“使用频率”变成合并权重

仅做“平均 $V$”仍然不够，因为 MoE 专家使用频率差异很大。Sub-MoE 用路由器 top-k 激活统计定义第 $i$ 个专家的采样频率：
$$
f(V_i)=\frac{\sum_{x\in X}\mathbf{1}[i\in \mathrm{TopK}(G(x),k)]}{|X|}
$$
然后做频率加权平均得到合并后的 $V$：
$$
V_{\text{merged}}
=
\frac{\sum_{i\in Q} f(V_i)\,V_i}{\sum_{i\in Q} f(V_i)}
$$
直觉：经常被激活的专家更“主干”，它的表示更值得保留；低频专家仍以较小权重注入，保留长尾能力。

------

### 4.4 专家重构：回到权重空间得到合并专家

最后用共享 $U,\Sigma$ 与合并后的 $V_{\text{merged}}$ 重构合并专家：
$$
W_{\text{merged}} = U\Sigma V_{\text{merged}}^\top
$$

------

### 4.5 这套流程的“优化意义”：从“冲突的加法”到“共享子空间上的拟合”

论文给了一个更直接的“输出保持”解释（式 (9)）：把合并看作在共享子空间里逼近原专家输出（带路由权重）：
$$
\min_{U,\Sigma,V}\ \sum_{i\in Q}\left\|G_i(x)\,W^{(i)}x - U\Sigma V^\top x\right\|_2^2
$$
其中关键约束就是：同一组专家共享 $U$（对齐表示空间），而专家差异主要体现在 $V$ 上；最终合并就是把多份 $V$ 再融合成一份。

------

## 5）Sub-MoE†：在“专家数减少”之上，再做“专家内低秩压缩”（activation-aware trunc SVD）

Sub-MoE 主体只减少专家数量，不改变单个专家内部大小。为了进一步压缩，论文提出 Sub-MoE†：在联合分解前先做 MoE 场景下的 activation-aware re-weight（类似 whitening/重要性加权），再做截断 SVD。

对第 $i$ 个专家，先根据输入激活相关性构造一个加权矩阵 $S_i$，然后重加权：
$$
W_i' = W_i S_i
$$
对同组专家拼接后的 $W'_{\cup}$ 再做联合 SVD：
$$
\mathrm{SVD}\!\left(
\begin{bmatrix}
W^{\prime(1)}\\ \vdots\\ W^{\prime(n)}
\end{bmatrix}
\right)=U'\Sigma' V'^\top
$$
合并 $V$ 时要“去白化/反变换”，论文写成（式 (11)）：
$$
V_{\text{merged}}
=
\frac{\sum_{i\in Q} f(V_i)\, V^{\prime(i)} S_i^{-1}}{\sum_{i\in Q} f(V_i)}
$$
然后对 $\Sigma'$ 做截断（丢掉小奇异值）控制压缩率：
$$
W_{\text{merged}}^{\text{trunc}}
=
U'\cdot \mathrm{Trunc}(\Sigma')\cdot V_{\text{merged}}
$$
这里的关键点是：引入 $S_i$ 后，奇异值大小与“压缩损失”更可控，且在高压缩率下更稳。

------

## 6）Step 实现过程：从校准数据到部署权重（可直接对照工程实现）

下面按可落地的执行顺序写清楚：

### Step 0：准备校准数据

- 从目标域采样一小批 token 序列（论文实验常用 128 条、长度 2048 的 WikiText-2 抽样），得到 token 集合 $X$。
- 用原模型跑一遍，记录：
  - 每层每个专家对每个 token 的输出 $E_i(x)$（用于相似度）
  - 路由 top-k 的选择结果（用于频率 $f(\cdot)$）

### Step 1：Adaptive Expert Clustering（每个 MoE 层）

1. 计算专家两两相似度 $\mathrm{Sim}(E_i,E_j)$

2. 用 K-means++ 初始化聚类中心

3. 迭代“分配—更新—收敛”，最小化
   $$
   J=\sum_{t=1}^{k}\sum_{E_j\in Q_t}\|Y_j-C_t\|_2^2
   $$

4. 若做 multi-layer allocation：在全模型目标压缩率约束下，为每层自适应确定聚类数（相似度高的层聚得更少、保留更少组；相似度低的层聚得更多组以减少信息损失）。

输出：每层若干组专家集合 $\{Q_t\}$。

### Step 2：Subspace Expert Merging（对每组 $Q$）

1. 收集该组每个专家的权重矩阵 $W^{(i)}$

2. 拼接 $W_{\cup}=[W^{(1)};\dots;W^{(n)}]$

3. 联合 SVD：$W_{\cup}=U\Sigma V^\top$，并切分得到每个专家对应的 $V^{(i)}$

4. 计算每个专家的路由激活频率 $f(V_i)$

5. 频率加权合并：
   $$
   V_{\text{merged}}
   =
   \frac{\sum_{i\in Q} f(V_i)\,V_i}{\sum_{i\in Q} f(V_i)}
   $$

6. 重构合并专家：
   $$
   W_{\text{merged}} = U\Sigma V_{\text{merged}}^\top
   $$

7. 用该合并专家替换原组内多个专家，同时更新路由器的专家索引映射（让被合并的路由指向新专家）。

### Step 3：可选的 Sub-MoE†（组内再做低秩截断）

对每组在 Step 2 前加入：

1. 估计激活相关性并构造 $S_i$，做 $W_i'=W_iS_i$
2. 对拼接后的 $W'_{\cup}$ 做联合 SVD 得 $U',\Sigma',V'$
3. 合并 $V$ 时反变换 $S_i^{-1}$
4. 对 $\Sigma'$ 截断得到目标秩/压缩率，再重构最终权重

------

## 7）与 SVD-LLM v2 / SAES-SVD 的主要理论差异（写新论文时的“定位句”）

1. **对象不同：专家合并 vs 单模型权重低秩**

- SVD-LLM v2 / SAES-SVD：对 dense LLM 的某层权重矩阵做低秩近似，目标是单模型加速与压缩。
- Sub-MoE：对 MoE 的“多专家集合”做结构化压缩，核心是减少专家数，同时尽量不丢知识。

1. **难点不同：误差传播 vs 参数冲突（基底不一致）**

- SAES-SVD 的核心矛盾是 SVD 误差的“局部 + 累计”传播，需要抑制误差积累。
- Sub-MoE 的核心矛盾是专家专门化造成“参数空间不对齐”，直接加权平均会冲突；它用 union SVD 强制共享子空间来对齐。

1. **SVD 用法不同：对同一矩阵分解 vs 对“专家集合拼接矩阵”分解**

- SVD-LLM：对单个 $W$ 做 SVD，截断奇异值。
- Sub-MoE：对 $[W^{(1)};\dots;W^{(n)}]$ 做联合 SVD，得到共享 $U$，再在 $V$ 上融合。

1. **权重分配信号不同：路由频率 vs 激活/损失敏感度**

- Sub-MoE 用 $f(\cdot)$（路由 top-k 频率）决定合并权重，更偏系统统计信号。
- SVD-LLM v2 / SAES-SVD 更偏“任务误差/激活相关性/误差模型”来决定截断与补偿。

------

## 8）一些可延展的 idea（只给方向，不展开成详细融合方案）

1. **把“频率加权”升级为“任务敏感度加权”**
    目前 $V$ 的融合权重是路由频率 $f_i$。可以引入更“任务对齐”的权重：

$$
\omega_i \propto f_i \cdot s_i
$$

其中 $s_i$ 可来自专家输出对 loss 的敏感度（例如梯度范数、或近似 Fisher），让合并更偏向“既常用又关键”的专家。

1. **把“共享子空间”从硬共享 $U$ 改成软约束（更像“子空间对齐正则”）**
    现在 union SVD 相当于强制同组专家共享一个基底。可以考虑软化成：

$$
\min_{\{U_i,V_i\}}\ \sum_i \|W^{(i)}-U_iV_i^\top\|_F^2
\ +\ \lambda\sum_i d(U_i, U_{\text{ref}})
$$

其中 $d(\cdot)$ 是子空间距离（Grassmann 距离），允许“部分共享、部分独立”，可能对相似度中等的专家更稳。

1. **multi-layer clustering 可引入“跨层一致性约束”**
    论文做了多层联合分配，但聚类仍以输出相似度为主。可以加一个约束：同一路由模式/同一语义簇在相邻层的聚类尽量一致，避免层间合并方案割裂。
2. **Sub-MoE† 的 $S_i$ 可以用更接近 SVD-LLM v2 的 truncation-aware 设计**
    论文的 $S_i$ 主要来自激活相关性/白化思路。你可以让 $S_i$ 与“截断后误差上界/传播风险”挂钩，把“专家内截断”做得更像 truncation-aware，而不是纯统计白化。
3. **把 SAES-SVD 的“累计误差抑制”借到 Sub-MoE† 的截断阶段**
    Sub-MoE† 截断 $\Sigma'$ 时本质会引入残差。可以在 MoE 堆叠多层后出现累积效应时，引入“逐层残差抑制/补偿”的机制（比如对后续层输入做小幅校正），把极端压缩下的稳定性拉回来。