# MoE-SVD：Structured Mixture-of-Experts LLMs Compression via Singular Value Decomposition

它的核心是：**把 MoE 的“专家冗余 + 分解敏感层”问题显式结构化**，用一套“选择性分解 + 低秩共享/裁剪”的框架，做到 **无需额外训练（training-free）** 的高压缩与可见的推理加速。Li 等 - Structured mixture-of-ex…

------

### 0）背景：MoE 压缩为什么“直接套 SVD-LLM/ASVD”会崩

MoE-FFN 的输出（单层 MoE）可写为：
$$
y=\sum_{i=1}^{N} G(x)_i\cdot E_i(x),\qquad G'(x)=\mathrm{TopK}(G(x),k)
$$
其中只有 top-k 专家被激活，导致两个关键现象：

1. **分解敏感性（layer sensitivity）高度不均匀**：某些 MoE 层（尤其开头和结尾）对 SVD 截断非常敏感，均匀压缩会出现 perplexity “爆炸式”崩溃。
2. **统计量失配**：在 dense LLM 上好用的 outlier 指标（如 OWL）在 MoE 上与“可分解性”相关性变差（因为路由 + 多专家导致激活统计混杂）。
3. **专家冗余（redundancy）更强**：不同专家虽然参数独立，但分解后发现右奇异子空间（V 侧）高度相似，存在可共享的结构。

MoE-SVD 就是围绕这三点做了“定制化的 SVD 路线”。

------

## 1）从基础优化目标开始：SVD 压缩本质是在解什么问题

对任意专家的线性层权重 $W_i\in\mathbb{R}^{m\times n}$，最基础的低秩近似目标是：
$$
\min_{\mathrm{rank}(\widetilde W_i)\le r}\ \|W_i-\widetilde W_i\|_F^2
$$
Eckart–Young 定理给出最优解是截断 SVD：
$$
W_i = U_i\Sigma_i V_i^\top,\qquad \widetilde W_i = U_{i,r}\Sigma_{i,r}V_{i,r}^\top
$$
但在 LLM（尤其有 outlier 激活）中，**“权重误差”不是最关键，关键是“对激活的输出误差”**。因此很多方法会把目标改成 activation-aware / weighted 形式。

------

## 2）改进 1：Activation-weighted SVD（白化/加权思想）——把目标从“权重误差”改成“输出误差近似”

令该层输入激活样本矩阵为 $X$（把 token/batch 展开），输出误差近似是：
$$
\| (W_i-\widetilde W_i)X\|_F^2
= \|(W_i-\widetilde W_i)\,C^{1/2}\|_F^2,\quad C=XX^\top
$$
因此一个等价的加权低秩目标可以写成：
$$
\min_{\mathrm{rank}(\widetilde W_i)\le r}\ \|(W_i-\widetilde W_i)\,S\|_F^2
$$
其中 $S\approx C^{1/2}$（MoE-SVD 用 Cholesky 形式得到 $S$）。

论文给的实现形式是先构造“激活加权权重”：
$$
W_{aw}=W_{\text{original}}\cdot S
$$
然后对 $W_{aw}$ 做截断 SVD，并“解白化”回去：
$$
W_{aw}=U\cdot\mathrm{Trunc}(\Sigma)\cdot V^\top\cdot S^{-1}
$$
直觉：这相当于在“激活主方向”上更精确地逼近，从而缓解 outlier 引起的重构误差放大（和 SVD-LLM/ASVD 的 activation-aware 思路同源，但这里用于 MoE 专家层）。Li 等 - Structured mixture-of-ex…

------

## 3）改进 2：选择性分解（Selective Decomposition）——把“哪些层能分解”变成可计算的近似决策

### 3.1 把“选层”写成一个（隐式）组合优化问题

理想上，你想在满足压缩预算的情况下，选择一部分 MoE 层去分解，使得性能损失最小：
$$
\min_{\mathbf z\in\{0,1\}^L}\ \Delta\mathcal{P}(\mathbf z)
\quad \text{s.t.}\quad \mathrm{Comp}(\mathbf z)\ge \rho
$$
其中 $z_\ell=1$ 表示第 $\ell$ 层分解，$\rho$ 是目标压缩率，$\Delta\mathcal{P}$ 是性能退化（不可直接算）。

MoE-SVD 用一个**层敏感度指标**近似 $\Delta\mathcal{P}$，并用阈值/排序策略实现选层。

### 3.2 敏感度指标 $S_L$：把“路由频率 + 权重谱结构 + 激活 outlier”融合

对某一 MoE 层（含 $N$ 个专家），定义层敏感度：
$$
S_L=\sum_{i=1}^{N} f_i\cdot r_i\cdot a_i
$$

- 路由采样频率（专家重要性）：

$$
f_i=\frac{\sum_{x\in X}\mathbb{I}[i\in \mathrm{TopK}(G(x),k)]}{|X|}
$$

- 主秩（权重谱复杂度）：$r_i$ 是专家权重 $W_i$ 的奇异值中“显著大值”的数量（阈值计数）。
- 激活 outlier 比例：

$$
a_i=\frac{\sum_{a\in A_i}\mathbb{I}(|a|>\tau\cdot \mathrm{Mean}(|A_i|))}{|A_i|}
$$

用法：给定目标压缩率，设置阈值 $\tau_S$。若某层 $S_L\ge \tau_S$，认为敏感、**不分解**；否则进入分解列表。它解决了 MoE 的“层间敏感度非均匀”导致的崩溃问题。Li 等 - Structured mixture-of-ex…

------

## 4）改进 3：低秩矩阵共享与裁剪——把“专家冗余”变成结构化约束

MoE-SVD 的关键观察：分解后 **V 侧（右奇异子空间）在专家间高度相似**，可共享；而保持专家差异更多依赖 **UΣ 侧**。

### 4.1 V-matrix Sharing：共享一个 $V_s$

对每个专家（或其分解得到的 $V_i$），定义其对应的路由频率 $f(V_i)$（本质与该专家被选中频率一致）。选择最常被路由到的那个作为共享基：
$$
V_s=\arg\max_{V_i} f(V_i)
$$
共享后专家近似：
$$
E_i \approx U_i\Sigma_i V_s^\top
$$
从“目标函数”角度，你可以把它理解为在加入结构约束：
$$
\min_{\{U_i,\Sigma_i\},V_s}\ \sum_i \|W_i-U_i\Sigma_i V_s^\top\|_F^2
$$
但论文的求解是启发式的：直接选频率最大的 $V$ 作为公共右子空间（更像“以路由分布加权的经验风险最小化”）。Li 等 - Structured mixture-of-ex…

### 4.2 U-matrix Trimming：只保留 top-k 个 $U$（按频率）并做组合

进一步，他们认为很多低频专家可以不再保留独立的 $U_i$，而是用更高频专家的 $U$ 来拼：

对每个专家 $i$，从比它更常用的专家集合里选 top-k 个 $U$：
$$
\{U_{i,1},U_{i,2}\}=\mathrm{TopK}(\{U_j\mid f(V_j)>f(V_i)\},k)
$$
最终专家函数写成（示例 k=2）：
$$
E_i(x)=(U_{i,1}\Sigma_{i,1}+U_{i,2}\Sigma_{i,2})V_s^\top x
$$
这一步可以看作一种“结构化字典表示”：每个专家的输入变换来自一个小字典（top-k 高频 $U$），而差异主要体现在系数（$\Sigma_{i,1},\Sigma_{i,2}$）与选择组合上。Li 等 - Structured mixture-of-ex…

### 4.3 参数量为什么能大幅下降（核心算式）

原始 MoE 一层（N 个专家）参数约为 $Nmn$。
 低秩分解后每个专家约 $m r + r n$，总 $N(mr+rn)$。
 共享 $V$ 后只保留 1 个 $V_s\in\mathbb{R}^{r\times n}$，V 侧从 $Nrn\to rn$。
 再裁剪 U：典型只保留很少的 $U$（例如 2 个），则 U 侧从 $Nmr$ 降到常数倍 $mr$。论文给的近似总量：
$$
\text{Params}\approx 2mr+rn,\qquad
\text{ratio}\approx \frac{2mr+rn}{Nmn}
$$
当 $r\ll \min(m,n)$ 且 $N$ 不小（Mixtral 8 experts、Phi 16 experts、DeepSeek 更大），压缩非常可观。Li 等 - Structured mixture-of-ex…

------

## 5）整体 Step 实现过程（按工程流水线整理）

下面把 MoE-SVD 的 pipeline 写成可复现实操步骤（对应图示与伪代码描述）：

1. 校准数据前向（Calibration）

- 通过 forward hooks 收集每层每个专家的：
  - 路由 top-k 命中次数（得到 $f_i$）
  - 专家激活统计（得到 outlier 比例 $a_i$，以及用于构造 $S$ 的激活矩阵/Gram）
  - 专家权重谱信息（对 $W_i$ 做 SVD 或近似得到 $r_i$）

1. 计算每个 MoE 层敏感度 $S_L=\sum_i f_i r_i a_i$
2. 选择要分解的层集合

- 给定目标压缩率/预算，设置阈值 $\tau_S$ 或按 $S_L$ 排序选择
- 敏感层（高 $S_L$）保留原权重不动，低敏感层进入分解

1. 对选中的层，逐专家做 activation-weighted SVD

- 计算 $S$（由激活 Gram 的 Cholesky 得到）
- 构造 $W_{aw}=W S$，对 $W_{aw}$ 做 SVD 并截断
- 解白化回去，得到低秩因子（或等价重构权重）

1. V-matrix sharing

- 统计每个专家（或其 $V_i$）的路由频率
- 选 $V_s=\arg\max f(V_i)$，所有专家共享 $V_s$

1. U-matrix trimming（top-k 选择）

- 只保留少量高频 $U$ 基向量/矩阵
- 每个专家用 top-k 高频 $U$ 的线性组合近似自身（通过对应 $\Sigma$ 做“差异化”）

1. 替换模型权重并导出（training-free 完成）

- 结构上形成“天然稀疏 + 低秩共享”的 MoE 专家层，实现显著降参和推理加速。Li 等 - Structured mixture-of-ex…

------

## 6）这篇方法的“理论本质”总结（你后续写新论文时可用的抽象）

MoE-SVD 可以抽象成一个**带结构约束的多矩阵联合低秩问题**：

- 单专家：activation-weighted 低秩逼近
  $$
  \min_{\mathrm{rank}(\widetilde W_i)\le r}\ \|(W_i-\widetilde W_i)S_i\|_F^2
  $$

- 多专家联合：共享右子空间（共享 $V$）+ 左侧字典裁剪（U 侧字典表示）
  $$
  \min_{\{U_i,\Sigma_i\},V_s}\ \sum_i \|(W_i-U_i\Sigma_i V_s^\top)S_i\|_F^2
  \quad \text{s.t.}\quad U_i \in \mathrm{span}(\mathcal{U}_{\text{top-k}})
  $$

- 选层策略：用 $S_L$ 近似层重要性，实现组合优化的启发式解。

------

## 7）一些可以继续往前推的 idea（不展开成“怎么和你那两篇融合”的细节，只给方向和公式骨架）

1. 把“共享 $V$”从启发式频率最大，升级为真正的联合最优（可解释为加权平均目标）

$$
\min_{V_s,\{U_i,\Sigma_i\}}\ \sum_i \omega_i\|W_i-U_i\Sigma_i V_s^\top\|_F^2,
\quad \omega_i\propto f_i
$$

这会让 “频率最大选 $V$” 变成该目标的近似（当前是 heuristic，你可以给出一个更严谨的推导与近似求解）。

1. 让共享的不是一个 $V_s$，而是一个小字典 $\{V^{(t)}\}_{t=1}^{T}$（MoE 中可能存在多个“族群子空间”）

$$
\min_{\{V^{(t)}\},\{U_i,\Sigma_i,\alpha_{i,t}\}}
\sum_i \left\|W_i-U_i\Sigma_i\left(\sum_{t=1}^{T}\alpha_{i,t}V^{(t)}\right)^\top\right\|_F^2
$$

并约束 $\alpha_{i,\cdot}$ 稀疏（每个专家只选少数 $V$），更贴合“专家分簇”。

1. 把 U-trimming 的“top-k 频率”升级为“误差—预算最优化”选择
    给每个候选 U 字典项 $u$ 定义带激活权重的重构增益 $\Delta(u)$，做类似背包/子模选择：

$$
\max_{\mathcal{U}:|\mathcal{U}|\le K}\ \sum_i f_i\cdot \mathrm{Gain}(W_i;\mathcal{U})
$$

让“为什么选 top-k 频率”变成“为什么它近似最优”的理论桥梁。

1. 选择性分解指标 $S_L$ 可以从乘积 $f_i r_i a_i$ 升级为“可分解性上界/漂移界”
    比如把“截断导致输出漂移”显式写成界（类似某些谱界思路）：

$$
\|(W_i-\widetilde W_i)X\|_F
\le \sigma_{r+1}(W_i S_i)\cdot \|S_i^{-1}X\|_F
$$

用上界替代经验 outlier 比例，使指标更“理论化”。