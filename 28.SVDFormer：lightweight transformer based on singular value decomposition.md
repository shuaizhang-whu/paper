# SVDFormer: lightweight transformer based on singular value decomposition

## 1）论文核心：把 SVD 截断用在“特征矩阵/Token 矩阵”而不是权重矩阵，并用 SVD 的 $U,V$ 代替注意力里的 $Q,K$

这篇与 SVD-LLM/SAES-SVD 最大不同是：它不是做 LLM 权重压缩，而是 **在视觉 Transformer 的特征流上做低秩降维 + 稀疏化 attention**。它的两块贡献：

1. **Trunc-SVD on feature matrix**：对 CNN/patch embedding 后的特征矩阵 $X$ 做截断 SVD，得到低秩近似 $X_{\text{trunc}}=U_k\Sigma_k V_k^\top$，用于“去噪 + 降维 + 降参 + 防过拟合”。
2. **SVD-Attention**：对输入特征矩阵 $M$ 直接做 SVD，取 $Q=U_k$，$K=V_k^\top$（论文写成 $K=V_k^T$），替代标准 self-attention 里的线性投影 $Q=XW_Q, K=XW_K$，从而把注意力复杂度从 $O(N^2D)$ 改到 $O(N^2k)$（$k\ll D$）。

------

## 2）原始优化目标：标准截断 SVD 与其等价形式

### 2.1 经典低秩近似目标（Frobenius 最优）

给定矩阵 $X\in\mathbb{R}^{m\times n}$，截断 SVD 的经典优化写法（论文 Eq.3）：
$$
X_k=\arg\min_{Y\in\mathbb{R}^{m\times n},\ \mathrm{rank}(Y)=k}\|X-Y\|_F^2
$$
其最优解为（Eckart–Young）：
$$
X_k = U_k\Sigma_kV_k^\top
$$

### 2.2 “保留最大信息”的等价视角：能量/迹约束

论文同时把目标解释为“尽量保留主奇异值能量”（Eq.4），并给出一个约束形式（Eq.5）：
$$
X_k=\arg\min_{\mathrm{rank}(Y)=k}\|X-Y\|_F^2\quad
\text{s.t. }\sum_{i=1}^k \sigma_i(Y)\ge C
$$
这里 $C$ 是希望保留的“能量阈值”（论文设为归一化能量的 0.95）。

更标准、也更常用的能量保留写法通常是“奇异值平方占比”（论文后面 Eq.16 用了平方）：
$$
\frac{\sum_{i=1}^k\sigma_i^2(X)}{\sum_{i=1}^{\min(m,n)}\sigma_i^2(X)}\ge \tau
$$
其中 $\tau=0.95$。

> 直觉：$\sum\sigma_i^2=\|X\|_F^2$，所以这是在控制“保留了多少 Frobenius 能量”。

------

## 3）改进后的目标：把“约束问题”变成一个可调权衡的无约束目标（论文 Eq.17）

论文在确定 $k$（用能量阈值）之后，又提出一个“无约束优化”形式（Eq.17），把“丢弃的奇异值能量”当作惩罚项：
$$
X_k=\arg\min_{\mathrm{rank}(Y)=k}
\Big(\|X-Y\|_F^2 + \lambda\sum_{i=k+1}^{\min(m,n)}\sigma_i^2\Big)
$$
这里第二项 $\sum_{i>k}\sigma_i^2$ 是“高阶奇异值能量”（被截断部分），作为 penalty 强调“要尽量压低尾部能量的影响”。

如果把它和 Eckart–Young 的性质对齐，你会发现：

- 对给定 $k$，最优近似误差满足
  $$
  \|X-X_k\|_F^2=\sum_{i=k+1}\sigma_i^2(X)
  $$

- 因此 Eq.17 某种意义上是在把“近似误差”与“尾部能量”绑定成一个统一的 trade-off（不过它写在一起时会有点“重复惩罚”的味道，更像一种启发式稳定项，而不是严格推导的必要项）。

------

## 4）SVD-Attention：从标准注意力到“用 $U,V$ 代替 $Q,K$”的公式与复杂度推导

### 4.1 标准 self-attention（单头写法）

输入 token 矩阵 $X\in\mathbb{R}^{N\times D}$：
$$
Q=XW_Q,\quad K=XW_K,\quad V=XW_V
$$
注意力输出：
$$
\mathrm{Attn}(X)=\mathrm{Softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$
核心复杂度来自 $QK^\top$：$O(N^2D)$。

### 4.2 SVD-Attention 的替代：对特征矩阵做 SVD

论文对一个矩阵 $M\in\mathbb{R}^{m\times n}$ 做 SVD：
$$
M=U\Sigma V^\top
$$
截断后取
$$
U_k=U[:,1\!:\!k],\quad \Sigma_k=\Sigma[1\!:\!k,1\!:\!k],\quad V_k^\top=V^\top[1\!:\!k,:]
$$
然后**直接令**
$$
Q=U_k,\quad K=V_k^\top
$$
并用它们计算 attention score（论文 Algorithm 1 给出）：
$$
S=\mathrm{Softmax}\left(\frac{QK}{\sqrt{d}}\right)
$$
最后输出通过元素乘得到（论文写成 $R=S\cdot M$，更像“用注意力掩码重加权原矩阵”，而不是标准的 $SV$ 乘法）：
$$
R=S\odot M
$$

> 从注意力语义上说，它更接近“用低秩结构得到的相似度矩阵去重标定特征”，而不是严格遵循 transformer 的 $\mathrm{Softmax}(QK^\top)V$ 形式。这也是它更像一个“轻量特征增强模块”而非通用注意力算子的原因。

### 4.3 复杂度推导：从 $D$ 降到 $k$

论文给出复杂度对比：

- 原 attention 的 $QK^\top$ 与 $SV$ 主项：约 $O(N^2D)$
- 用截断后：约 $O(N^2k)$

并给出一个“按截断率 $r=k/N$”的 FLOPs 简化（式子写得比较粗，但结论是：随着 $r$ 变小，节省近似按 $1-r^2$ 增长；例如 $r=0.28$ 时节省约 92% FLOPs）。

------

## 5）“最优截断 rank k”的选择：能量阈值（Eq.16）与静/动态截断对比

### 5.1 能量保留准则（论文 Eq.16）

它用奇异值平方的累计占比确定 $k$：
$$
k = \arg\min\left\{\frac{\sum_{i=1}^k\sigma_i^2}{\sum_{i=1}^{\min(W,H)}\sigma_i^2} > 0.95\right\}
$$
这其实就是标准的“保留 95% Frobenius 能量”的 rank 选择策略。

### 5.2 静态 vs 动态截断

论文实验里比较了：

- 静态截断：固定 $k$（比如按 token_num 的 14%），效果更好更快
- 动态截断：训练中根据奇异值分布和学习率动态调整 $k$，反而更差更慢
   论文给出的解释是：训练早期奇异值谱不稳定，动态 $k$ 会抖动导致偏差。

从优化角度看：动态截断相当于在训练过程中引入了一个非平稳的、会改变模型容量的控制变量，若没有额外的稳定化（例如对 $k$ 加动量/滞后/惩罚），确实很容易抖。

------

## 6）Step 实现过程：按论文框架拆成可落地模块

我按“一个 SVDTa Block”的实现逻辑写（你如果要复刻结构会很直接）：

### Step A：输入与 token 化

1. 输入图像 $I$，经过浅层编码器/卷积与 layernorm，得到特征 $X\in\mathbb{R}^{B\times H\times W\times C}$
2. reshape 成二维矩阵（token 视角）：

$$
X_{2d}\in\mathbb{R}^{(H\cdot W)\times C}
$$

### Step B：Trunc-SVD（特征低秩化）

1. 对 $X_{2d}$ 做 SVD：

$$
X_{2d}=U\Sigma V^\top
$$

1. 用能量阈值选 $k$（或固定截断率），得到 $U_k,\Sigma_k,V_k^\top$
2. 重构低秩特征：

$$
X_{\text{trunc}}=U_k\Sigma_kV_k^\top
$$

### Step C：SVD-Attention（用 SVD 得到的子空间构造注意力掩码）

1. 对输入矩阵 $M$（可以是 $X_{\text{trunc}}$ 或其映射）做 SVD 截断得到 $U_k,V_k^\top$
2. 构造

$$
Q=U_k,\quad K=V_k^\top
$$

1. 计算

$$
S=\mathrm{Softmax}\left(\frac{QK}{\sqrt{d}}\right)
$$

1. 输出（按论文算法）：

$$
R=S\odot M
$$

### Step D：残差 + LN + MLP

1. 残差连接：

$$
X_{\text{res}}=R+X_{\text{trunc}}
$$

1. LayerNorm
2. 两层 MLP（扩展 4C 再回到 C）：

$$
Y = D((X_{\text{res}}W_1+b_1)W_2+b_2)
$$

其中 $D$ 表示 dropout，激活用 GELU。

------

## 7）一些可延展的 idea（不展开到详细融合建议）

### idea 1：把“能量阈值选 k”换成“任务敏感度加权能量”

目前 $k$ 只看 $\sum\sigma_i^2$（Frobenius 能量）。但在 transformer 里，不同奇异方向对分类/检测的贡献不均衡。可以考虑用类别梯度或注意力分布敏感度对奇异值加权：
$$
\text{keep }k:\ \frac{\sum_{i=1}^k w_i\sigma_i^2}{\sum_i w_i\sigma_i^2}\ge\tau
$$
其中 $w_i$ 可来自对 $U_i,V_i$ 方向的任务梯度投影能量。

### idea 2：SVD-Attention 的形式可改成更“正宗”的 $\mathrm{Softmax}(QK^\top)V$

论文算法用的是 $R=S\odot M$。你可以尝试构造一个更接近 transformer 的版本，比如让
$$
V = XW_V \quad\text{或}\quad V=U_k\Sigma_k
$$
再输出
$$
\hat X = \mathrm{Softmax}\left(\frac{U_kV_k^\top}{\sqrt{k}}\right)V
$$
这样更容易和 LLM 里的 attention 结构做类比。

### idea 3：动态截断失败的根源是“k 的高频抖动”，可用滞后/正则稳定

让 $k_t$ 随训练步变化时加一个平滑项：
$$
\min_k\ L + \eta\|k_t-k_{t-1}\|_1
$$
或对能量占比的阈值 $\tau$ 做温启动（从小到大）。

### idea 4：把“特征矩阵低秩化”改成“分块/局部窗口低秩化”

视觉特征天然有局部结构。对全局 $X_{2d}$ 做 SVD 会过于全局，也贵。可以考虑：

- 按 window 做局部 SVD（类似 Swin window）
- 或对 channel 维/空间维分别做低秩（近似 Tucker）

### idea 5：与 LLM/SVD-LLM 系的桥：把“对激活做 SVD 截断”当成一种“中间表征压缩”

这篇的思路非常像 Dobi-SVD 的“对激活截断更正宗”，只是它在视觉侧直接用截断激活做后续计算；你可以把它抽象成：在某些层对 $X$ 做低秩投影
$$
X\leftarrow X P_k,\quad P_k=V_kV_k^\top
$$
再研究这种投影对后续层误差传播的影响，和 SAES-SVD 的累计误差抑制在同一框架里讨论。