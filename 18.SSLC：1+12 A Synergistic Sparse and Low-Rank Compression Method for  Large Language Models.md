# 1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large Language Models”（SSLC）

它的核心贡献是：把 **剪枝（稀疏）** 和 **低秩（SVD）** 从“先做一个、再做另一个”的串联，提升为一个**统一的、数据感知（activation-aware）的联合优化问题**，并用一个简单可实现的**交替迭代算法**在不额外训练（training-free）的前提下实现更好的压缩-性能折中。Zong 等 - 1+12 A synergistic spa…

------

## 1）从基础优化目标开始：LLM 的后训练压缩为何常写成“层级重构”

SSLC 采用主流 pruning/quant/SVD-LLM 系列共同的 layer-wise reconstruction 框架：对某层权重矩阵 $W\in\mathbb{R}^{m\times n}$，校准集输入激活矩阵 $X\in\mathbb{R}^{n\times (N\!\times\!L)}$，目标是让压缩后权重 $W'$ 的输出尽量接近原输出：
$$
\min_{W'} \ \|(W - W')X\|_F
\tag{1}
$$
这一定义把“模型性能保持”近似成“该层输出重构误差最小”。

------

## 2）关键洞察：剪枝与低秩分别擅长保留什么，为什么要“联合分解”

论文给了一个很重要的现象解释（也对应图 1 的 salience 可视化）：

- **剪枝**擅长从“离散角度”保留少数高重要度（高 salience）的权重，但很难系统性提取“整体一致的结构成分”。
- **低秩**擅长提取“连贯/可共享的结构基”（orthogonal bases / subspace），但会丢失那些**非连贯但关键的全秩细节**，在高压缩比时崩得很厉害。

因此 SSLC 的目标是把权重拆成两部分：

- 一个 **低秩部分 $L$**：负责提取“连贯结构”（coherent part）
- 一个 **稀疏部分 $S$**：负责保留“非连贯但重要的细节”（incoherent but crucial part）

------

## 3）从“单一压缩”到“联合压缩”：优化目标如何改写（公式链路）

### 3.1 直接给出联合目标（核心）

SSLC 将一层的权重分解为：
$$
W \approx L + S
$$
并把重构误差写成同时由 $L,S$ 共同承担：
$$
\min_{L,S}\ \|(W - L - S)X\|_F
\quad
\text{s.t.}\ \operatorname{rank}(L)=r,\ \operatorname{sparsity}(S)=k\%
\tag{2}
$$
这就是它的“统一问题”：**在输出重构意义下联合做低秩与稀疏**。

### 3.2 为什么它是 NP-hard，怎么变成可解的迭代

同时满足 rank 约束 + 稀疏约束属于典型组合优化（类似 RPCA / GoDec 的结构），直接求解困难。SSLC 用交替优化，把它拆成两个子问题反复迭代：
$$
\begin{cases}
S_t = \arg\min_{\operatorname{sparsity}(S)=k\%}\ \|(W - L_t - S)X\|_F \\
L_t = \arg\min_{\operatorname{rank}(L)=r}\ \|(W - L - S_{t-1})X\|_F
\end{cases}
\tag{3}
$$
直觉：

- 固定 $L_t$，对残差 $R^L_t = W-L_t$ 做“最好的稀疏保留”；
- 固定 $S_{t-1}$，对残差 $R^S_t = W-S_{t-1}$ 做“最好的低秩逼近”。

------

## 4）子问题 A：稀疏化如何做（用 OBS/OBD 风格的 salience）

### 4.1 从二阶近似到 salience 形式

在 OBS/OBD / SparseGPT / GPTQ 框架里，改变单个权重 $w_{ij}$ 会造成一个近似二次误差项。SSLC 沿用这种思想，把 Hessian 近似为 $H\approx X^\top X$，并进一步把代价近似到与输入列范数相关。

它最终对“低秩残差” $R^L_t = W-L_t$ 定义逐元素 salience：
$$
\delta_{ij} \approx \left(|R^L_t| \cdot \|X_j\|_2\right)^2_{ij}
\tag{4}
$$
其中 $X_j$ 表示输入激活的第 $j$ 个通道对应的列向量统计。

### 4.2 按阈值保留 top-k% salience（输出维度逐行/逐输出剪枝）

把 $\delta$ 展开排序，取第 $k\%$ 分位阈值 $\theta$，生成稀疏矩阵 $S_t$：
$$
[S_t]_{ij}=
\begin{cases}
[R^L_t]_{ij} & \delta_{ij}\ge \theta \\
0 & \text{otherwise}
\end{cases}
\tag{5}
$$
注意这里剪的是 **“低秩之后的残差”**，这就是“协同”的关键：低秩先把连贯成分吸走，残差的 salience 分布会更“干净”，让剪枝更有效、误删更少。

------

## 5）子问题 B：低秩如何做（把 activation-aware 写成“缩放后做 SVD”）

### 5.1 原本的低秩子问题

固定 $S_{t-1}$ 后，定义残差 $R^S_t = W - S_{t-1}$，低秩子问题是：
$$
L_t=\arg\min_{\operatorname{rank}(L)=r}\ \|(R^S_t - L)X\|_F
$$
这看起来像“加权/白化 SVD”，但如果每次都对大矩阵做完整 SVD，迭代会极慢。

### 5.2 关键近似：把 $X$ 的作用折到一个缩放矩阵上

利用前面 salience 推导中出现的输入列范数 $\|X\|_2$（可以理解为对输入通道做缩放），论文将该子问题近似改写为对一个**缩放后的矩阵**做低秩逼近：
$$
L'_t=\arg\min_{L'} \ \|R^S_t\cdot \|X\|_2 - L'\|_F^2
\tag{6}
$$
然后再“缩放还原”得到真正的低秩部分：
$$
L_t = L'_t\cdot \|X\|_2^{-1}
$$
直觉：这是一个更便宜的 activation-aware 近似：把“$\cdot X$”的影响转化为“对权重列的 reweight/whitening”。

### 5.3 为了快：用 randomized SVD（双侧随机投影 BRP）

论文用一种双侧随机投影的随机 SVD（bilateral random projections, BRP）来避免 full SVD。令缩放矩阵：
$$
\widetilde L = R^S_t\cdot \|X\|_2
$$
采样随机矩阵 $A_1, A_2$，做双侧投影：
$$
Y_1 = \widetilde L A_1,\quad Y_2 = \widetilde L^\top A_2
$$
然后用一个小矩阵的逆来重构 rank-$r$ 的近似（论文给出一个闭式样子）：
$$
L'_t = Y_1 (A_2^\top Y_1)^{-1} Y_2^\top
\tag{7}
$$
最后再还原：
$$
L_t = L'_t \cdot \|X\|_2^{-1}
\tag{8}
$$

------

## 6）“1% 最重要权重永久保留”：把极端关键点从优化中剥离

SSLC 还有一个很实用的 trick：先按式 (4) 的 salience 选出 **top 1%** 权重永久保留，不参与后续迭代分解。理由是这些权重往往对应非常关键的知识/功能，迭代分解（特别是低秩）容易把它们“平均化”导致灾难性退化。

参数预算分配也因此变成：

- 总体要保留 $p\%$ 参数（压缩率为 $1-p\%$）
- 其中 1% 固定保留
- 剩余 $(p-1)\%$ 在：
  - 稀疏部分分配 $(k-1)\%$
  - 低秩部分分配 $r\cdot \frac{m+n}{mn}$

这让你在工程上能更稳定地调到目标压缩率。

------

## 7）整体算法 Step（你复现/改造时最关键的流程）

下面把论文的 SSLC pipeline 按“可实现步骤”写清楚（对应它的 Fig.2 与 Algorithm）：

### Step 0：校准数据与统计量

- 取一小份校准集（例如 128×2048 tokens）
- 对每个线性层收集输入激活 $X$
- 预计算每个输入通道的 $\|X\|_2$（列范数向量）

### Step 1：预保留 top 1% 重要权重

- 用一次 salience 估计（可用式 (4) 的近似形式）
- mask 掉 top 1% 权重，后续不参与迭代更新

### Step 2：初始化

- $S_0 = 0$

### Step 3：迭代 $t=1,\dots,T$

1. **SVD step（更新低秩）**
   - 计算 $R^S_t=W-S_{t-1}$
   - 构造 $\widetilde L=R^S_t\cdot \|X\|_2$
   - 用 randomized SVD/BRP 得到 $L'_t$（rank $r$）
   - 还原 $L_t=L'_t\cdot \|X\|_2^{-1}$
2. **Pruning step（更新稀疏）**
   - 计算 $R^L_t=W-L_t$
   - 计算 salience $\delta$（式 (4)）
   - 按阈值保留 top $(k-1)\%$ 得到 $S_t$（式 (5)）

### Step 4：输出

- 得到最终分解 $W\approx L_T + S_T$
- 如果要部署：存 $S_T$（稀疏）与 $U,V$（低秩因子），或把 $L_T$ 合并成两个低维矩阵乘法的形式

### Step 5（可选）：低秩恢复微调（不用新增 LoRA）

论文还强调一个优势：不像“剪枝后再插 LoRA”，SSLC 可以直接拿分解得到的低秩因子 $U_t,V_t$ 作为可训练参数进行恢复微调，而稀疏部分 $S_t$ 冻结不动。形式上：
$$
h = (U_t V_t^\top + S_t)X + b
$$
微调就是更新 $U,V$，不引入额外 LoRA 分支结构。

------

## 8）为什么它会“协同增益”：从误差分解角度给一个更直观的解释

把目标写成输出误差：
$$
\|(W-L-S)X\|_F
$$
如果你只做低秩，$W-L$ 的残差里既有“重要的非连贯点”，也有“噪声/可剪部分”，很难保；
 如果你只做剪枝，salience 空间里存在大量“连贯结构”但被离散地保留，等价于浪费预算；
 SSLC 的交替迭代相当于让：

- $L$ 吸收能量最大的连贯子空间（正交基最大化能量保留）
- $S$ 专门拾取 $L$ 无法表达的全秩关键残差（离散的高 salience 点）

这就是“1+1>2”。

------

## 9）一些可以继续往前推的 idea（不展开融合细节，只给可落公式/方向）

### Idea 1：把“交替两步”升级为带惩罚项的单目标（更像 ADMM / 近端交替）

现在是硬约束：$\operatorname{rank}(L)=r$、$\operatorname{sparsity}(S)=k\%$。可以改成软约束（更容易推导收敛与自适应分配）：
$$
\min_{L,S}\ \|(W-L-S)X\|_F^2 + \lambda\|S\|_0 + \mu\|L\|_\*
$$
其中 $\|L\|_\*$（核范数）是 rank 的凸替代。再用近端交替：

- $S$：硬阈值（top-k）或近似 $L_0$ 近端
- $L$：奇异值软阈值（SVT）或截断

这能把 SSLC 的“启发式 alternation”推成更标准的优化框架。

### Idea 2：把稀疏 salience 从 $|R|\cdot\|X\|_2$ 升级为“考虑低秩子空间投影后的残差”

现在 salience 用的是 $R^L_t=W-L_t$。你可以显式分解残差：
$$
R^L_t = R_{\parallel} + R_{\perp}
$$
其中 $R_{\parallel}$ 是在低秩子空间的投影，$R_{\perp}$ 是正交补。直觉上应更偏向保留 $R_{\perp}$ 的高 salience（因为 $R_{\parallel}$ 低秩还能再吸收）。可构造：
$$
\delta_{ij}\propto \left(|R_{\perp}|\cdot \|X_j\|_2\right)^2_{ij}
$$

### Idea 3：动态分配 $r$ 与 $k$（层级/模块级自适应）

论文目前是全层统一压缩率。可以让每层在固定总预算下自适应分配低秩与稀疏比例：
$$
\min_{\{r_\ell,k_\ell\}}\ \sum_\ell \|(W_\ell-L_\ell-S_\ell)X_\ell\|_F^2
\quad
\text{s.t.}\ \sum_\ell \text{Cost}(r_\ell,k_\ell)\le B
$$
其中 $\text{Cost}$ 近似为：
$$
\text{Cost}(r,k)\approx k\cdot mn + r(m+n)
$$
再用类似拉格朗日/贪心，或用你之前分析过的 effective rank / Fisher 指标驱动分配。

### Idea 4：把 randomized SVD 里的 rank 选择变成“按残差能量自适应增长”

现在 rank 固定 $r$。可以在迭代中监控：
$$
E_t=\|(W-L_t-S_t)X\|_F
$$
当 $E_t$ 降不动时，增加 $r$ 或增加稀疏保留 $k$，形成自适应 schedule（尤其适合不同层敏感度差异大的情况）。

### Idea 5：把“保留 top 1% 权重”做成更理论化的“异常值分量”处理

你可以把权重或 salience 的异常点看成稀疏噪声（robust decomposition 视角）：
$$
W = L + S_{\text{outlier}} + S_{\text{rest}}
$$
先用规则把 $S_{\text{outlier}}$ 抽出来（top 1%），再对剩余做 SSLC 迭代，会更容易解释“为什么 1% 有用”，并能把 1% 变成可调的阈值策略（比如按谱统计或分位数自适应）。