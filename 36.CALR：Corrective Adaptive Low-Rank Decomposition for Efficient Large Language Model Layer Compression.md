# CALR: Corrective Adaptive Low-Rank Decomposition for Efficient Large Language Model Layer Compression

这篇的核心不是“更聪明地截断 SVD”，而是承认 **单一路径的低秩近似表达能力有限**，于是用一个**并行的、可学习的低秩校正支路**去拟合“功能残差（functional residual）”，把压缩的目标从“权重重构好看”转成“模块输出对齐”。Kautsar 等 - 2025 - CALR Correct…

------

## 1）原始优化目标：SVD 只保证权重重构最优，但不保证功能最优

对任意线性层权重 $W\in\mathbb{R}^{d_{\text{out}}\times d_{\text{in}}}$，标准截断 SVD 做的是：
$$
\min_{\operatorname{rank}(W_r)\le r}\ \|W-W_r\|_F^2
\quad\Rightarrow\quad
W_r = U_{:,1:r}\Sigma_{1:r,1:r}V_{:,1:r}^\top
$$
这保证了 **Frobenius 意义的最优矩阵近似**，但 LLM 的真实目标是保持输出函数 $F(\cdot)$ 的行为。对于一个模块（比如 FFN 子块），更直接的目标应该是（对校准数据分布 $\mathcal{D}$）：
$$
\min \ \mathbb{E}_{X\sim\mathcal{D}} \big\|F_{\text{orig}}(X)-F_{\text{compressed}}(X)\big\|^2
$$
CALR 的出发点是：当 $r$ 压得很低时，单个 $W_r$ 很难兼顾“主能量成分”和“功能关键但分散的细节成分”，导致 downstream 性能明显掉。Kautsar 等 - 2025 - CALR Correct…

------

## 2）CALR 的改进目标：把“丢失的功能信息”显式建模为可学习信号（functional residual）

### 2.1 被压缩对象：Gated FFN（以门控 FFN 为例）

原始 gated FFN（输入 $X\in\mathbb{R}^{b\times d_{\text{model}}}$）：
$$
F_{\text{orig}}(X)=\big[(XW_g)\odot \operatorname{ACT}(XW_u)\big]W_d
$$
其中 $W_g,W_u,W_d$ 分别是 gate / up / down 投影矩阵，$\odot$ 是逐元素乘。Kautsar 等 - 2025 - CALR Correct…

### 2.2 主路径：对每个线性层做低秩分解（SVD 初始化）

对任意 $W_j$（$j\in\{g,u,d\}$）：
$$
W_j = U_j\Sigma_j V_j^\top
$$
CALR 用 rank-$r$ 的两因子表示 $W_j\approx A_jB_j$，并用“平方根拆分”做初始化（这样 $A_jB_j$ 就是标准截断 SVD 的重构）：
$$
A_j \leftarrow U_{j,:,1:r}\ \Sigma_{j,1:r,1:r}^{1/2},\qquad
B_j \leftarrow \Sigma_{j,1:r,1:r}^{1/2}\ V_{j,:,1:r}^\top
$$
于是压缩后的 FFN 主路径：
$$
F_{\text{SVD}}(X)=\big[(XA_gB_g)\odot \operatorname{ACT}(XA_uB_u)\big]A_dB_d
$$
注意：在 CALR 里 $A_j,B_j$ 是**可训练参数**（不是冻结）。Kautsar 等 - 2025 - CALR Correct…

### 2.3 校正路径：并行的低秩模块拟合“功能残差”

他们额外加一个低秩校正模块（类似 LoRA 的形式）：
$$
F_{\text{corr}}(X)=X(AB),\qquad
A\in\mathbb{R}^{d_{\text{model}}\times r_c},\ B\in\mathbb{R}^{r_c\times d_{\text{model}}}
$$
最终模块输出是两路相加：
$$
F_{\text{CALR}}(X)=F_{\text{SVD}}(X)+F_{\text{corr}}(X)
$$
定义**功能残差**：
$$
R_F(X)=F_{\text{orig}}(X)-F_{\text{SVD}}(X)
$$
校正模块的核心优化目标是直接逼近这个功能残差：
$$
\min_{A,B}\ \mathbb{E}_{X\sim\mathcal{D}}
\big\|F_{\text{orig}}(X) - (F_{\text{SVD}}(X)+XAB)\big\|_2^2
$$
这一步把“压缩损失”从 **权重空间 $\|W-AB\|_F$** 转到了 **输出空间 $\|F_{\text{orig}}-F_{\text{CALR}}\|$**，也就是论文反复强调的 functional fidelity。Kautsar 等 - 2025 - CALR Correct…

------

## 3）为什么校正支路“低秩就够”：残差谱快速衰减的经验依据

CALR 给了一个重要的经验论证：他们在某个“低 transformation”的层上，把校准集上计算得到的残差矩阵拿去做奇异值谱分析，发现奇异值下降很快，呈现强低秩结构（论文图 3）。因此假设：
$$
R_F(X) \ \text{在校准数据上可被低秩结构良好逼近}
$$
于是用 $XAB$ 这种低秩（且计算友好的）结构去学残差，是参数效率很高的选择。Kautsar 等 - 2025 - CALR Correct…

------

## 4）另一个关键点：不是“全层都压”，而是选择“功能更稳定（更安静）”的 FFN 去压

CALR 先做一件很工程但很关键的事：用一个“层变换度量”去判断哪些层更适合压缩。

对一个 block 或 FFN 子块，输入/输出分别为 $X_{\text{in}},X_{\text{out}}$，定义 transformation（余弦距离）：
$$
\mathrm{Transformation}(X_{\text{in}},X_{\text{out}})
=
1-\frac{\langle X_{\text{in}},X_{\text{out}}\rangle}{\|X_{\text{in}}\|_2\ \|X_{\text{out}}\|_2}
$$
他们观察到很多模型沿深度呈 “U 型”：前后层变换大、中间层变换小。于是 CALR 选择 **transformation 最低的 $N_{\text{target}}$ 个 FFN** 来做结构替换与训练，从而避免对“高度动态（敏感）”的层强行 SVD 造成灾难性破坏。Kautsar 等 - 2025 - CALR Correct…

------

## 5）改进后的整体优化目标：主路径因子 + 校正因子联合微调

当你把某些 FFN 替换为 CALR 结构后，最终训练目标就是标准语言模型损失（例如 next-token CE）：
$$
\min_{\Theta_{\text{CALR}}}\ \mathbb{E}_{(x,y)\sim\mathcal{D}_{\text{cal}}} 
\Big[-\log p_{\Theta_{\text{CALR}}}(y|x)\Big]
$$
其中可训练参数集合一般是：

- 主路径：所有被压缩线性层的 $\{A_j,B_j\}$
- 校正路径：每个被压缩 FFN 对应的 $\{A,B\}$

也就是说：CALR 不仅仅是“学校正”，而是 **主路径也允许偏离纯 SVD 重构**，两者协同适配任务目标。Kautsar 等 - 2025 - CALR Correct…

------

## 6）Step 实现过程（按论文 Algorithm 1 的工程流程重写）

### Phase 0：输入设定

给定：

- 预训练模型 $M_{\text{orig}}$
- 目标压缩 FFN 数 $N_{\text{target}}$
- 主路径秩 $r$
- 校正秩 $r_c$（默认 $r_c=r$）
- 校准集 $\mathcal{D}_{\text{cal}}$

### Phase 1：选择要压缩的 FFN（选择“安静层”）

1. 在 $\mathcal{D}_{\text{cal}}$ 上前向
2. 对每个 FFN（或 block）计算 transformation 分数
3. 选取分数最低的 $N_{\text{target}}$ 个 FFN 组成集合 $\mathcal{F}_{\text{target}}$

### Phase 2：结构替换（主路径 SVD 初始化 + 加校正支路）

对每个 $m\in\mathcal{F}_{\text{target}}$：

1. 对模块内每个线性层 $W_j$ 做 SVD，按

$$
A_j=U_{:,1:r}\Sigma_{1:r}^{1/2},\ B_j=\Sigma_{1:r}^{1/2}V_{:,1:r}^\top
$$

替换 $W_j\leftarrow A_jB_j$

2) 初始化校正模块 $A,B$（论文建议 $B$ 初始化为零，类似 LoRA 的“起步不扰动”思路），使得初始时：

$$
F_{\text{CALR}}(X)\approx F_{\text{SVD}}(X)
$$

### Phase 3：联合微调（恢复功能）

1. 冻结未压缩部分，仅训练 $\{A_j,B_j,A,B\}$（或按论文实验设定训练全部压缩相关参数）
2. 用 LM loss 在 $\mathcal{D}_{\text{cal}}$ 上训练若干步
3. 得到最终压缩模型 $M_{\text{CALR}}$

------

## 7）CALR 的方法学要点总结（从公式角度看它“新”在哪里）

你可以用一句非常“method”式的表达总结它的改动：

- 传统：$\min\|W-W_r\|_F^2$（权重重构最优）
- CALR：在结构上令

$$
F_{\text{CALR}}(X)=F_{\text{SVD}}(X)+XAB
$$

并在训练上直接最小化
$$
\mathbb{E}\|F_{\text{orig}}(X)-F_{\text{CALR}}(X)\|^2
$$
把“压缩残差”变成可学习信号（functional residual learning）。

------

## 8）一些可延展的 idea（只给方向，不展开成详细融合方案）

1. **校正支路的输入不一定要是 $X$，可以是更贴近残差来源的中间特征**
    例如对 gated FFN，残差可能主要来自 $(XW_g)\odot \mathrm{ACT}(XW_u)$ 的门控交互误差。可以考虑校正支路输入用门控后的中间张量 $Z=(XW_g)\odot \mathrm{ACT}(XW_u)$，再做低秩校正：

$$
F_{\text{corr}}(X)=Z\,C D
$$

（仍是低秩，但更对齐残差结构。）

1. **把“选安静层”从启发式变成预算优化**
    他们用 transformation 排序选 $N_{\text{target}}$。你可以把它写成：

$$
\min_{\mathcal{S}\subseteq \text{FFN},|\mathcal{S}|=N}
\sum_{m\in\mathcal{S}} \Delta_m
$$

其中 $\Delta_m$ 是“该模块压缩后不可恢复的功能损失预测值”，transform 只是 $\Delta$ 的一个 proxy；后续可以用更强的 proxy（例如输出 Hessian/Fisher 或 SAES 风格的累计误差风险）。

1. **让校正秩 $r_c$ 自适应（按层分配）**
    论文消融显示 $r_c$ 越大恢复越强但收益递减。可以考虑一个“分配型”目标：

$$
\min_{\{r_c^{(m)}\}}\ \sum_m \mathcal{E}_m(r_c^{(m)})
\quad \text{s.t.}\quad \sum_m r_c^{(m)} \le R_{\text{budget}}
$$

把校正参数预算集中给最需要恢复的层。

1. **校正支路可加“谱约束/正则”以稳定训练**
    例如对校正矩阵 $AB$ 加核范数或奇异值衰减正则：

$$
\min \ \mathcal{L}_{\text{LM}} + \lambda\|AB\|_*
$$

避免校正支路在少量校准数据上过拟合成“高频补丁”。

1. **“校正残差低秩”这一假设可以进一步验证成可预测指标**
    他们用残差谱衰减做经验佐证。你可以把“残差的有效秩”当作一个可测信号，用来决定是否对某层启用 CALR、以及用多大 $r_c$。