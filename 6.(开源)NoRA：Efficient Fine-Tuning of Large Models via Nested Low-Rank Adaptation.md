# NoRA（Efficient Fine-Tuning of Large Models via Nested Low-Rank Adaptation）

这篇论文围绕一个问题：**LoRA 虽然参数高效，但训练参数仍然不小；而更“极致参数少”的 LoRA 变体（把低秩矩阵固定/冻结、只训练少量向量/小模块）往往性能掉点**。作者给出两条主线改进：

1. **结构**：提出 **NoRA（Nested LoRA）**——外层 LoRA 用更好的分解初始化并冻结，只训练一个更小的“内层 LoRA”（串联结构），在极少训练参数下保持适配能力。
2. **初始化**：提出 **AwSVD**——用激活统计构造一个对角缩放矩阵 $S$，先“激活加权”再做 SVD，让外层低秩初始化更贴近真实输出，从而减少分解输出误差、加速收敛。

------

## 1) 基线：LoRA 的优化对象与常见形式

对线性层（或投影矩阵）$W\in\mathbb{R}^{m\times n}$，输入 $x\in\mathbb{R}^n$。

### 1.1 标准 LoRA 前向与训练变量

LoRA 写成对权重增量的低秩更新：
$$
h = Wx + \Delta Wx = Wx + BAx
$$
其中

- $A\in\mathbb{R}^{r\times n}$, $B\in\mathbb{R}^{m\times r}$
- $r\ll \min(m,n)$
- 训练时冻结 $W$，只训练 $A,B$。

**隐含优化目标（经验风险最小化）**：对下游任务损失 $\mathcal{L}$：
$$
\min_{A,B}\ \mathbb{E}_{(x,y)}\big[\mathcal{L}(f(W + BA; x), y)\big]
$$

------

## 2) NoRA：从“单层低秩增量”到“嵌套低秩增量”的改进目标

### 2.1 NoRA 的嵌套结构（外层冻结 + 内层可训）

NoRA 把 LoRA 的增量写成“外层低秩 + 内层低秩的嵌套”：
$$
h = Wx + \Delta Wx = Wx + BB' A' A x
\tag{NoRA}
$$
其中

- 外层：$A\in\mathbb{R}^{r\times n},\ B\in\mathbb{R}^{m\times r}$（**冻结**）
- 内层：$A'\in\mathbb{R}^{r'\times r},\ B'\in\mathbb{R}^{r\times r'}$（**训练**）
- 通常 $r' < r$（比如 $r/2$ 或 $r/4$）。

等价地，NoRA 的可训练增量是：
$$
\Delta W_{\text{NoRA}} = B\,M\,A,\quad M = B'A' \in \mathbb{R}^{r\times r}
$$
也就是：**把训练自由度压缩到“外层子空间”内的一个小矩阵 $M$**。

### 2.2 参数量对比（为什么 NoRA 能极致省参数）

- 标准 LoRA 训练参数：$\#(A,B)=rn + mr$
- NoRA 训练参数只来自内层：$\#(A',B')=r'r + rr' = 2rr'$

当 $m,n$ 很大、$r'$ 很小，$2rr'\ll rn+mr$，训练参数可下降几个数量级。

------

## 3) AwSVD：从“权重 SVD 初始化”到“激活感知 SVD 初始化”的目标变化

NoRA 的关键在于：**外层 $A,B$ 冻结**，因此外层初始化是否“贴近有效子空间”非常重要。

### 3.1 Vanilla SVD 初始化的局限

如果直接对 $W$ 做截断 SVD：
$$
W \approx U_r\Sigma_rV_r^\top
$$
可以用它初始化 LoRA 的 $B,A$，但这种分解目标本质是权重空间的：
$$
\min_{\mathrm{rank}(W_r)\le r}\ \|W - W_r\|_F^2
$$
它并不直接最小化下游前向的输出误差 $\|Wx - W_rx\|$，尤其当输入激活各维度尺度差异大、存在 outlier 时，**“权重误差小”不等于“输出误差小”**。

### 3.2 AwSVD 的核心：用激活二阶统计做对角缩放

设校准 batch 的输入激活矩阵 $X\in\mathbb{R}^{b\times n}$。构造对角缩放：
$$
S=\sqrt{\mathrm{diag}(X^\top X)}\in\mathbb{R}^{n\times n}
\tag{1}
$$
（直观：第 $j$ 维输入越“活跃/尺度越大”，对应的 $S_{jj}$ 越大。）

定义激活加权矩阵：
$$
W_{\text{aw}} = W\cdot S
\tag{2}
$$
然后对 $W_{\text{aw}}$ 做 SVD：
$$
W_{\text{aw}} = U\Sigma V^\top
\tag{3}
$$
取前 $r$ 阶：
$$
U_r,\Sigma_r,V_r
$$
并用它初始化外层：
$$
B = U_r\Sigma_r,\qquad
A = V_r^\top S^{-1}
\tag{4}
$$
于是外层等价近似：
$$
W \approx B A = U_r\Sigma_r(V_r^\top S^{-1})
\tag{5}
$$

### 3.3 AwSVD 对应的“改进目标”是什么？

可以把它理解成：AwSVD 不是在 $\|W-W_r\|_F$ 意义下做 rank-$r$ 逼近，而是优先对齐“激活加权后的误差”：

若把输入尺度吸收到权重里，考虑变换输入 $x' = S^{-1}x$，则：
$$
Wx = (WS)\,x'
$$
因此 AwSVD 本质是在近似 $WS$：
$$
\min_{\mathrm{rank}(M)\le r}\ \|WS - M\|_F^2
\quad\Rightarrow\quad
M = U_r\Sigma_rV_r^\top
$$
再映回：
$$
W \approx M S^{-1}
$$
这就解释了为什么初始化里会出现 $S^{-1}$。

从“输出误差”的角度看，如果把输入协方差粗略近似为对角（用 $S$ 捕捉每维尺度），那么
$$
\mathbb{E}\| (W-\hat W)x\|_2^2
\approx
\|(W-\hat W)S\|_F^2
=
\|WS - \hat W S\|_F^2
$$
AwSVD 相当于在这个近似下直接最小化输出误差代理。

------

## 4) 结构对比：为什么作者更偏好 “Serial / wa 位置 / Nested”

论文把“可训练结构”看成在低秩分解后的某些位置（$W_A, W_B$ 或更细的 $w_a,w_b$）插入可训练模块，比较了：

- 串联（serial） vs 并联（parallel）
- 插入位置 wa vs wb
- “嵌套 LoRA” vs “适配器（adapter）”

一个很有用的抽象是把不同结构都写成 “冻结基底 + 小的可训扰动”：

- Adapter 串联（示意）：

$$
h_{\text{adapter}} = Wx + B D A x
$$

其中 $D\in\mathbb{R}^{r\times r}$ 可训。

- 并联 LoRA（示意）：

$$
h_{\text{parallel}} = Wx + (B + C A')A x
$$

其中 $C\in\mathbb{R}^{m\times r'}$, $A'\in\mathbb{R}^{r'\times r}$ 可训。

- NoRA（嵌套）：

$$
h_{\text{NoRA}} = Wx + B B' A' A x
$$

可训只在很小的 $r\times r$ 子空间内发生（通过 $B'A'$）。

作者的经验结论可以理解为：**串联更稳**（更像“在低秩路径内精修”），而 wa 位置更像直接作用在“更关键的奇异向量承载处”，因此优化更友好。

------

## 5) 理论性质（论文给了可用于写作的三个点）

### 5.1 分解误差上界（外层冻结为何不至于崩）

论文给出一个谱范数误差上界形式：
$$
\|W - W_{\text{approx}}\|_2 \le \sigma_{r+1}\cdot \kappa(S)
$$
其中 $\sigma_{r+1}$ 是 $W_{\text{aw}}$ 的第 $r\!+\!1$ 个奇异值，$\kappa(S)$ 是 $S$ 的条件数。直观意义：**激活加权让重要方向的奇异值衰减更快**，从而更小的 $r$ 就能压低尾部误差；但若 $S$ 病态（维度尺度差距极大），上界会变松。

### 5.2 梯度“有效分量”放大/保留（为何收敛更快）

作者给出一种解释：用 AwSVD 初始化把外层基底对齐到权重的主列空间/主行空间后，梯度在这些子空间的投影分量被更好保留，噪声分量相对被抑制，因此在相同训练预算下更快收敛。

### 5.3 冻结外层仍有“幅度 + 方向”调制能力（为何可适配）

把更新写成：
$$
\Delta W = (U_r\Sigma_r)\,(B'A')\,(V_r^\top S^{-1})
$$
内层矩阵 $B'A'$ 同时能改变更新的**大小（magnitude）\**与\**方向（direction）**：

- magnitude：$\|B'A'\|_F$ 缩放整体更新幅度
- direction：$B'A'$ 与单位矩阵/某目标方向的夹角决定更新方向偏转程度

因此即便外层冻结，也不意味着更新“死板”，而是把更新限制在一个合理的、预训练对齐的子空间里再精修。

------

## 6) Step-by-step 实现流程（从目标公式到代码落地）

下面给一个“你写实现时最顺”的流程，按单个线性层/投影矩阵来描述；在实际模型里对一组层重复即可。

### Step 0：选择插入位置与 rank

- 选定要做 NoRA 的模块（如 attention 的 q/k/v/o 或 FFN 的投影）
- 设外层 rank $r$，内层 rank $r'$（通常 $r'/r\in\{1/2,1/4\}$）

### Step 1：收集校准激活并构造缩放 $S$

1. 用少量校准样本跑前向，收集输入激活矩阵 $X\in\mathbb{R}^{b\times n}$
2. 计算：

$$
S=\sqrt{\mathrm{diag}(X^\top X)}\quad(\text{对角矩阵})
$$

> 工程建议：给 $S$ 加下界 $\epsilon$，避免出现非常小的对角项导致 $S^{-1}$ 爆炸：

$$
S_{jj}\leftarrow \max(S_{jj},\epsilon)
$$

### Step 2：AwSVD 分解并初始化外层 $A,B$

1. 计算激活加权矩阵：$W_{\text{aw}} = WS$
2. 做截断 SVD：$W_{\text{aw}}\approx U_r\Sigma_rV_r^\top$
3. 初始化外层：

$$
B \leftarrow U_r\Sigma_r,\qquad A\leftarrow V_r^\top S^{-1}
$$

1. **冻结** $A,B$（不参与训练）

### Step 3：初始化内层 $A',B'$

- 用高斯初始化（论文经验上优于 0 / 均匀 / 奇异值对角初始化）：

$$
A'\sim\mathcal{N}(0,\sigma^2),\quad B'\sim\mathcal{N}(0,\sigma^2)
$$

- 只训练 $A',B'$。

### Step 4：前向计算（训练与推理一致）

$$
h = Wx + B(B'A')Ax
$$

通常实现时会按计算图重排以减少开销，例如：

1. $t_1 = Ax$（$r$ 维）
2. $t_2 = A't_1$（$r'$ 维）
3. $t_3 = B't_2$（$r$ 维）
4. $t_4 = Bt_3$（$m$ 维）
5. 输出：$Wx + t_4$

### Step 5：训练

优化：
$$
\min_{A',B'}\ \mathbb{E}_{(x,y)}\big[\mathcal{L}(f(W + B(B'A')A; x), y)\big]
$$
其余参数冻结。

------

## 7) 一些可以延展的 ideas（不展开“怎么结合”，只给方向）

### Idea 1：把 AwSVD 的对角缩放 $S$ 从 “diag(XᵀX)” 升级为更鲁棒的统计

当前 $S$ 对 outlier 很敏感（因为平方和会被极端值主导）。可以尝试：

- 用分位数/截断均值估计每维尺度
- 或用 Huber/trimmed 估计构造鲁棒对角缩放
   目标：降低 $\kappa(S)$，让误差上界更紧、更稳。

### Idea 2：把 AwSVD 从“对角预条件”推广到“低秩 + 对角”的预条件（更接近真实协方差）

对角近似忽略维间相关。可用：
$$
X^\top X \approx D + QQ^\top
$$
其中 $D$ 对角、$Q$ 低秩，既保持可计算性，又能捕捉主要相关结构；然后在该预条件下做分解初始化。

### Idea 3：内层 $B'A'$ 改成“结构化可训”（提升表达而不显著增参）

例如让 $B'A'$ 具备：

- block-diagonal（按 head / 通道分组）
- 对称/反对称约束（控制方向调制）
- sparse + low-rank 混合
   在参数几乎不变的前提下提升适配灵活性。

### Idea 4：rank 选择做任务自适应（外层 r 由分解误差/能量决定，内层 r' 由任务难度决定）

外层 $r$ 适合用“能量保留/尾奇异值衰减”自动选；内层 $r'$ 可用验证集或训练早期信号（loss 曲线/梯度范数）动态调节，实现更稳的“性能—参数”权衡。

### Idea 5：把 NoRA 推广为“多子空间混合”（Mixture-of-Subspaces NoRA）

当前外层基底是单一子空间 $U_r,V_r$。可以做多个子空间并门控选择（类似 MoE 思路但保持低成本）：
$$
\Delta W = \sum_{t=1}^{T} \pi_t(x)\,B_t M_t A_t
$$
其中 $T$ 很小、$\pi_t$ 轻量，适合多域/多任务适配。