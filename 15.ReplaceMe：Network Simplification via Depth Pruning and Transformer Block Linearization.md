# ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization

## 论文要解决的核心问题

**ReplaceMe** 关注的是一种和“低秩/量化/稀疏剪枝”不同的压缩维度：**深度剪枝（depth pruning）**。它的假设是：在 Transformer 里，**一段连续的 block（若足够“线性/冗余”）可以被一个线性映射近似替代**，从而把这段 block 直接删掉，并用一个估计出来的线性变换把“切点前后的表征空间”对齐回来。

关键点在于：它是 **training-free（无需 healing 微调）** 的，在“合理剪掉比例”（例如 25% 深度）时依然能保持较高精度，并且估计出来的线性变换还能**融合进保留下来的某个线性层权重**，不增加参数结构。

------

## 1）从标准 Transformer block 的表达开始（原始结构公式）

对第 $i$ 个 Transformer block，令输入 hidden state 为 $X_i \in \mathbb{R}^{N\times d}$（$N$ token 数、$d$ hidden 维）。标准的 pre-norm 结构可写为：
$$
Y_i = X_i + \mathrm{MHA}_i(\mathrm{LN}^{(1)}_i(X_i))
$$
其中：

- $Y_i$：attention 子块输出（含残差）
- $M_i$：MLP 子块输出（不含最后残差）
- $L_i$：整个 block 输出（含第二个残差）

ReplaceMe 要剪掉的是一段连续 block：$(i^*+1, \dots, i^*+n)$。直观地，它想让模型在第 $i^*$ 个 block 之后**跳过**这 $n$ 个 block，直接去接第 $i^*+n+1$ 个 block。

------

## 2）层段选择：把“剪哪里”写成一个可计算的优化（从目标到可执行）

### 2.1 原始选择目标（最小化剪掉后表征差异）

它用一个距离函数 $D(\cdot,\cdot)$ 来度量“剪断前后输出 hidden state 的相似性”，并用滑窗选择最适合剪掉的连续段起点：
$$
i^* = \arg\min_i D(L_i, L_{i+n})
$$
直觉：如果 $L_i$ 和 $L_{i+n}$ 很接近，那么中间这 $n$ 个 block 对表征的“净变化”很小，可能冗余、可剪。

### 2.2 为什么用 cosine distance 更好

论文经验结论是：用 cosine distance 做 $D$ 往往比 $L_2$ 距离更能找到“可剪但不崩”的段。形式上（把 token 展开成行向量 $L_{i,k}\in\mathbb{R}^d$）：
$$
D_{\cos}(L_i, L_{i+n})=\sum_{k=1}^{N}\left(1-\frac{L_{i,k}^{\top}L_{i+n,k}}{\|L_{i,k}\|_2\|L_{i+n,k}\|_2}\right)
$$
这一步本质是一个 **深度剪枝的 block-influence / block-linearity 指标**：越“线性/冗余”的段，cos 距离越小。

------

## 3）线性替代：从“想要等价输出”到“可解的线性回归”

### 3.1 ReplaceMe 的核心：估计一个线性变换 $T$

ReplaceMe 不直接让 $L_i \mapsto L_{i+n}$，而是选在一个更好融合的位置：**在第 $i^\*$ 个 block 内，MLP 输出 $M_i$ 之后、加残差之前**插入线性变换。

它希望估计 $T\in\mathbb{R}^{d\times d}$，使得经过线性补偿后，第 $i$ 个 block 的输出能逼近剪掉 $n$ 个 block 后的输出：
$$
T^* = \arg\min_T\ h(M_i T + Y_i,\ L_{i+n})
$$
这里：

- $M_iT + Y_i$ 相当于“把第 $i$ 个 block 的 MLP 分支经过 $T$ 再加回 attention 残差”
- $h(\cdot,\cdot)$ 可以是 $L_2$ 或 cosine distance（论文两条路线都做了）

这就是从“结构剪枝”转成一个**校准集上的线性映射估计问题**。

------

## 4）两种目标函数：L2（有闭式解） vs Cosine（需要数值解）

### 4.1 目标 1：$L_2$ 距离（Least Squares，闭式解）

令目标为：
$$
\min_T \ \| (M_iT + Y_i) - L_{i+n} \|_F^2
$$
把 $Y_i$ 移到右侧，等价为：
$$
\min_T \ \|M_iT - (L_{i+n}-Y_i)\|_F^2
$$
这是标准最小二乘，正规方程给出闭式解：
$$
T^*=(M_i^\top M_i)^{-1}M_i^\top(L_{i+n}-Y_i)
$$
优点：一次矩阵运算就搞定，极快、稳定、完全 training-free。缺点：它在优化上更像匹配“欧氏误差”，未必最贴合生成式模型真正关心的方向一致性。

### 4.2 目标 2：Cosine distance（更贴合层选择指标，但无闭式解）

原式（逐 token 聚合）：
$$
\min_T \sum_{k=1}^{N}\left(1-\frac{(M_{i,k}T+Y_{i,k})^\top L_{i+n,k}}{\|M_{i,k}T+Y_{i,k}\|_2\|L_{i+n,k}\|_2}\right)
$$
因为分母里有 $\|M_{i,k}T+Y_{i,k}\|_2$，不再是二次型，因此没有闭式解，论文用 Adam / L-BFGS 等数值优化求解。

#### 重要工程化改写：减少内存的等价近似

为了避免存三份激活（$M_i, Y_i, L_{i+n}$），论文把目标改写为只存两份：
$$
\min_T\ \mathrm{cosine\_distance}(M_iT,\ L_{i+n}-Y_i)
$$
理由：在 cosine 相似度意义下，是否把同一个 $Y_i$ 加到两边，对最优 $T$ 的影响很小；但内存直接从“存 3 个张量”降到“存 2 个张量”，在长序列/大批量校准时非常关键。

------

## 5）把线性变换“无参融合”进模型：为什么不会增加结构复杂度

估计出 $T^*$ 后，ReplaceMe 会把它**融合进第 $i^\*$ 个 block 的 MLP 的某个线性层权重**（论文描述为融合到 FFN 的第二层/下投影），从而：

- 模型图里不需要显式多一个层（仍可等价实现）
- 同时删除中间 $n$ 个 block
- 整体架构对下游部署更友好（不需要特制算子）

抽象写法：如果 MLP 的某一线性层权重是 $W_{down}$，把 $T$ 融进去相当于把某处线性组合替换为 $W_{down}\leftarrow W_{down}T$（或 $T$ 与相邻权重左右乘的某种等价融合，具体取决于你插入点在前还是后）。

------

## 6）正则化：从“纯拟合”到“泛化/权衡 PPL vs Accuracy”

论文把目标统一写成带正则项：
$$
T^*=\arg\min_T\ h(M_iT+Y_i,\ L_{i+n}) + \alpha R(T)
$$

- 对 $L_2$ 目标，常用 ridge（$R(T)=\|T\|_F^2$）还能推导闭式解（变成 $(M^\top M+\alpha I)^{-1}M^\top\cdot$）。
- 对 cosine 目标，论文尝试 $L_1$ / $L_2$ 正则，经验上会出现一种典型权衡：**accuracy 可能涨，但 perplexity 可能变差**。这很像“让 $T$ 更偏向任务判别/对齐，而不是语言建模的概率校准”。

从理论视角看，这一步的意义在于：当校准集有限时，$T$ 的自由度是 $d^2$（非常大），纯拟合容易把 $T$ 拟合到校准集的局部结构；正则化相当于施加先验（例如让 $T$ 更接近稀疏/小范数），提升泛化。

------

## 7）扩展：多个线性变换（Multi-LT）

单个 $T$ 替代一段连续 block；Multi-LT 把剪枝分成多个不重叠段，每段各自学一个 $T_j$。形式上：

- 对每个段 $(i_j, n_j)$，估计 $T_j$：
  $$
  T_j^*=\arg\min_T h(M_{i_j}T+Y_{i_j},\ L_{i_j+n_j})
  $$

- 统一剪掉各段中间 block，并分别融合 $T_j$ 到对应位置。

Multi-LT 的意义：当压缩率更高时，一个 $T$ 很难拟合“长跨度”的非线性累积，拆成多个短跨度，线性近似更靠谱。

此外论文还讨论了“非连续段也可以合并或强制不连续”的变体，本质是让选择空间更灵活。

------

## 8）结构化 $T$：对 $T$ 加结构约束的解析解

这部分很适合做“理论补充”：

### 8.1 $T$ 限制为对角矩阵（只做通道缩放）

$$
T\in\mathcal{D}_{d\times d}
$$

在 $L_2$ 目标下有闭式解（本质上是把正规方程的非对角项去掉），可写成 Hadamard 形式：
$$
T^*=\big((M^\top M)\circ I\big)^{-1}\big(M^\top(L-Y)\circ I\big)
$$
这等价于“每个通道单独回归一个缩放系数”，可解释性强，但表达力有限。

### 8.2 $T$ 限制为正交矩阵（只允许旋转/反射）

约束 $T^\top T=I$，目标：
$$
\min_{T^\top T=I}\ \|M T-(L-Y)\|_F^2
$$
这是经典 Procrustes 问题：令
$$
M^\top(L-Y)=U\Sigma V^\top
$$
则
$$
T^*=UV^\top
$$
论文经验上认为“只旋转不缩放”往往不够，因为实际需要恢复的不只是方向，还需要通道幅度重标定。

------

## 9）Step 实现过程（按你做复现/对接工程的角度写清楚）

下面给一个“最小可复现”的流水线（训练自由）：

### Step 0：设定要剪的深度比例

例如要剪 25%，决定窗口长度 $n$（一次剪掉连续 $n$ 个 block），或决定 Multi-LT 的段数与每段长度。

### Step 1：准备校准集并跑前向，缓存每层激活

对校准数据前向一次，缓存（对每个候选 cut $i$）：

- $L_i$
- $L_{i+n}$

以及对最终选中的 $i^*$，额外缓存：

- $M_{i^*}$（MLP 输出）
- $Y_{i^*}$（attention 残差后输出）

### Step 2：层段选择（滑窗）

对所有候选 $i$，计算
$$
D(L_i,L_{i+n})
$$
取最小者 $i^*$。

### Step 3：估计线性变换 $T$

- 走 LS（闭式解）：
  $$
  T^*=(M^\top M)^{-1}M^\top(L-Y)
  $$
  可选加 ridge：
  $$
  T^*=(M^\top M+\alpha I)^{-1}M^\top(L-Y)
  $$

- 走 cosine（数值解）：
  $$
  \min_T\ \mathrm{cos}(MT,\ L-Y) + \alpha R(T)
  $$
  用 Adam/L-BFGS 优化；若内存紧，用上面的两激活近似目标。

### Step 4：剪掉 block 并插入/融合 $T$

删除 $(i^*+1,\dots,i^*+n)$ 这些 block；
 把 $T$ 插入到第 $i^*$ 个 block 的 MLP 输出分支（加残差之前），并进一步融合到相邻线性层权重里（保证部署无额外结构）。

### Step 5：可选 Multi-LT

重复 Step 1–4，对多个段各自估 $T_j$ 并融合。

------

## 10）一些可写新论文的 idea（不展开成具体融合建议，只给“可落公式的方向”）

### Idea 1：把 ReplaceMe 的线性替代目标，从“激活对齐”升级为“白化后的最小误差替代”

ReplaceMe 的 LS 目标是
$$
\min_T \|MT-(L-Y)\|_F^2
$$
如果你引入输入统计（例如 $S=MM^\top$ 或更泛化的 token/通道权重），可以写成加权最小二乘：
$$
\min_T \|(MT-(L-Y))W\|_F^2
$$
或二次型：
$$
\min_T \mathrm{tr}\big((MT-(L-Y))^\top A (MT-(L-Y))\big)
$$
这样能把“哪些 token/哪些通道更重要”显式写进目标（特别适合生成任务早期 token 更重要的场景）。

### Idea 2：把线性替代从“单点映射”变成“带误差抑制项的纠偏映射”

ReplaceMe 只用 $M$ 去拟合 $L-Y$。如果你能估计 teacher/student 差值 $E$（例如 $E=(L-Y)-M$ 或更一般的误差项），可引入一个可控系数 $\beta$：
$$
\min_T \|M T - (L-Y + \beta E)\|_F^2
$$
这能把“线性替代”变成一个“可调的误差补偿”问题（$\beta$ 可以用闭式、约束、或简单网格选择）。

### Idea 3：层段选择不再仅靠 cosine，而是靠“可替代性上界”

现在选段是
$$
\arg\min_i D(L_i,L_{i+n})
$$
你可以用一个更贴合 ReplaceMe 的“可替代性指标”，比如最小二乘残差能量（拟合难度）：
$$
\epsilon_i = \min_T \|M_iT-(L_{i+n}-Y_i)\|_F^2
$$
然后选
$$
i^*=\arg\min_i \epsilon_i
$$
这比单纯 cosine 更直接地度量“是否真能用线性映射替代”。

### Idea 4：把 $T$ 结构化成“低秩 + 对角缩放”，兼顾表达力与稳定性

对角 $T$ 太弱、全矩阵 $T$ 太自由。可以考虑：
$$
T = D + UV^\top
$$
其中 $D$ 对角、$U,V$ 低秩。这样既保留了“通道尺度校准”的主效应，又给少量跨通道耦合能力，目标仍可写成可解的最小二乘（或交替最小化）。

### Idea 5：Multi-LT 的段划分可以从“手动/贪心”升级为预算约束的组合优化

把每个候选段 $j$ 的收益/代价写清楚：

- 代价：剪掉的 block 数（latency savings）
- 误差：该段最优拟合残差 $\epsilon_j$

写成一个类似背包问题：
$$
\min_{\mathcal{S}}\ \sum_{j\in\mathcal{S}} \epsilon_j
\quad \text{s.t.}\quad \sum_{j\in\mathcal{S}} n_j \ge \text{目标剪枝量}
$$
这样“剪哪些段 + 用几个 LT”就有明确数学形态。