#coding:utf8
"""
SAES-SVD v2.1.4: å‚æ•°ä¿®æ­£ç‰ˆ (åŸºäºå®éªŒç»“æœä¼˜åŒ–)

ç‰ˆæœ¬å†å²ï¼š
- v2.1.4 (2025-12-30): åŸºäºalphaæ¶ˆèå®éªŒç»“æœçš„å‚æ•°ä¼˜åŒ–
  â˜… å…³é”®ä¿®æ”¹: åŸºäºExp-B5æœ€ä¼˜ç»“æœ(PPL=8.055)çš„å‚æ•°è°ƒæ•´
  â˜… beta_capä¿®æ­£: 0.5 â†’ 0.375 (å¯¹åº”alpha_max=0.6çš„ç†è®ºä¸Šç•Œ)
  â˜… rhoä¿®æ­£: 0.8/1.0 â†’ 0.95 (å‡å°‘è¿‡åº¦æ”¶ç¼©ï¼Œå®éªŒæ˜¾ç¤ºrho=0.8å¯¼è‡´avg_beta=0.287è¿‡ä½)
  â˜… é»˜è®¤alphaèŒƒå›´: [0.25,0.75] â†’ [0.4,0.6] (å®éªŒè¯æ˜è¯¥åŒºé—´æœ€ä¼˜)
  â˜… ç›®æ ‡: æ¶ˆé™¤å‚æ•°æŠ¤æ å¯¹æœ€ä¼˜Î²é€‰æ‹©çš„ä¸åˆç†é™åˆ¶

- v2.1.3 (2025-12-21): è°ƒå‚ç‰ˆæœ¬

å®éªŒä¾æ®ï¼š
- Exp-B5: PPL=8.055, rho=0.8, eps=1e-4, alpha=[0.4,0.6], avg_beta=0.287
- é—®é¢˜: avg_beta=0.287 < beta_min=0.286 (alpha=0.4å¯¹åº”å€¼)ï¼Œè¯´æ˜rho=0.8è¿‡åº¦æ”¶ç¼©
- è§£å†³: æé«˜rhoè‡³0.95ï¼Œé™ä½beta_capè‡³0.375ï¼Œç¡®ä¿Î²åœ¨åˆç†åŒºé—´å†…

ä½œè€…ï¼šé‡æ„ç‰ˆ v2.1.4
"""

import os
print(f"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")
import sys
import argparse
import torch
import torch.nn as nn
from tqdm import tqdm
from datetime import datetime

# æ·»åŠ è·¯å¾„
current_path = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_path)

from utils.data_utils import *
from utils.model_utils import *
from evaluater import *
from component.svd_llama import SVD_LlamaAttention, SVD_LlamaMLP
from component.svd_mistral import SVD_MistralAttention, SVD_MistralMLP
from component.svd_opt import SVDOPTDecoderLayer

# ==================== Wandb Configuration ====================
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    print("âš ï¸ Warning: wandb not installed.")
    WANDB_AVAILABLE = False
    wandb = None


def compute_H_inv_sqrt(H, device, eps=1e-4):
    """
    ä½¿ç”¨ç‰¹å¾åˆ†è§£è®¡ç®— H^{-1/2}
    
    Args:
        H: è¾“å…¥åæ–¹å·®çŸ©é˜µ (n, n)
        device: è®¡ç®—è®¾å¤‡
        eps: æ­£åˆ™åŒ–å‚æ•°
    
    Returns:
        L: H^{-1/2} (å¯¹ç§°çŸ©é˜µ)
    """
    d = H.shape[0]
    
    # â˜… å¯¹äºå¤§çŸ©é˜µ(>4096)ï¼Œåœ¨CPUä¸Šè®¡ç®—ç‰¹å¾åˆ†è§£æ›´å®‰å…¨
    if d > 4096:
        H_cpu = H.cpu().float()
        H_reg = H_cpu + eps * torch.eye(d, device='cpu', dtype=torch.float32)
        eigenvalues, eigenvectors = torch.linalg.eigh(H_reg)
        eigenvalues = torch.clamp(eigenvalues, min=eps)
        L = eigenvectors @ torch.diag(1.0 / torch.sqrt(eigenvalues)) @ eigenvectors.T
        return L.to(device)
    else:
        H_reg = H + eps * torch.eye(d, device=device, dtype=H.dtype)
        eigenvalues, eigenvectors = torch.linalg.eigh(H_reg)
        eigenvalues = torch.clamp(eigenvalues, min=eps)
        L = eigenvectors @ torch.diag(1.0 / torch.sqrt(eigenvalues)) @ eigenvectors.T
        return L


def aces_beta_select_unified(W, H, Delta, L, rank, 
                              alpha_min=0.4, alpha_max=0.6,
                              rho=0.95, beta_cap=0.375):
    """
    SAES-SVD Algorithm 3: ACES Beta Selection (ä¿®æ­£ç‰ˆ v2.1.4)
    
    â˜… v2.1.4å…³é”®ä¿®æ­£:
    - beta_cap: 0.5 â†’ 0.375 (ç²¾ç¡®å¯¹åº”alpha_max=0.6çš„Î²ä¸Šç•Œ)
    - rho: 0.8/1.0 â†’ 0.95 (å®éªŒæ˜¾ç¤º0.8è¿‡åº¦æ”¶ç¼©)
    - é»˜è®¤alpha: [0.25,0.75] â†’ [0.4,0.6] (å®éªŒæœ€ä¼˜åŒºé—´)
    
    å®éªŒä¾æ®:
    Exp-B5æœ€ä¼˜ç»“æœ(PPL=8.055)æ˜¾ç¤ºavg_beta=0.287ï¼Œä½äºç†è®ºä¸‹ç•Œ0.286(alpha=0.4)ï¼Œ
    è¯´æ˜rho=0.8è¿‡åº¦æ”¶ç¼©ã€‚æé«˜rhoè‡³0.95å¯è®©ACESé€‰æ‹©çš„Î²æ›´æ¥è¿‘ç†è®ºæœ€ä¼˜å€¼ã€‚
    
    Args:
        W: æƒé‡çŸ©é˜µ (m, n)
        H: åæ–¹å·®çŸ©é˜µ (n, n)
        Delta: å·®åˆ†åæ–¹å·® (n, n)
        L: ç™½åŒ–çŸ©é˜µ H^{-1/2}
        rank: ç›®æ ‡ç§©
        alpha_min: Î±ä¸‹ç•Œ (é»˜è®¤0.4ï¼ŒåŸºäºå®éªŒ)
        alpha_max: Î±ä¸Šç•Œ (é»˜è®¤0.6ï¼ŒåŸºäºå®éªŒ)
        rho: æ”¶ç¼©å› å­ (é»˜è®¤0.95ï¼Œè½»å¾®æ”¶ç¼©)
        beta_cap: Î²å®‰å…¨ä¸Šé™ (é»˜è®¤0.375=0.6/(1+0.6))
    
    Returns:
        beta_star: æœ€ä¼˜ Î²
        info: è¯Šæ–­ä¿¡æ¯ï¼ˆå«Î±_starï¼‰
    """
    # â˜… æ ¸å¿ƒä¿®æ”¹: Î± â†’ Î² è½¬æ¢
    beta_min = alpha_min / (1 + alpha_min)  # 0.25 -> 0.2
    beta_max = alpha_max / (1 + alpha_max)  # 0.75 -> 0.4286
    
    device = W.device
    m, n = W.shape
    
    # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
    W = W.float()
    H = H.float()
    Delta = Delta.float()
    L = L.float()
    
    # æ„é€  S = W H L å’Œ D = W Î” L (è®ºæ–‡å…¬å¼13)
    S = W @ H @ L
    D = W @ Delta @ L
    
    # Step 2: å¯¹ S åš SVD è·å–ç§©-r ä¸»å­ç©ºé—´
    U, Sigma, Vt = torch.linalg.svd(S, full_matrices=False)
    U_r = U[:, :rank]
    V_r = Vt[:rank, :].T
    
    # â˜… ä½¿ç”¨éšå¼æŠ•å½±é¿å…OOM (è®ºæ–‡å…¬å¼15-16)
    # S_perp = P_L S P_R = (I - U_r U_r^T) S (I - V_r V_r^T)
    UrTS = U_r.T @ S
    SVr = S @ V_r
    UrTSVr = U_r.T @ SVr
    S_perp = S - U_r @ UrTS - SVr @ V_r.T + U_r @ UrTSVr @ V_r.T
    
    # D_perp åŒç†
    UrTD = U_r.T @ D
    DVr = D @ V_r
    UrTDVr = U_r.T @ DVr
    D_perp = D - U_r @ UrTD - DVr @ V_r.T + U_r @ UrTDVr @ V_r.T
    
    # è®¡ç®—ç³»æ•° a, b, c, A, B, C (è®ºæ–‡å…¬å¼ 18)
    a = torch.norm(S_perp, 'fro')**2
    b = torch.sum(S_perp * D_perp)
    c = torch.norm(D_perp, 'fro')**2
    
    A = torch.norm(S, 'fro')**2
    B = torch.sum(S * D)
    C = torch.norm(D, 'fro')**2
    
    # Step 4: æ±‚è§£äºŒæ¬¡æ–¹ç¨‹ (è®ºæ–‡å…¬å¼ 19)
    # p(Î²) = (cB - bC)Î²Â² + (cA - aC)Î² + (bA - aB) = 0
    coef_2 = c * B - b * C
    coef_1 = c * A - a * C
    coef_0 = b * A - a * B
    
    # å€™é€‰é›†: è¾¹ç•Œ + é©»ç‚¹
    candidates = [beta_min, beta_max]
    
    if abs(coef_2.item()) > 1e-10: # â† æ£€æŸ¥æ˜¯å¦ä¸ºäºŒæ¬¡æ–¹ç¨‹
        discriminant = coef_1**2 - 4 * coef_2 * coef_0 #â† åˆ¤åˆ«å¼Î”
        if discriminant >= 0: # â† æœ‰å®æ ¹
            sqrt_disc = torch.sqrt(discriminant)
            root1 = (-coef_1 + sqrt_disc) / (2 * coef_2) # â† å®æ ¹1
            root2 = (-coef_1 - sqrt_disc) / (2 * coef_2) # â† å®æ ¹2
            candidates.extend([root1.item(), root2.item()]) # â† åŠ å…¥å€™é€‰é›†
    
    # Step 5: è¯„åˆ†é€‰æ‹©æœ€ä¼˜ beta (è®ºæ–‡å…¬å¼17)
    best_beta = beta_min # åˆå§‹åŒ–ä¸ºä¸‹ç•Œ 0.2
    best_score = float('inf') # åˆå§‹åŒ–ä¸ºæ­£æ— ç©·å¤§
    
    for beta_candidate in candidates:
        # è£å‰ªåˆ°å¯è¡ŒåŸŸ [beta_min, beta_max]
        beta = max(beta_min, min(beta_candidate, beta_max))
        
        # è¾¹ç•Œå®‰å…¨æ£€æŸ¥ï¼ˆé¢å¤–ä¿æŠ¤ï¼‰
        if beta < 0 or beta >= 1:
            continue  # è·³è¿‡ï¼Œä¸æ›´æ–°best_betaå’Œbest_score
        
        # èƒ½é‡æ¯”ç›®æ ‡å‡½æ•° Ï_e(Î²) = (a + 2bÎ² + cÎ²Â²) / (A + 2BÎ² + CÎ²Â²)
        numerator = a + 2 * b * beta + c * beta**2
        denominator = A + 2 * B * beta + C * beta**2 + 1e-10
        score = numerator / denominator
        
        if score < best_score:
            best_score = score
            best_beta = beta
    
    # â˜… æŠ¤æ  (è®ºæ–‡Algorithm 3 Line 19)
    # Î²* = Ï Â· min(Î², Î²_cap)
    # æ³¨æ„: rho=1.0 è¡¨ç¤ºä¸æ”¶ç¼©ï¼Œbeta_cap=0.5 åªæ˜¯å®‰å…¨ä¸Šé™
    best_beta = rho * min(best_beta, beta_cap)
    
    # è®¡ç®—å¯¹åº”çš„ Î±*
    alpha_star = best_beta / (1 - best_beta) if best_beta < 1 else float('inf')
    
    return best_beta, {
        'alpha_star': alpha_star,
        'beta_min': beta_min,
        'beta_max': beta_max,
        'a': a.item(), 'b': b.item(), 'c': c.item(),
        'A': A.item(), 'B': B.item(), 'C': C.item(),
        'best_score': best_score if isinstance(best_score, float) else best_score.item()
    }


@torch.no_grad()
def saes_svd_compress_layerwise(model_name, model, calib_loader, ratio, dev, 
                                 eps=1e-4, rho=1.0, alpha_min=0.25, alpha_max=0.75, verbose=True):
    """
    SAES-SVD é€å±‚é—­ç¯å‹ç¼©
    
    æ ¸å¿ƒæµç¨‹ï¼š
    1. åˆ›å»º teacher model (å­˜å‚¨fp16ï¼Œè®¡ç®—fp32)
    2. é€å±‚å¤„ç†ï¼š
       a. ç”¨ã€å·²å‹ç¼©çš„ studentã€‘å‰å‘å¾—åˆ° X
       b. ç”¨ã€å…¨ç²¾åº¦ teacherã€‘å‰å‘å¾—åˆ° X^f
       c. ç»Ÿè®¡ H = X X^T, Î” = (X^f - X) X^T
       d. è®¡ç®— L = H^{-1/2}
       e. ACES é€‰æ‹© Î²* (ä½¿ç”¨åŒä¸€ä¸ª L)
       f. æ„é€  G = W(H + Î²*Î”)L, SVD å‹ç¼©
       g. ç«‹åˆ»æ›¿æ¢æœ¬å±‚æƒé‡
       h. â˜… é‡æ–°å‰å‘ä¼ æ’­è·å–å‹ç¼©åè¾“å‡ºï¼ˆé—­ç¯å…³é”®ï¼ï¼‰
       i. è¿›å…¥ä¸‹ä¸€å±‚
    
    Args:
        model_name: æ¨¡å‹åç§°
        model: å¾…å‹ç¼©æ¨¡å‹ (å°†è¢«åŸåœ°ä¿®æ”¹)
        calib_loader: æ ¡å‡†æ•°æ®
        ratio: ä¿ç•™æ¯”ä¾‹ (å¦‚ 0.8 è¡¨ç¤ºä¿ç•™ 80%)
        dev: è®¾å¤‡
        eps: æ­£åˆ™åŒ–å‚æ•°
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        compression_stats: å‹ç¼©ç»Ÿè®¡ä¿¡æ¯
    """
    print("=" * 60)
    print("ğŸš€ SAES-SVD v2.1.4: å‚æ•°ä¿®æ­£ç‰ˆ (åŸºäºå®éªŒä¼˜åŒ–)")
    print("=" * 60)
    print(f"ğŸ“Š å‹ç¼©æ¯”ä¾‹: {ratio} (ä¿ç•™ {ratio*100:.1f}%)")
    print(f"ğŸ“Š æ­£åˆ™åŒ–å‚æ•° eps: {eps}")
    print(f"ğŸ“Š AlphaèŒƒå›´: [{alpha_min}, {alpha_max}] â†’ BetaèŒƒå›´: [{alpha_min/(1+alpha_min):.4f}, {alpha_max/(1+alpha_max):.4f}]")
    print(f"ğŸ“Š Rhoæ”¶ç¼©å› å­: {rho}")
    print("=" * 60)
    
    # ========== Step 1: åˆ›å»º teacher model ==========
    # â˜… æ˜¾å­˜ä¼˜åŒ–: teacherå­˜å‚¨fp16ï¼Œè®¡ç®—æ—¶è½¬fp32
    print("\nğŸ“š Loading teacher model (fp16 storage, fp32 compute)...")
    from transformers import AutoModelForCausalLM
    teacher_model = AutoModelForCausalLM.from_pretrained(
        model.config._name_or_path,
        torch_dtype=torch.float16,  # â˜… å­˜å‚¨ç”¨fp16èŠ‚çœæ˜¾å­˜
        device_map="cpu"
    )
    teacher_model.eval()
    
    # ========== Step 2: è·å–å±‚ç»“æ„ ==========
    if "opt" in model_name:
        layers = model.model.decoder.layers
        teacher_layers = teacher_model.model.decoder.layers
        model.model.decoder.embed_tokens = model.model.decoder.embed_tokens.to(dev)
        model.model.decoder.final_layer_norm = model.model.decoder.final_layer_norm.to(dev)
        model.model.decoder.embed_positions = model.model.decoder.embed_positions.to(dev)
    else:
        layers = model.model.layers
        teacher_layers = teacher_model.model.layers
        model.model.embed_tokens = model.model.embed_tokens.to(dev)
        model.model.norm = model.model.norm.to(dev)
    
    # ========== Step 3: æ•è·ç¬¬ 0 å±‚è¾“å…¥ ==========
    layers[0] = layers[0].to(dev)
    
    dtype = next(iter(model.parameters())).dtype
    num_samples = len(calib_loader)
    
    # â˜… æ˜¾å­˜ä¼˜åŒ–: æ‰€æœ‰å¤§å‹ç¼“å­˜éƒ½æ”¾CPUï¼Œç”¨fp16å­˜å‚¨
    # Student è¾“å…¥ç¼“å­˜ (æ”¾CPUï¼)
    inps_student = torch.zeros(
        (num_samples, model.seqlen, model.config.hidden_size), 
        dtype=torch.float16, device='cpu'  # â˜… æ”¹ä¸ºCPU + fp16
    )
    # Teacher è¾“å…¥ç¼“å­˜ (fp16å­˜å‚¨èŠ‚çœæ˜¾å­˜)
    inps_teacher = torch.zeros(
        (num_samples, model.seqlen, model.config.hidden_size), 
        dtype=torch.float16, device='cpu'
    )
    
    cache = {'i': 0}
    attention_masks_list = []
    position_ids_list = []
    
    class StudentCatcher(nn.Module):
        def __init__(self, module):
            super().__init__()
            self.module = module
        def forward(self, inp, **kwargs):
            inps_student[cache['i']] = inp.detach().half().cpu()  # â˜… å­˜åˆ°CPU+fp16
            # ä¸ºæ¯ä¸ªæ ·æœ¬æ”¶é›† attention_mask å’Œ position_ids
            attention_masks_list.append(kwargs['attention_mask'].detach().cpu())  # â˜… ä¹Ÿå­˜CPU
            if "opt" not in model_name:
                pos_ids = kwargs.get('position_ids')
                position_ids_list.append(pos_ids.detach().cpu() if pos_ids is not None else None)
            cache['i'] += 1
            raise ValueError
    
    class TeacherCatcher(nn.Module):
        def __init__(self, module):
            super().__init__()
            self.module = module
            self.idx = 0
        def forward(self, inp, **kwargs):
            inps_teacher[self.idx] = inp.detach().half().cpu()  # â˜… å­˜å‚¨ä¸ºfp16
            self.idx += 1
            raise ValueError
    
    # æ•è·è¾“å…¥
    layers[0] = StudentCatcher(layers[0])
    teacher_layers[0] = TeacherCatcher(teacher_layers[0])
    
    print("ğŸ“Š Collecting layer-0 inputs...")
    for batch in calib_loader:
        try:
            batch = {k: v.to(dev) for k, v in batch.items()}
            model(**batch)
        except ValueError:
            pass
        
        try:
            teacher_model.to(dev)
            teacher_model(**batch)
        except ValueError:
            pass
        finally:
            # â˜… ä¿®å¤: ç¡®ä¿teacher_modelåœ¨æ¯æ¬¡batchåéƒ½ç§»å›CPUï¼Œé¿å…æ˜¾å­˜æ³„æ¼
            teacher_model.to("cpu")
    
    # æ¢å¤ç¬¬ 0 å±‚
    layers[0] = layers[0].module
    layers[0] = layers[0].cpu()
    teacher_layers[0] = teacher_layers[0].module
    
    # æ¸…ç†åµŒå…¥å±‚
    if "opt" in model_name:
        model.model.decoder.embed_tokens = model.model.decoder.embed_tokens.cpu()
        model.model.decoder.final_layer_norm = model.model.decoder.final_layer_norm.cpu()
        model.model.decoder.embed_positions = model.model.decoder.embed_positions.cpu()
    else:
        model.model.embed_tokens = model.model.embed_tokens.cpu()
        model.model.norm = model.model.norm.cpu()
    
    torch.cuda.empty_cache()
    
    # è¾“å‡ºç¼“å­˜ (â˜… å…¨éƒ¨æ”¾CPU)
    outs_student = torch.zeros_like(inps_student)  # CPU + fp16
    outs_teacher = torch.zeros(
        (num_samples, model.seqlen, model.config.hidden_size),
        dtype=torch.float16, device='cpu'  # â˜… å­˜å‚¨ä¸ºfp16
    )
    # å°†åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡æˆ–ä¿æŒä¸ºåˆ—è¡¨ï¼ˆæ¯ä¸ªå…ƒç´ å¯èƒ½å½¢çŠ¶ä¸åŒï¼‰
    attention_masks = attention_masks_list
    position_ids = position_ids_list if position_ids_list and position_ids_list[0] is not None else None
    
    # ========== Step 4: é€å±‚é—­ç¯å¤„ç† ==========
    compression_stats = []
    
    print("\nğŸ”§ Starting layer-wise compression with cumulative error tracking...")
    
    for layer_idx in tqdm(range(len(layers)), desc="SAES-SVD Compression"):
        layer_stats = {'layer': layer_idx, 'sublayers': {}}
        
        # å°†å½“å‰å±‚ç§»åˆ° GPU (â˜… è½¬fp32è®¡ç®—)
        layer = layers[layer_idx].to(dev).float()
        teacher_layer = teacher_layers[layer_idx].to(dev)  # teacherä¿æŒfp16
        
        subset = find_layers(layer)
        teacher_subset = find_layers(teacher_layer)
        
        # ---------- 4.1 ç»Ÿè®¡ H å’Œ Î” ----------
        # ä¸ºæ¯ä¸ªå­å±‚åˆå§‹åŒ–ç»Ÿè®¡é‡
        for name in subset:
            subset[name].H_matrix = 0
            subset[name].inp_cache = []
            teacher_subset[name].inp_cache = []
        
        # å®šä¹‰é’©å­
        def make_hook_student(module_name):
            def hook(module, input, output):
                inp = input[0].detach().float()
                if inp.dim() == 2:
                    inp = inp.unsqueeze(0)
                # ç´¯åŠ  H = X X^T
                H_batch = torch.matmul(inp.transpose(1, 2), inp)
                module.H_matrix = module.H_matrix + torch.sum(H_batch, dim=0)
                # ç¼“å­˜è¾“å…¥ç”¨äºè®¡ç®— Î”
                module.inp_cache.append(inp.half().cpu())  # â˜… å­˜å‚¨ä¸ºfp16
            return hook
        
        def make_hook_teacher(module_name):
            def hook(module, input, output):
                inp = input[0].detach()
                if inp.dim() == 2:
                    inp = inp.unsqueeze(0)
                module.inp_cache.append(inp.half().cpu())  # â˜… å­˜å‚¨ä¸ºfp16
            return hook
        
        # æ³¨å†Œé’©å­
        handles = []
        for name in subset:
            handles.append(subset[name].register_forward_hook(make_hook_student(name)))
            handles.append(teacher_subset[name].register_forward_hook(make_hook_teacher(name)))
        
        # å‰å‘ä¼ æ’­æ”¶é›†ç»Ÿè®¡é‡ï¼ˆç”¨å‹ç¼©å‰çš„layerï¼‰
        for j in range(num_samples):
            # â˜… ä»CPUåŠ è½½åˆ°GPUï¼Œè½¬fp32è®¡ç®—
            inp_j = inps_student[j].unsqueeze(0).to(dev).float()
            attn_mask_j = attention_masks[j].to(dev)
            pos_ids_j = position_ids[j].to(dev) if position_ids is not None else None
            
            if "opt" not in model_name:
                out_j = layer(inp_j, attention_mask=attn_mask_j, position_ids=pos_ids_j)[0]
                outs_student[j] = out_j.half().cpu()  # â˜… å­˜å›CPU fp16
                
                # Teacher å‰å‘ï¼ˆfp16åŠ è½½ï¼Œè½¬fp32è®¡ç®—ï¼‰
                teacher_inp = inps_teacher[j].unsqueeze(0).to(dev).float()
                teacher_out = teacher_layer.float()(
                    teacher_inp,
                    attention_mask=attn_mask_j,
                    position_ids=pos_ids_j
                )[0]
                outs_teacher[j] = teacher_out.half().cpu()
                teacher_layer.half()  # è½¬å›fp16
            else:
                out_j = layer(inp_j, attention_mask=attn_mask_j)[0]
                outs_student[j] = out_j.half().cpu()
                
                teacher_inp = inps_teacher[j].unsqueeze(0).to(dev).float()
                teacher_out = teacher_layer.float()(teacher_inp, attention_mask=attn_mask_j)[0]
                outs_teacher[j] = teacher_out.half().cpu()
                teacher_layer.half()
            
            # â˜… åŠæ—¶æ¸…ç†GPUä¸´æ—¶å˜é‡
            del inp_j, out_j, teacher_inp, teacher_out
            if j % 32 == 0:
                torch.cuda.empty_cache()
        
        # ç§»é™¤é’©å­
        for h in handles:
            h.remove()
        
        # è®¡ç®— Î” = (X^f - X) X^T
        for name in subset:
            subset[name].Delta_matrix = 0
            for idx in range(len(subset[name].inp_cache)):
                # â˜… ä»fp16åŠ è½½å¹¶è½¬fp32è®¡ç®—
                X = subset[name].inp_cache[idx].to(dev).float()  # Student è¾“å…¥
                X_f = teacher_subset[name].inp_cache[idx].to(dev).float()  # Teacher è¾“å…¥
                diff = X_f - X  # ç´¯ç§¯è¯¯å·®ï¼
                Delta_batch = torch.matmul(diff.transpose(1, 2), X)
                subset[name].Delta_matrix = subset[name].Delta_matrix + torch.sum(Delta_batch, dim=0)
                del X, X_f, diff, Delta_batch
            
            # ç§»åˆ° CPU é‡Šæ”¾æ˜¾å­˜
            subset[name].H_matrix = subset[name].H_matrix.cpu()
            subset[name].Delta_matrix = subset[name].Delta_matrix.cpu()
            del subset[name].inp_cache
            del teacher_subset[name].inp_cache
        
        torch.cuda.empty_cache()
        
        # ---------- 4.2 åˆ›å»º SVD æ›¿èº«æ¨¡å— ----------
        if "llama" in model_name or "vicuna" in model_name:
            svd_attn = SVD_LlamaAttention(config=model.config, ratio=ratio).to(dev)
            svd_mlp = SVD_LlamaMLP(
                hidden_size=layer.hidden_size,
                intermediate_size=model.config.intermediate_size,
                hidden_act=model.config.hidden_act,
                ratio=ratio
            ).to(dev)
        elif "mistral" in model_name:
            svd_attn = SVD_MistralAttention(config=model.config, ratio=ratio).to(dev)
            svd_mlp = SVD_MistralMLP(config=model.config, ratio=ratio).to(dev)
        elif 'opt' in model_name:
            svd_decoder = SVDOPTDecoderLayer(model.config, ratio=ratio).to(dev)
        
        # ---------- 4.3 é€ä¸ªå­å±‚å‹ç¼© ----------
        for name in subset:
            W = subset[name].weight.data.float().to(dev)
            original_dtype = subset[name].weight.data.dtype
            
            H = subset[name].H_matrix.float().to(dev)
            Delta = subset[name].Delta_matrix.float().to(dev)
            
            # è®¡ç®—æˆªæ–­ç§©
            m, n = W.shape
            num_s_after_trunc = int(m * n * ratio / (m + n))
            # â˜… ä¿®å¤: é˜²æ­¢rankä¸º0æˆ–è¶…è¿‡çŸ©é˜µç§©
            num_s_after_trunc = max(1, min(num_s_after_trunc, min(m, n)))
            
            # â˜… å…³é”®ï¼šè®¡ç®— L = H^{-1/2}ï¼Œåç»­ ACES å’Œå‹ç¼©éƒ½ç”¨è¿™ä¸ª L
            L = compute_H_inv_sqrt(H, dev, eps=eps)
            
            # â˜… ACES é€‰æ‹© Î²ï¼ˆä½¿ç”¨åŒä¸€ä¸ª Lï¼‰
            # â˜… è®ºæ–‡Section 5.3æ¨è: Î±âˆˆ[0.25, 0.75] â†’ Î²âˆˆ[0.2, 0.4286]
            beta, aces_info = aces_beta_select_unified(
                W, H, Delta, L, num_s_after_trunc,
                alpha_min=alpha_min, alpha_max=alpha_max, rho=rho
            )
            
            # â˜… æ„é€ ç™½åŒ–ç›®æ ‡çŸ©é˜µ G = W(H + Î²Î”)L
            G = W @ (H + beta * Delta) @ L
            
            # æˆªæ–­ SVD
            U, S, Vt = torch.linalg.svd(G, full_matrices=False)
            truc_u = U[:, :num_s_after_trunc]
            truc_s = S[:num_s_after_trunc]
            Vt_r = Vt[:num_s_after_trunc, :]
            
            truc_sigma = torch.diag(truc_s)
            sqrtSigma = torch.sqrt(truc_sigma)
            
            # è®¡ç®—ä½ç§©å› å­
            # A = U_r Î£^{1/2}
            # B = Î£^{1/2} V_r^T L
            # â˜… ä¿®å¤: å…ˆä¿æŒåœ¨devä¸Šï¼Œé¿å…è®¾å¤‡æ¼‚ç§»
            svd_u = torch.matmul(truc_u, sqrtSigma).to(original_dtype)
            svd_v = torch.matmul(sqrtSigma, Vt_r @ L).to(original_dtype)
            
            # è®¡ç®—é‡å»ºè¯¯å·®(æƒé‡ç©ºé—´ - ä»…ä¾›å‚è€ƒï¼Œä¸æ˜¯ä¼˜åŒ–ç›®æ ‡)
            W_reconstructed = svd_u.float() @ svd_v.float()
            weight_recon_error = (torch.norm(W - W_reconstructed) / torch.norm(W)).item()
            
            # â˜… è®¡ç®—æ›´æœ‰æ„ä¹‰çš„æŒ‡æ ‡: GçŸ©é˜µçš„truncation error
            G_reconstructed = truc_u @ torch.diag(truc_s) @ Vt_r
            G_trunc_error = (torch.norm(G - G_reconstructed) / torch.norm(G)).item()
            
            # â˜… è®¡ç®—ä¿ç•™èƒ½é‡æ¯” (retained energy ratio)
            total_energy = torch.sum(S**2).item()
            retained_energy = torch.sum(truc_s**2).item()
            energy_ratio = retained_energy / (total_energy + 1e-10)
            
            # è®¡ç®— Delta èŒƒæ•°ï¼ˆç”¨äºè¯Šæ–­ï¼‰
            delta_norm = torch.norm(Delta).item()
            h_norm = torch.norm(H).item()
            
            # è®°å½•ç»Ÿè®¡
            layer_stats['sublayers'][name] = {
                'beta': beta,
                'rank': num_s_after_trunc,
                'G_trunc_error': G_trunc_error,
                'energy_ratio': energy_ratio,
                'delta_norm': delta_norm,
                'h_norm': h_norm,
                'delta_ratio': delta_norm / (h_norm + 1e-10),
                'aces_info': aces_info
            }
            
            # æ‰“å°ä¿¡æ¯ï¼ŒæŒ‡æ ‡è¯´æ˜
            # > â”‚ Ïâ‚‘(Î²*)  â”‚ ç™½åŒ–ç©ºé—´çš„æˆªæ–­èƒ½é‡æ¯”ï¼ˆACESä¼˜åŒ–ç›®æ ‡ï¼‰
            # > â”‚ G_err   â”‚ ç™½åŒ–çŸ©é˜µGçš„æˆªæ–­è¯¯å·® ||G - G_r|| / ||G||
            # > â”‚ E_kept  â”‚ ä¿ç•™èƒ½é‡æ¯” Î£(Ïƒ_iÂ²) / Î£(Ïƒ_allÂ²) [iâ‰¤r]
            # > â”‚ Î”/H     â”‚ ç´¯ç§¯è¯¯å·®ä¸åæ–¹å·®çš„æ¯”å€¼ï¼ˆè¶Šå¤§è¶Šéœ€è¦è¯¯å·®è¡¥å¿ï¼‰
            # å…³äºÎ²çš„æŒ‡æ ‡è¯´æ˜
            # ç¬¦å·	        æ•°å­¦å®šä¹‰	      ç‰©ç†å«ä¹‰	          ä»£ç å˜é‡	                  å…¸å‹å€¼
            # Î±	æƒé‡å‚æ•°	ç´¯ç§¯è¯¯å·®è¡¥å¿ç³»æ•°	alpha_min,        alpha_max	                 0.25~0.75
            # Î²	             Î±/(1+Î±)	   è½¬æ¢åçš„å‚æ•°	        beta_min,beta_max	       0.2~0.4286
            # Î±*	       æœ€ä¼˜å‚æ•°	        ACESé€‰å‡ºçš„æœ€ä¼˜Î±	  aces_info['alpha_star']	    0.3~0.6
            # Î²*	       Î±*/(1+Î±*)	  ACESé€‰å‡ºçš„æœ€ä¼˜Î²	   beta (è¿”å›å€¼)	            0.25~0.38
            # Ïâ‚‘(Î²*)     	ç›®æ ‡å‡½æ•°å€¼	   ç™½åŒ–ç©ºé—´æˆªæ–­èƒ½é‡æ¯”	   aces_info['best_score']	  0.01~0.1
            # E_kept    	å®é™…èƒ½é‡æ¯”	    ä¿ç•™çš„èƒ½é‡ç™¾åˆ†æ¯”	   energy_ratio	              0.95~0.99
            if verbose and (layer_idx == 0 or layer_idx == len(layers) - 1 or layer_idx % 8 == 0):
                print(f"  Layer {layer_idx}, {name}: Î²={beta:.4f}, Î±*={aces_info['alpha_star']:.4f}, "
                      f"Ïâ‚‘(Î²*)={aces_info['best_score']:.6f}, rank={num_s_after_trunc}, "
                      f"G_err={G_trunc_error:.4f}, E_kept={energy_ratio:.4f}, Î”/H={delta_norm/h_norm:.4f}")
            
            # â˜… ç«‹åˆ»å†™å…¥ SVD æ›¿èº«æ¨¡å— (ä½¿ç”¨copy_é¿å…è®¾å¤‡æ¼‚ç§»)
            if 'opt' in model_name:
                if "q_proj" in name:
                    svd_decoder.self_attn.q_u_proj.weight.data.copy_(svd_u)
                    svd_decoder.self_attn.q_v_proj.weight.data.copy_(svd_v)
                    svd_decoder.self_attn.q_u_proj.bias.data.copy_(layer.self_attn.q_proj.bias.data)
                elif "k_proj" in name:
                    svd_decoder.self_attn.k_u_proj.weight.data.copy_(svd_u)
                    svd_decoder.self_attn.k_v_proj.weight.data.copy_(svd_v)
                    svd_decoder.self_attn.k_u_proj.bias.data.copy_(layer.self_attn.k_proj.bias.data)
                elif "v_proj" in name:
                    svd_decoder.self_attn.v_u_proj.weight.data.copy_(svd_u)
                    svd_decoder.self_attn.v_v_proj.weight.data.copy_(svd_v)
                    svd_decoder.self_attn.v_u_proj.bias.data.copy_(layer.self_attn.v_proj.bias.data)
                elif "out_proj" in name:
                    svd_decoder.self_attn.out_u_proj.weight.data.copy_(svd_u)
                    svd_decoder.self_attn.out_v_proj.weight.data.copy_(svd_v)
                    svd_decoder.self_attn.out_u_proj.bias.data.copy_(layer.self_attn.out_proj.bias.data)
                elif "fc1" in name:
                    svd_decoder.fc1_u_proj.weight.data.copy_(svd_u)
                    svd_decoder.fc1_v_proj.weight.data.copy_(svd_v)
                    svd_decoder.fc1_u_proj.bias.data.copy_(layer.fc1.bias.data)
                elif "fc2" in name:
                    svd_decoder.fc2_u_proj.weight.data.copy_(svd_u)
                    svd_decoder.fc2_v_proj.weight.data.copy_(svd_v)
                    svd_decoder.fc2_u_proj.bias.data.copy_(layer.fc2.bias.data)
                    svd_decoder.self_attn_layer_norm = layer.self_attn_layer_norm
                    svd_decoder.final_layer_norm = layer.final_layer_norm
            else:
                if "q_proj" in name:
                    svd_attn.q_u_proj.weight.data.copy_(svd_u)
                    svd_attn.q_v_proj.weight.data.copy_(svd_v)
                elif "k_proj" in name:
                    svd_attn.k_u_proj.weight.data.copy_(svd_u)
                    svd_attn.k_v_proj.weight.data.copy_(svd_v)
                elif "v_proj" in name:
                    svd_attn.v_u_proj.weight.data.copy_(svd_u)
                    svd_attn.v_v_proj.weight.data.copy_(svd_v)
                elif "o_proj" in name:
                    svd_attn.o_u_proj.weight.data.copy_(svd_u)
                    svd_attn.o_v_proj.weight.data.copy_(svd_v)
                elif "gate_proj" in name:
                    svd_mlp.gate_u_proj.weight.data.copy_(svd_u)
                    svd_mlp.gate_v_proj.weight.data.copy_(svd_v)
                elif "down_proj" in name:
                    svd_mlp.down_u_proj.weight.data.copy_(svd_u)
                    svd_mlp.down_v_proj.weight.data.copy_(svd_v)
                elif "up_proj" in name:
                    svd_mlp.up_u_proj.weight.data.copy_(svd_u)
                    svd_mlp.up_v_proj.weight.data.copy_(svd_v)
            
            # æ¸…ç†
            del W, H, Delta, L, G, U, S, Vt, svd_u, svd_v, W_reconstructed
            del subset[name].H_matrix, subset[name].Delta_matrix
            torch.cuda.empty_cache()
        
        # ---------- 4.4 â˜… ç«‹åˆ»æ›¿æ¢æœ¬å±‚æƒé‡ ----------
        if 'opt' in model_name:
            layers[layer_idx] = svd_decoder
        else:
            layer.self_attn = svd_attn
            layer.mlp = svd_mlp
        
        # è®¡ç®—å±‚çº§ç»Ÿè®¡
        avg_beta = sum(s['beta'] for s in layer_stats['sublayers'].values()) / len(layer_stats['sublayers'])
        avg_delta_ratio = sum(s['delta_ratio'] for s in layer_stats['sublayers'].values()) / len(layer_stats['sublayers'])
        layer_stats['avg_beta'] = avg_beta
        layer_stats['avg_delta_ratio'] = avg_delta_ratio
        
        # è®¡ç®—å±‚çº§å¹³å‡ alpha_star å’Œ best_score
        avg_alpha_star = sum(s['aces_info']['alpha_star'] for s in layer_stats['sublayers'].values()) / len(layer_stats['sublayers'])
        avg_best_score = sum(s['aces_info']['best_score'] for s in layer_stats['sublayers'].values()) / len(layer_stats['sublayers'])
        
        if verbose:
            print(f"Layer {layer_idx} Summary: avg_Î²={avg_beta:.4f}, avg_Î±*={avg_alpha_star:.4f}, "
                  f"avg_Ïâ‚‘={avg_best_score:.6f}, avg_Î”/H={avg_delta_ratio:.4f}")
        
        compression_stats.append(layer_stats)
        
        # ========== â˜…â˜…â˜… å…³é”®ä¿®å¤ï¼šé‡æ–°å‰å‘ä¼ æ’­è·å–å‹ç¼©åè¾“å‡º â˜…â˜…â˜… ==========
        # é—­ç¯è¯­ä¹‰ï¼šä¸‹ä¸€å±‚çš„è¾“å…¥ = å½“å‰å±‚ã€å‹ç¼©åã€‘çš„å®é™…è¾“å‡º
        # è¿™æ ·æ‰èƒ½æ­£ç¡®ç´¯ç§¯å‹ç¼©è¯¯å·®åˆ° Î” ä¸­
        for j in range(num_samples):
            inp_j = inps_student[j].unsqueeze(0).to(dev).float()  # â˜… CPU->GPU
            attn_mask_j = attention_masks[j].to(dev)
            pos_ids_j = position_ids[j].to(dev) if position_ids is not None else None
            
            if "opt" not in model_name:
                out_j = layer(inp_j, attention_mask=attn_mask_j, position_ids=pos_ids_j)[0]
            else:
                out_j = layers[layer_idx](inp_j, attention_mask=attn_mask_j)[0]
            
            outs_student[j] = out_j.half().cpu()  # â˜… å­˜å›CPU fp16
            del inp_j, out_j
        
        torch.cuda.empty_cache()
        
        # æ›´æ–°è¾“å…¥ä¸ºå½“å‰å±‚ã€å‹ç¼©åã€‘çš„è¾“å‡º
        inps_student = outs_student.clone()
        inps_teacher = outs_teacher.clone()  # Teacher è¾“å‡ºä¸å˜ï¼ˆå…¨ç²¾åº¦å‚è€ƒï¼‰
        
        # ç§»å› CPU (â˜… è½¬å›fp16èŠ‚çœå†…å­˜)
        layers[layer_idx] = layers[layer_idx].half().cpu()
        teacher_layers[layer_idx] = teacher_layers[layer_idx].cpu()
        torch.cuda.empty_cache()
    
    # æ¸…ç† teacher model
    del teacher_model
    torch.cuda.empty_cache()
    
    print("\n" + "=" * 60)
    print("âœ… SAES-SVD v2.1.3 compression complete!")
    print("=" * 60)
    
    # æ‰“å°æ€»ç»“
    all_betas = [s['avg_beta'] for s in compression_stats]
    all_delta_ratios = [s['avg_delta_ratio'] for s in compression_stats]
    print(f"ğŸ“Š Average Î² across all layers: {sum(all_betas)/len(all_betas):.4f}")
    print(f"ğŸ“Š Average Î”/H ratio: {sum(all_delta_ratios)/len(all_delta_ratios):.4f}")
    print(f"ğŸ“Š Î² range: [{min(all_betas):.4f}, {max(all_betas):.4f}]")
    
    return compression_stats


# ==================== Main ====================
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='SAES-SVD v2.1.3: Layer-wise Closed-loop Compression (Memory Optimized)')
    
    parser.add_argument('--model', type=str, required=True, help='Model path')
    parser.add_argument('--ratio', type=float, default=0.2, help='Compression ratio (0.2 = remove 20%)')
    parser.add_argument('--dataset', type=str, default='wikitext2', help='Calibration dataset')
    parser.add_argument('--nsamples', type=int, default=128, help='Number of calibration samples')
    parser.add_argument('--seed', type=int, default=0, help='Random seed')
    parser.add_argument('--DEV', type=str, default='cuda:0', help='Device')
    parser.add_argument('--save_path', type=str, default='./saes_svd_v2.1_output', help='Save path')
    parser.add_argument('--result_dir', type=str, default=None, help='Result directory')
    parser.add_argument('--model_seq_len', type=int, default=2048, help='Sequence length')
    parser.add_argument('--eps', type=float, default=1e-4, help='Regularization epsilon')
    parser.add_argument('--rho', type=float, default=0.95, help='ACES shrinkage factor (default: 0.95, v2.1.4 optimized)')
    parser.add_argument('--alpha_min', type=float, default=0.4, help='ACES alpha lower bound (default: 0.4, v2.1.4 optimized)')
    parser.add_argument('--alpha_max', type=float, default=0.6, help='ACES alpha upper bound (default: 0.6, v2.1.4 optimized)')
    parser.add_argument('--eval_only', action='store_true', help='Only run evaluation')
    parser.add_argument('--model_path', type=str, default=None, help='Path to compressed model for evaluation')
    
    args = parser.parse_args()

    # åœ¨ args è§£æåæ·»åŠ 
    if args.result_dir:
        args.save_path = args.result_dir
    
    # è½¬æ¢å‹ç¼©æ¯”
    keep_ratio = 1 - args.ratio  # 0.2 -> 0.8 (ä¿ç•™ 80%)
    
    print(f"=" * 60)
    print(f"SAES-SVD v2.1.4 Configuration (Experiment-Driven Optimization)")
    print(f"=" * 60)
    print(f"Model: {args.model}")
    print(f"Compression ratio: {args.ratio} (keeping {keep_ratio*100:.1f}%)")
    print(f"Dataset: {args.dataset}")
    print(f"Samples: {args.nsamples}")
    print(f"Device: {args.DEV}")
    print(f"Epsilon: {args.eps}")
    print(f"Alpha range: [{args.alpha_min}, {args.alpha_max}]")
    print(f"Rho: {args.rho}")
    print(f"=" * 60)
    
    if args.eval_only and args.model_path:
        # ä»…è¯„ä¼°æ¨¡å¼
        print("\nğŸ“Š Evaluation only mode...")
        model, tokenizer = get_model_from_local(args.model_path)
        model.eval()
        model = model.float()
        model = model.to(args.DEV)
        
        print("\nğŸ” Running perplexity evaluation...")
        ppl_eval(model, tokenizer, datasets=['wikitext2'], 
                 model_seq_len=args.model_seq_len, batch_size=4, device=args.DEV)
    else:
        # å‹ç¼©æ¨¡å¼
        print("\nğŸ“¦ Loading model...")
        model, tokenizer = get_model_from_huggingface(model_id=args.model)
        model = model.eval()
        # â˜… æ˜¾å­˜ä¼˜åŒ–: studentä¹Ÿç”¨fp16å­˜å‚¨ï¼Œè®¡ç®—æ—¶è½¬fp32
        model = model.half()  # fp16å­˜å‚¨
        
        print("\nğŸ“Š Loading calibration data...")
        calib_data = get_calib_train_data(
            args.dataset, tokenizer, args.nsamples, seqlen=args.model_seq_len
        )
        
        print("\nğŸ”§ Starting compression...")
        stats = saes_svd_compress_layerwise(
            args.model, model, calib_data, keep_ratio, args.DEV,
            eps=args.eps, rho=args.rho, alpha_min=args.alpha_min, alpha_max=args.alpha_max, verbose=True
        )
        
        # ä¿å­˜æ¨¡å‹
        if not os.path.exists(args.save_path):
            os.makedirs(args.save_path)
        
        save_file = os.path.join(
            args.save_path,
            f"{args.model.replace('/', '_').replace('-', '_')}_saes_svd_v2.1.4_{keep_ratio}.pt"
        )
        print(f"\nğŸ’¾ Saving compressed model to: {save_file}")
        # â˜… ä¿®å¤: ä¿å­˜å‰è½¬å›fp32ï¼Œé¿å…fp16ç²¾åº¦æŸå¤± (ä¸SVDLLMv2ä¿æŒä¸€è‡´)
        model = model.float()
        torch.save({'model': model, 'tokenizer': tokenizer, 'stats': stats}, save_file)
        
        # è¯„ä¼°
        print("\nğŸ” Running perplexity evaluation...")
        model = model.to(args.DEV)  # æ¨¡å‹å·²æ˜¯fp32ï¼Œç›´æ¥ç§»åˆ°GPU
        ppl_eval(model, tokenizer, datasets=['wikitext2'],
                 model_seq_len=args.model_seq_len, batch_size=4, device=args.DEV)
        
        print("\nâœ… All done!")
