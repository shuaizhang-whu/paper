# CorDA：Dynamic Context-oriented Decomposition for  Task-aware Low-rank Adaptation with Less  Forgetting and Faster Convergence

## 1）从基础目标开始：LoRA 的优化目标是什么、瓶颈在哪里

### 1.1 LoRA 形式化：把全参微调的更新限制为低秩

对任意线性层权重 $W\in\mathbb{R}^{d_{out}\times d_{in}}$，LoRA 写成
$$
W^\* = W + \Delta W,\qquad \Delta W = BA,
$$
微调目标等价于只优化 $A,B$：
$$
\min_{A,B}\ \mathbb{E}_{(x,y)\sim\mathcal{D}_{task}}\ \mathcal{L}\big(f(x; W+BA),y\big)
$$
LoRA 的常用初始化是：$A$ 随机（Kaiming/高斯），$B=0$，使得初始 $\Delta W=0$。这保证一开始不改变基座模型行为，但也带来两类问题：

- **初始化完全“任务无关”**：你把哪个任务喂进去都一样；因此学习效率和最终效果受限。
- **遗忘问题**：即便只训练少量参数，最终的 $BA$ 仍可能显著改变原有能力（尤其是世界知识/零样本能力）。

CorDA 的核心观点：**任务上下文应该决定“更新方向的子空间”**，而不是训练开始时随机猜一个低秩子空间。Yang 等 - 2025 - Dynamic context…

------

## 2）CorDA 的关键：用“上下文协方差”定向 SVD，把任务能力“压进”主成分

### 2.1 引入上下文：每层输入激活的协方差

对某层线性变换的输入激活 $X\in\mathbb{R}^{d_{in}\times (B\!L)}$（batch×seq 拉平），定义
$$
C = XX^\top \in \mathbb{R}^{d_{in}\times d_{in}}
$$
直觉：不同任务触发不同的激活“异常通道/outlier pattern”，这种结构会体现在 $C$ 里，因此 $C$ 是**任务上下文的统计摘要**。

### 2.2 不是分解 $W$，而是分解 $WC$：Context-Oriented SVD（CO-SVD）

CorDA 的“定向分解”是：
$$
\mathrm{SVD}(WC)=U\Sigma V^\top=\sum_{i=1}^{R}\sigma_i u_i v_i^\top
$$
与普通 SVD($W$) 的区别：右侧乘了 $C$，等价于让分解“朝着任务输入分布的主方向”旋转，从而使得**与该任务相关的方向更集中地出现在头部奇异分量**（heavy-head + long-tail 的谱形状）。

### 2.3 必须保证“初始行为不变”：乘回 $C^{-1}$ 做重构

如果只拿 $U\Sigma V^\top$ 直接替代，会改掉原模型。CorDA 用
$$
\widehat{W}=\mathrm{SVD}(WC)\,C^{-1}=U\Sigma(V^\top C^{-1})
$$
来保证 $\widehat{W}\approx W$（在 $C$ 可逆时可严格恢复；不可逆时用对角正则使其可逆）。这一步非常关键：它让你能“抽取分量来初始化 adapter”，但又不希望在训练开始就扰动推理输出。Yang 等 - 2025 - Dynamic context…

------

## 3）从“优化目标”到“两种模式”：KPM / IPM 的本质是在选择哪个子空间可训练

CorDA 的漂亮点在于：**同一个分解**，可以给两种不同目标服务。

### 3.1 目标 1：少遗忘（Knowledge-Preserved Mode, KPM）

目的：保留某些“原有知识/能力”（通常用 QA 数据采样来构造 $C$），让这些能力对应的主成分被冻结。

做法：让**尾部（最小的 $r$ 个）分量**作为可训练 adapter，而**头部（前 $R-r$ 个）冻结**：

初始化：
$$
B = U[:, -r:]\sqrt{\Sigma[-r:]},\quad
A = \sqrt{\Sigma[-r:]}(V^\top C^{-1})[-r:,:]
$$
并定义冻结的“残差权重”：
$$
W' = W - BA
$$
训练时只训练 $A,B$，$W'$ 冻结。训练后可合并：
$$
W^\* = W' + B^\*A^\*
$$
解释：头部主成分被认为更对齐于“要保留的知识检索能力”，因此冻结它们减少遗忘；尾部更像冗余/与该知识弱相关，可用于学习新任务而不太破坏原能力。

### 3.2 目标 2：更快收敛（Instruction-Previewed Mode, IPM）

目的：只追求新任务效果（不太在意保留旧知识），希望训练更快。

做法：把**头部（最大的 $r$ 个）分量**作为可训练 adapter：
$$
B = U[:, :r]\sqrt{\Sigma[:r]},\quad
A = \sqrt{\Sigma[:r]}(V^\top C^{-1})[:r,:]
$$
残差同样用 $W'=W-BA$ 冻结。

解释：头部主成分是“对当前任务输入分布最响应的方向”，让 adapter 一开始就站在这些方向上，相当于“预览了任务子空间”，自然更快收敛。Yang 等 - 2025 - Dynamic context…

------

## 4）CorDA++ 的理论升级：用“紧致度（compactness）”把随机性与固定 rank 两个痛点同时解决

CorDA 原版的两个问题：

1. **协方差采样有随机性**：用少量样本估计 $C$，不同抽样会导致某些层的 $C$ 不够代表性，初始化波动。
2. **所有层同一 rank**：但不同层对任务敏感度不同，固定 rank 浪费预算或压不住关键层。

CorDA++ 用一个“紧致度上界”推导出可排序的分数，然后做两种动态策略。

### 4.1 从“截断引起的输出漂移”出发，推导紧致度上界

截断底部 $r$ 个分量后，权重变化：
$$
\Delta W = W' - W
$$
输出漂移：
$$
\Delta Y = \Delta W X
$$
他们关心最大 token 的绝对漂移（$\|\Delta Y\|_1$ 定义成对 token 取最大、再对输出维求和）。通过范数不等式：
$$
\|\Delta Y\|_1 \le \sqrt{d_{out}}\|\Delta Y\|_2
\le \sqrt{d_{out}}\|\Delta W\|_2\|X\|_2
$$
注意 $\|X\|_2 = \sigma_{\max}(X)=\sqrt{\sigma_{\max}(C)}$。

而在 CO-SVD 的截断形式下，可以界到：
$$
\|\Delta W\|_2 \le \frac{\sigma_{-r}}{\sigma_{\min}(C)}
$$
其中 $\sigma_{-r}$ 是分解 $WC$ 时“从尾部数第 $r$ 个”的奇异值（也就是被截断那段的谱顶）。

于是得到核心上界：
$$
\|\Delta Y\|_1 \le
\underbrace{\left(\sqrt{d_{out}}\frac{\sigma_{\max}(C)}{\sigma_{\min}(C)}\right)}_{\pi(C)}
\cdot \sigma_{-r}
$$
定义：
$$
\pi(C)=\sqrt{d_{out}}\frac{\sigma_{\max}(C)}{\sigma_{\min}(C)}
$$
直觉：

- $\pi(C)$ 像是“上下文协方差的条件数惩罚”（越病态越糟）。
- $\sigma_{-r}$ 反映“尾部能量是否真的小”（尾巴越小，截断越安全）。
   两者一起刻画“任务能力是否被压进头部主成分”，也就是紧致度。Yang 等 - 2025 - Dynamic context…

### 4.2 动态协方差选择（Dynamic Covariance Selection）

做多轮采样得到候选集合 $\{C_i^{(l)}\}_{i=1}^N$（每层单独有候选），对每个候选计算分数：
$$
s(C_i^{(l)})=
\log(\pi(C_i^{(l)}))\cdot
\sum_{r=1}^{R}\frac{\sigma_{-r}}{\sigma_{\max}}
$$
然后选：
$$
C^{(l)}=\arg\min_i s(C_i^{(l)})
$$
解释：

- $\sum \sigma_{-r}/\sigma_{\max}$ 是“尾部总体占比”，越小表示谱能量越集中在头部。
- 乘上 $\log(\pi(C))$ 抑制病态协方差的风险。
   最终每层拿到“最能把任务能力压进主成分”的协方差估计，减少随机性。

### 4.3 动态 rank 分配（Dynamic Rank Allocation）：逐步过滤直到满足参数预算

他们把固定 rank 改成“按层逐个过滤分量”，直到满足总参数预算 $\tau$。

维护每层截断位置 $r^{(l)}$（初始为 1），定义每层当前“再多过滤一个分量”的得分：
$$
s^{(l)}=\log(\pi(C^{(l)}))\cdot
\frac{\sigma_{-r^{(l)}}}{\sum_{k=1}^{R^{(l)}-r^{(l)}}\sigma_k}
$$
每次选择最小的层 $l_0=\arg\min_l s^{(l)}$，更新：
$$
r^{(l_0)}\leftarrow r^{(l_0)}+1
$$
直到预算满足：

- KPM：adapter 用“被过滤出来的尾部分量”，参数量近似
  $$
  \tau'=\sum_l (d_{in}^{(l)}+d_{out}^{(l)})\cdot r^{(l)}
  $$
  当 $\tau'>\tau$ 停止。

- IPM：adapter 用“剩下的头部分量”，参数量近似
  $$
  \tau'=\sum_l (d_{in}^{(l)}+d_{out}^{(l)})\cdot (R^{(l)}-r^{(l)})
  $$
  当 $\tau'<\tau$ 停止。

直觉：每次都从“最不重要、最尾部、最安全截断”的层先扣掉一个 rank，让预算花在更敏感的层上。Yang 等 - 2025 - Dynamic context…

------

## 5）Step 实现过程（按工程流水线写清楚）

下面按你更关心的“可落地 step”组织（与文中 Algorithm 1/2 对齐）：

### Step 0：确定模式与采样数据来源

- KPM：采样来自“你要保留的知识域”（典型 QA 问题集合）。
- IPM：采样来自“你要学的新任务训练集”（指令+响应）。

### Step 1：跑一遍基座模型，收集每层输入激活 $X^{(l)}$，构造协方差

$$
C^{(l)} = X^{(l)}(X^{(l)})^\top
$$

若做动态协方差选择：重复采样 $N$ 轮得到 $\{C_i^{(l)}\}$。

### Step 2：对每层候选 $C_i^{(l)}$ 计算紧致度分数并选最优 $C^{(l)}$

- 先对 $C_i^{(l)}$ 做 SVD 得到 $\sigma_{\max},\sigma_{\min}$ 以算 $\pi(C)$；
- 再对 $W^{(l)}C_i^{(l)}$ 做 SVD 得到奇异值谱 $\{\sigma\}$；
- 算 $s(C_i^{(l)})$，选最小者。

### Step 3：对每层做 CO-SVD 并重构（保证初始行为不变）

$$
\mathrm{SVD}(W^{(l)}C^{(l)})=U^{(l)}\Sigma^{(l)}(V^{(l)})^\top
$$

若 $C^{(l)}$ 不可逆：做对角正则直到可逆（工程上就是不断增大 $\lambda I$）。

### Step 4：确定每层 rank（固定或动态分配）

- CorDA：给定固定 $r$。
- CorDA++：运行动态 rank 分配的“逐步过滤”循环，得到每层 $r^{(l)}$。

### Step 5：按模式初始化 adapter，并构造冻结残差权重

- KPM（尾部作为 adapter）：
  $$
  B^{(l)} = U^{(l)}[:, -r^{(l)}:]\sqrt{\Sigma^{(l)}[-r^{(l)}:]},\quad
  A^{(l)} = \sqrt{\Sigma^{(l)}[-r^{(l)}:]}\big((V^{(l)})^\top (C^{(l)})^{-1}\big)[-r^{(l)}:,:]
  $$

- IPM（头部作为 adapter）：
  $$
  B^{(l)} = U^{(l)}[:, :r^{(l)}]\sqrt{\Sigma^{(l)}[:r^{(l)}]},\quad
  A^{(l)} = \sqrt{\Sigma^{(l)}[:r^{(l)}]}\big((V^{(l)})^\top (C^{(l)})^{-1}\big)[:r^{(l)},:]
  $$

- 冻结残差：
  $$
  W'^{(l)}=W^{(l)}-B^{(l)}A^{(l)}
  $$

### Step 6：训练

训练时只更新 $A^{(l)},B^{(l)}$，$W'^{(l)}$ 全冻结。训练结束合并回去，推理无额外结构开销。

------

## 6）一些可写“新论文”的 idea（只给方向与公式骨架）

下面给一些你后面要写“新 SVD 论文”可能用得上的点，尽量都落到“可写成方法节”的公式骨架上：

### Idea A：把“上下文协方差定向”从 $WC$ 推广为更一般的双侧度量（更强的任务对齐）

现在是右乘 $C$。可以考虑引入输出侧统计 $D = YY^\top$ 或者梯度侧统计，做双侧定向：
$$
\mathrm{SVD}\big(D^{1/2} W C^{1/2}\big)
$$
或更一般的广义 SVD / 加权低秩：
$$
\min_{\mathrm{rank}(X)\le r}\ \|D^{1/2}(W-X)C^{1/2}\|_F^2
$$
这样可以同时对齐“输入上下文”和“输出重要通道”。

### Idea B：把 CorDA++ 的紧致度上界做成可微的“正则项”，训练时持续压缩谱尾

它现在只用于“选 $C$”与“分配 rank”。你可以把紧致度写成一个正则项并在 PEFT 训练时最小化：
$$
\Omega^{(l)} = \log(\pi(C^{(l)}))\cdot \sum_{i>r^{(l)}} \frac{\sigma_i(W^{(l)}C^{(l)})}{\sigma_{\max}}
$$
含义：不仅初始化时“把任务能量压进头部”，训练过程中也维持“谱集中”，利于更小 rank、也可能更抗遗忘。

### Idea C：协方差不是单一任务的——做“上下文混合”以同时保留多域知识（很适合接 SAES-SVD 的“抑制误差/干扰”叙事）

给多个知识域/能力域 $\{C_k\}$，做凸组合：
$$
C_{\text{mix}}^{(l)}=\sum_{k=1}^K \alpha_k C_k^{(l)},\quad \alpha_k\ge 0,\ \sum_k \alpha_k=1
$$
再用紧致度或下游验证来选 $\alpha$（甚至层级 $\alpha^{(l)}$）。这能把“保留哪些知识”变成显式可控的参数。

### Idea D：把“动态 rank 分配”升级为“token/阶段动态 rank”（与生成误差累积相呼应）

CorDA++ 的 rank 是层级静态的。可引入解码步 $t$ 的预算 $R(t)$：
$$
r^{(l)}(t)=\mathrm{Alloc}\big(s^{(l)}, R(t)\big),\quad R(t+1)\le R(t)
$$
或者用不确定性（熵）驱动：
$$
R(t)=R_{\max}\cdot \phi(H_t),\quad
H_t=-\sum_v p(v|x_{\le t})\log p(v|x_{\le t})
$$
这样可以把“少遗忘/稳态能力”与“生成早期更保真”的逻辑统一到一个动态计算预算框架里。