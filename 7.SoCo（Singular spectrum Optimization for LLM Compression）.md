# SoCo（Optimizing Singular Spectrum for Large Language Model Compression）

这篇论文的核心观点是：**“奇异值大小”只保证在 Frobenius 低秩逼近意义下的重要性排序，并不一定与下游任务性能相关**。因此它不再“按奇异值从大到小截断”，而是引入**可学习的奇异谱重要性分数**，在数据驱动下重新评估每个 SVD 分量的保留/裁剪，并允许对保留分量做“放大补偿”，从而在高压缩率下显著减缓性能崩塌。

------

## 1) 从基线目标到 SoCo 目标：优化对象如何变化

### 1.1 基线：SVD 截断的“固定重要性排序”

对某个权重矩阵 $W\in\mathbb{R}^{m\times n}$：
$$
W = U\Sigma V^\top,\quad \Sigma=\mathrm{diag}(\sigma_1,\dots,\sigma_D),\ D=\min(m,n)
$$
传统截断压缩是保留前 $k$ 个奇异值：
$$
W_k = U_k\Sigma_kV_k^\top
$$
它隐含的目标（权重空间低秩逼近）是：
$$
\min_{\mathrm{rank}(\hat W)\le k}\ \|W-\hat W\|_F^2
$$
问题在于：**$\sigma_i$ 的排序对应的是“逼近误差最小”，而不是“任务损失最小”**，尤其在高压缩率时，按 $\sigma$ 截断会很快进入灾难区。

------

### 1.2 SoCo：把“奇异值=重要性”改为“可学习分数=重要性”

SoCo 冻结 $U,\Sigma,V$，在奇异谱上引入一个**可学习对角分数矩阵**（或向量）$S=\mathrm{diag}(s_1,\dots,s_D)$，对奇异值逐分量重标定：
$$
W' = U(\Sigma \odot S)V^\top
\tag{3}
$$
其中 $\odot$ 是逐元素乘法（等价于 $\sigma_i \leftarrow \sigma_i s_i$）。

这一步把“固定谱截断”升级成“**学习谱形状（learnable spectrum）**”：

- 分数低的分量可以被裁剪
- 分数高的分量不仅被保留，还可能被放大（$s_i>1$）以补偿被裁剪分量带来的损失

------

## 2) 分数函数设计：为何能稳定训练且支持“放大补偿”

### 2.1 分数的参数化（scaled sigmoid）

每层学习一个对角参数 $z=\mathrm{diag}(z_1,\dots,z_D)$，通过如下函数得到分数：
$$
s_i \;=\; \frac{\lambda_m}{1+\exp\big(-\lambda_s z_i + \ln(2\lambda_m-1)\big)}
\tag{4}
$$
关键性质（很重要）：

- **阈值对齐**：当 $z_i=0$ 时，
  $$
  s_i=\frac{\lambda_m}{1+(2\lambda_m-1)}=\frac{1}{2}
  $$
  这让用固定阈值（比如 0.5）做选择非常自然。

- **可放大**：$s_i\in(0,\lambda_m)$。例如 $\lambda_m=2$ 时，$s_i$ 可到 2，从而允许“保留分量放大补偿”。

------

### 2.2 偏置补偿项 $d$（deviation term）

仅靠 $W'$ 的谱重标定仍可能引入系统性偏差，因此 SoCo 还引入一个可训练的偏置/偏移项 $d$（通常按输出通道维度），用于在线性变换后补偿：
$$
y' = xW' + d
$$
训练时冻结原模型所有权重，仅训练 $\{z\}$ 与 $\{d\}$，因此训练代价较低。

------

## 3) “压缩率”作为可优化量：从硬选择到可反传近似

SoCo 的训练过程需要把“保留了多少分量”变成可导量。

### 3.1 用阈值把分量选择变成计数 $c_l$

对第 $l$ 层的第 $d$ 个分量，分数为 $s_l^d$。定义保留分量数：
$$
c_l=\sum_{d=1}^{D}\mathbf{1}(s_l^d\ge 0.5)
$$
但 $\mathbf{1}(\cdot)$ 不可导，于是用 STE（Straight-Through Estimator）近似：
$$
c_l=\sum_{d=1}^{D}\Big(\mathbf{1}(s_l^d\ge 0.5)-\mathrm{sg}[s_l^d]+s_l^d\Big)
\tag{6}
$$
其中 $\mathrm{sg}[\cdot]$ 停止梯度。直观效果：前向仍是硬阈值计数，反向用 $s_l^d$ 传梯度。

### 3.2 全模型压缩率（参数比例）$r$

对线性层（输入维 $in_l$、输出维 $out_l$），rank-$c_l$ 分解后的参数量近似为 $(in_l+out_l)c_l$。SoCo 用下面的全局比例作为压缩率（也用作压缩损失）：
$$
r \;=\; \frac{\sum_{l=1}^{L}(in_l+out_l)c_l}{\sum_{l=1}^{L}(in_l\cdot out_l)}
\tag{5}
$$
目标是让 $r$ 达到一个指定的目标比例 $R<1$（等价于达到某个压缩率）。

------

## 4) 三阶段训练：从“快速达标”到“充分再评估”再到“强稀疏极化”

SoCo 的关键不是“有个可学习分数”就完事，而是：**如果直接单阶段优化，分数会连续分布、阈值附近堆积，导致剪掉一堆“边界但仍重要”的分量，跨层累积误差很大**。因此它设计了三阶段训练来“先达标、再摇摆评估、最后极化”。

------

### 4.1 Stage 1：快速粗压缩（Rapid & Rough Compression）

Stage 1 的目标：**尽快把 $r$ 从 1 拉到目标 $R$**，同时尽量维持输出一致性。

- **压缩损失（推动更小的 $r$）**：
  $$
  L_{\text{dec}} = r
  $$

- **性能保持损失：输出分布对齐（KL）**
  $$
  L_{\text{inc}} = \frac{1}{T}\sum_{t=1}^{T}\sum_{c=1}^{C} y_t^c \log\frac{y_t^c}{p_t^c}
  \tag{7}
  $$
  其中 $y_t$ 是原模型（未压缩）预测分布，$p_t$ 是当前压缩模型预测分布。

Stage 1 优化：
$$
\min\ L_{\text{inc}} + L_{\text{dec}}
$$
但训练早期 $L_{\text{dec}}$ 的梯度更强，主导快速降 $r$。当 $r 时进入 Stage 2。

------

### 4.2 Stage 2：交替优化（Alternating Optimization）做“边界分量再评估”

Stage 1 后，很多分数仍然是“连续分布”，阈值不好切。Stage 2 的做法是让压缩率在目标附近**受控振荡**：

- 若当前 $r < R$：关闭 $L_{\text{dec}}$，只用 $L_{\text{inc}}$ 让模型“恢复”一些确实重要但被压得太狠的分量
- 若当前 $r \ge R$：重新启用 $L_{\text{dec}}$，继续压回目标

即：
$$
\text{if } r<R:\ \min L_{\text{inc}}
\qquad
\text{else } \min (L_{\text{inc}}+L_{\text{dec}})
$$
直观上，这相当于给“阈值附近的分量”更多机会在数据下被反复检验，最终分数更稳定、更可分。

------

### 4.3 Stage 3：重要性稀疏化（Importance Sparsity）把分数推向两极

Stage 3 引入稀疏化损失 $L_{\text{spa}}$，让分数靠近 0 或 1，并刻意允许一部分分数 **超过 1** 用于补偿：
$$
L_{\text{spa}}=\frac{1}{LD}\sum_{l=1}^{L}\sum_{d=1}^{D}\mathrm{spa}_l^d
\tag{8}
$$
其中二次惩罚为：
$$
\mathrm{spa}_l^d=
\begin{cases}
(s_l^d)^2,& s_l^d\le 0.5\\
(s_l^d-1)^2,& 0.5<s_l^d\le 1\\
0,& s_l^d>1
\end{cases}
\tag{9}
$$
Stage 3 的优化目标：
$$
\min\ L_{\text{inc}} + L_{\text{spa}}
$$
关键效果：

- $s\le 0.5$ 被推向 0（更明确可剪）
- $0.5 被推向 1（更明确保留）
- $s>1$ 不再被惩罚（允许“放大补偿”机制出现）

------

## 5) 训练完成后的“真正压缩”：按分数剪枝 + 重构低秩形式

训练阶段模型仍可用 $W'=U(\Sigma\odot S)V^\top$ 表示，但真正落地压缩需要把低分分量彻底删掉。

### 5.1 按阈值选择要保留的分量索引

$$
\mathcal{I}_l = \{d\mid s_l^d\ge 0.5\}
$$

然后取子矩阵：

- $U_l \leftarrow U_l[:,\mathcal{I}_l]$
- $V_l \leftarrow V_l[\mathcal{I}_l,:]$（或 $V_l^\top$ 取对应行）
- $\Sigma_l \leftarrow \Sigma_l[\mathcal{I}_l]$
- $S_l \leftarrow s_l[\mathcal{I}_l]$

### 5.2 保留分量的“可学习重标定”并进入低秩存储

最终该层压缩权重为：
$$
\widehat W_l = U_{l,k}\ \mathrm{diag}(\sigma_{l,k}\odot s_{l,k})\ V_{l,k}^\top
$$
若要部署成两层线性，可吸收平方根：
$$
\widehat W_l = (U_{l,k}\mathrm{diag}(\sqrt{\sigma_{l,k}\odot s_{l,k}}))\cdot
(\mathrm{diag}(\sqrt{\sigma_{l,k}\odot s_{l,k}})V_{l,k}^\top)
$$
这样参数量就是 $(in_l+out_l)\cdot k$（外加可选偏置 $d$）。

------

## 6) SoCo 的实现流程（Step-by-step，训练自由）

下面给一个从工程到算法都清晰的 pipeline（你按这个写代码就能复现核心机制）。

### Step 0：准备校准/训练数据

- 用一小段语料作为“谱学习”的训练集（论文用 WikiText-2 等）
- 训练只需要前向得到 logits/softmax，不需要反传到原模型参数

### Step 1：对每个待压缩矩阵做 SVD 并冻结分解

对每层每个线性模块权重 $W_l$：

1. 计算 $U_l,\Sigma_l,V_l$
2. 缓存并冻结

### Step 2：为每层创建可训练谱参数与偏置补偿

- 初始化 $z_l=0 \Rightarrow s_l=0.5$
- 可训练参数：$\{z_l\}$ 与 $\{d_l\}$

### Step 3：前向构造压缩权重并计算压缩模型输出

$$
s_l = \frac{\lambda_m}{1+\exp(-\lambda_s z_l+\ln(2\lambda_m-1))}
$$

同时跑原模型得到 $y$。

### Step 4：计算压缩率 $r$ 与三种损失

- 用 STE 得到各层 $c_l$，计算 $r$
- 计算 $L_{\text{inc}}$（KL 对齐）
- 计算 $L_{\text{dec}}=r$
- Stage 3 再加 $L_{\text{spa}}$

### Step 5：按三阶段规则更新

- Stage 1：优化 $L_{\text{inc}}+L_{\text{dec}}$，直到 $r
- Stage 2：根据 $r$ 相对 $R$ 开关 $L_{\text{dec}}$，反复震荡评估
- Stage 3：优化 $L_{\text{inc}}+L_{\text{spa}}$，把分数推向 0/1 并允许 >1 放大

### Step 6：训练结束后执行真正剪枝与权重落地

- 按阈值选索引 $\mathcal{I}_l$
- 构造低秩两层线性替换原线性层
- 保留并部署偏置补偿 $d_l$

------

## 7) 一些可延展的 Ideas（不展开“结合建议”，只给方向）

### Idea 1：阈值从固定 0.5 改成“可学习/可自适应阈值”

当前用固定阈值会让不同层、不同矩阵共享一个切点。可以设每层阈值 $\tau_l$ 可学习或由目标 $c_l$ 动态决定：
$$
c_l = \sum_d \mathbf{1}(s_l^d\ge \tau_l)
$$
并把 $\tau_l$ 纳入预算约束优化。

### Idea 2：把分数从“对角”推广到“分块/按 head 的结构化分数”

对注意力投影矩阵（尤其多头结构），可让分数在 head 级别共享，形成 block-diagonal 重要性：
$$
S=\mathrm{blockdiag}(S^{(1)},\dots,S^{(h)})
$$
这样既更可解释，也更利于硬件友好的结构化剪枝。

### Idea 3：用“排序一致性正则”鼓励分数与任务重要性一致，而不是完全自由

完全自由的 $S$ 可能产生不稳定的重排序。可以加一个温和正则，让分数与原奇异值排序保持一定单调性但允许局部交换（类似 isotonic / soft ranking）：
$$
\sum_{i<j}\max(0,\ (s_j-s_i)\cdot(\sigma_i-\sigma_j) - \delta)
$$

### Idea 4：把 $L_{\text{inc}}$ 从 logits-KL 扩展到“中间层对齐”或“多点蒸馏”

仅对齐输出分布可能对一些任务不够。可加入少量中间层 representation 对齐（仍冻结原模型）：
$$
\sum_{\ell\in\mathcal{S}}\|\phi_\ell - \phi'_\ell\|_2^2
$$
以减少跨层累积误差。

### Idea 5：让“放大补偿”更可控：对 $s>1$ 加弱约束或分层上限

Stage 3 对 $s>1$ 直接零梯度，虽然能补偿，但可能导致少数分量过度放大。可加一个非常弱的上界正则：
$$
\eta \sum_{i}\max(0, s_i-s_{\max})^2
$$
让补偿更平滑、避免极端值。