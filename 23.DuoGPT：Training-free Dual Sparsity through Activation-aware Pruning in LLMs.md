# DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in LLMs

**DuoGPT：Training-free Dual Sparsity through Activation-aware Pruning in LLMs**（核心：把“运行时激活稀疏”当作“动态结构化权重稀疏”，并在 OBC/SparseGPT 框架里做“激活稀疏感知的闭式校准 + 高效 GPU 实现”）。Yin 等 - 2025 - DuoGPT Training-…

------

## 1）问题设定：把解码阶段的 GEMV 变成 spMspV（双稀疏）

LLM 解码阶段（single-batch decoding）线性层典型是 GEMV：
 给定一层权重 $W\in\mathbb{R}^{n\times k}$，输入向量 $x\in\mathbb{R}^{k}$，输出
$$
y = Wx.
$$

### 1.1 激活稀疏的等价视角：动态“结构化行选择”

若运行时激活存在掩码 $m_x\in\{0,1\}^k$，输入变成 $x\odot m_x$，则
$$
y = W(x\odot m_x)=\sum_{i:m_{x,i}=1} W_{:,i}\,x_i.
$$
这等价于：只需要访问与非零激活对应的权重“列/通道”（在 GEMV 视角里相当于只计算部分乘加）。但**仅靠激活稀疏并不能减少 HBM 中模型存储**：因为你事先不知道哪些通道会被选中，所以仍得存全量权重。

### 1.2 DuoGPT 的目标：同时引入权重稀疏 + 激活稀疏（Dual-sparsity）

离线把权重做成稀疏（静态），运行时再叠加激活稀疏（动态），使实际执行更像 “稀疏矩阵 × 稀疏向量”（spMspV）：

- HBM：只存压缩后的稀疏权重（模型尺寸下降）
- SRAM/算力：只加载被激活选中的权重行/块，并且这些行本身是稀疏格式（进一步减少 load + compute）

关键难点：**校准（calibration）时如何把“激活稀疏导致的信息损失”纳入 OBC 闭式更新**，否则精度会崩。

------

## 2）原始优化目标：OBC / SparseGPT 的逐元素剪枝闭式解

按 OBC 传统做法，对每一行权重 $w\in\mathbb{R}^{1\times k}$（输出通道独立处理），给定校准输入矩阵 $X\in\mathbb{R}^{k\times m}$（$m$ 是 token 数），原输出为 $wX$。

### 2.1 SparseGPT 的行级目标（等价于最小化输出误差）

要剪掉第 $p$ 个权重 $w_p$，并对剩余权重做补偿。令更新量 $\Delta w$，约束“剪掉该元素后为 0”：
$$
\Delta w\,e_p^\top + w_p = 0,
$$
其中 $e_p$ 是第 $p$ 个单位向量。目标是让剪枝前后该行输出尽量一致：
$$
\min_{\Delta w}\ \|(\Delta w+w)X - wX\|_F^2
= \min_{\Delta w}\ \|\Delta w X\|_F^2
\quad \text{s.t. }\Delta w e_p^\top + w_p=0.
$$
令
$$
H = XX^\top \in \mathbb{R}^{k\times k},\quad H^{-1}=(XX^\top)^{-1}
$$
（在 OBC 里它扮演“近似 Hessian inverse”的角色），则 SparseGPT 的经典闭式结果是：

- 剪掉 $p$ 时造成的最小损失：

$$
L_p^{\text{SparseGPT}}=\frac{w_p^2}{H^{-1}_{pp}}.
$$

- 对剩余权重的最优补偿更新：

$$
\Delta w^{\text{SparseGPT}}
= -\frac{w_p}{H^{-1}_{pp}}\; H^{-1}_{p,:}.
$$

SparseGPT 的“逐列/分块 + 同步 Hessian”工程化，是为了让全行并行共享同一个 $H^{-1}$（否则每行单独维护会爆显存与算力）。

------

## 3）DuoGPT 的改进目标：把“激活稀疏”引入校准，并用 dense 残差补信息

DuoGPT 关键想法：校准时如果直接把输入变成稀疏 $\hat X$（模拟运行时激活稀疏），虽然更贴近推理，但信息更少，导致补偿不准；于是用 **asymmetric calibration**：用“稀疏输入驱动被剪枝模型”，但用“密集模型产生的目标输出”来保真。

### 3.1 稀疏校准输入与 dense 参考输入

- 令 $X$ 是来自“前一层已剪枝模型”的输出（校准过程中逐层进行）
- 对 $X$ 做幅值剪枝得到稀疏输入 $\hat X$，每列稀疏率为 $p_x$
- 令 $\tilde X$ 是原始 dense 模型在同位置产生的密集输入（或等价的 dense 参考）

### 3.2 改进后的行级校准目标（asymmetric）

希望剪枝后的权重（含补偿）在稀疏输入 $\hat X$ 上，输出逼近 dense 参考输出 $w\tilde X$：
$$
\min_{\Delta w}\ \|(\Delta w+w)\hat X - w\tilde X\|_F^2
\quad \text{s.t. }\Delta w e_p^\top + w_p=0.
$$
把 “dense vs sparse 输入差异”吸收到一个残差项。定义
$$
\Delta X = \tilde X - \hat X,
\quad r = w(\tilde X-\hat X)=w\Delta X \in \mathbb{R}^{1\times m}.
$$
则目标等价为
$$
\min_{\Delta w}\ \|\Delta w \hat X - r\|_F^2
\quad \text{s.t. }\Delta w e_p^\top + w_p=0.
$$
注意这一步非常关键：它把问题从 “拟合 dense 输出” 转成 “用稀疏输入 $\hat X$ 去拟合一个可计算的残差 $r$”——于是仍能写出闭式解。

------

## 4）从改进目标到闭式解：DuoGPT 的最优损失与补偿公式（核心推导结果）

定义
$$
H = \hat X\hat X^\top,\quad H^{-1}=(\hat X\hat X^\top)^{-1}.
$$
对 Lagrangian：
$$
\mathcal{L}(\Delta w,\lambda)=\|\Delta w\hat X-r\|_F^2+\lambda(\Delta w e_p^\top+w_p).
$$
对 $\Delta w$ 求导置零会得到：
$$
2\Delta w H - 2r\hat X^\top + \lambda e_p = 0
\;\Rightarrow\;
\Delta w = r\hat X^\top H^{-1} - \frac{\lambda}{2}H^{-1}_{p,:}.
$$
结合约束 $\Delta w e_p^\top + w_p=0$ 求出 $\lambda$，最终得到 DuoGPT 的更新形式（论文给出的等价表达）：

### 4.1 DuoGPT 的最优补偿更新

$$
\Delta w^{\text{Duo}}
= -\frac{w_p}{H^{-1}_{pp}}\,H^{-1}_{p,:}
\;+\;
r\hat X^\top H^{-1}_{-p}.
$$

这里 $H^{-1}_{-p}$ 表示对第 $p$ 个变量做一次“高斯消元式的删除”（等价于把第 $p$ 行列消去后的剩余逆矩阵块），直观上：第二项是“用 dense-sparse 残差信息给补偿加一个校正”。

你也可以把它理解成：

- 第一项 = SparseGPT 原本的 OBC 最优补偿
- 第二项 = 为了对抗激活稀疏信息缺失而引入的“残差驱动修正项”

### 4.2 DuoGPT 的剪枝损失（用于决定剪哪个权重）

论文推导出的损失形式（重排后）可写为：
$$
L_p^{\text{Duo}}
=
\frac{w_p^2}{H^{-1}_{pp}}
\;+\;
rr^\top
\;-\;
r\hat X^\top H^{-1}_{-p}\hat X r^\top
\;+\;
\frac{2w_p}{H^{-1}_{pp}}\; r\hat X^\top H^{-1}_{:,p}.
$$
这比 SparseGPT 多了三块与 $r$ 有关的项，分别对应：

- 激活稀疏造成的残差能量（$rr^\top$）
- 删除第 $p$ 个变量后，稀疏输入对残差的可解释部分（负号项，起“修正/抵消”作用）
- 权重重要性与残差的互相关（最后的交叉项）

于是选择要剪的 $p$：
$$
p^\star = \arg\min_p\ L_p^{\text{Duo}}.
$$

------

## 5）工程瓶颈与 DuoGPT 的第二个核心：把“不可行的闭式迭代”改写成可向量化的打分 + Cholesky 预计算

上面的 $L_p^{\text{Duo}}$ 若按“最优顺序逐元素”计算，会出现两个不可承受点：

1. 每次剪一个权重都要更新 $H^{-1}$（而且如果按行独立，会非常大）
2. 每个候选 $p$ 都要算一次复杂的 $r\hat X^\top H^{-1}_{-p}\hat X r^\top$

因此 DuoGPT 沿用 SparseGPT 的关键工程策略：
 **同步 Hessian + 固定列顺序处理（act_order/blocking/lazy update）**，让所有行共享同一个 $H^{-1}$，并把 $H^{-1}$ 的更新转为一次 Cholesky/逆 Cholesky 的预计算。

### 5.1 用 Cholesky 表示 $H^{-1}$

令
$$
H^{-1}=LL^\top
$$
其中 $L$ 是下三角（实现里常说 “Inverse_Cholesky(H)” 得到 $L$）。

SparseGPT 里有一个非常实用的恒等式：
$$
\frac{H^{-1}_{:,p}}{H^{-1}_{pp}} = \frac{L_{:,p}}{L_{pp}}.
$$
这使得很多“按列的补偿更新”能用 $L$ 的列向量快速算出来，而不需要显式构造完整 $H^{-1}$。

### 5.2 把残差项也改写成可预计算结构

关键在于 $\Delta X=\tilde X-\hat X$。DuoGPT 把很多与 $r=w\Delta X$ 相关的量，重写成“只依赖 $\Delta X,\hat X,L$”的矩阵运算，从而能一次性为整层预计算出打分所需的中间量。

------

## 6）从“精确损失 $L_p^{\text{Duo}}$”到“可并行剪枝打分 $S_{:,p}$”：公式结构与含义

为了并行选 mask，DuoGPT 定义每列 $p$ 的逐行打分（对整层权重 $W$ 同时算）：

先给出一个中间形式（论文从 $L_p$ 推到 score）：
$$
S_{:,p}
=
\frac{W_{:,p}^2}{H^{-1}_{pp}}
\;+\;\text{(残差能量项)}
\;-\;\text{(残差可解释项)}
\;+\;\text{(权重-残差互相关项)}.
$$
直接实现仍很重，因为残差矩阵 $R=W(\hat X-\tilde X)= -W\Delta X$ 需要每次权重更新后跟着更新：
$$
R \leftarrow R - \Delta W(\tilde X-\hat X)=R-\Delta W\Delta X.
$$
这会让每列都出现 $O(nmk)$ 的更新开销。

### 6.1 关键近似：把残差按列分解成“外积和”，避免显式更新 $R$

把残差写成列分解：
$$
R \approx \sum_{j=1}^k W_{:,j}\Delta X_{j,:}.
$$
并聚焦于“当前列 $p$”的贡献
$$
R_p = W_{:,p}\Delta X_{p,:}.
$$
这样能把 score 进一步化简为对每列只需算三个标量序列 $a_p,b_p,c_p$，最后统一乘上 $W_{:,p}^2$。

### 6.2 最终可向量化的 score（论文核心实现式）

DuoGPT 最终把打分写成：
$$
S_{:,p}
=
W_{:,p}^2\left(
\frac{1}{H^{-1}_{pp}}
+ a_p
- b_p
+ 2c_p
\right).
$$
其中三项是：

1. $a_p$：残差输入自身能量（可直接预计算）

$$
a = \mathrm{diag}(\Delta X\Delta X^\top),\quad a_p=\Delta X_{p,:}\Delta X_{p,:}^\top.
$$

1. $b_p$：一个“经由 $H^{-1}_{-p}$”的投影能量项（用 Cholesky 结构化算）
    它可写成：

$$
b = \mathrm{diag}(UU^\top),
\quad
U_{p,:} = \Delta X_{p,:}\hat X^\top L_{p+1:,p+1:}.
$$

并且论文进一步把 $U$ 重写成共享中间量的形式：
$$
Q=\Delta X\hat X^\top L,\quad
U = Q\odot M_u
$$
其中 $M_u$ 是严格上三角 mask（只保留 $p+1$ 之后的部分），因此 $b$ 也能“平方 + 列求和”得到。

1. $c_p$：与 $\frac{H^{-1}_{:,p}}{H^{-1}_{pp}}$ 相关的线性项（用 $L_{:,p}/L_{pp}$ 快速算）
    论文给出可预计算形式：

$$
c = \mathrm{diag}(Q)\oslash \mathrm{diag}(L),
$$

其中 $\oslash$ 是逐元素除法。

此外还有一个非常重要的可复用中间量：
$$
D = UL^\top,
$$
它可以直接用于后续权重补偿更新的“残差校正部分”，避免重复计算。

这套变换带来的本质收益是：整层 score 的计算变成以矩阵乘为主的 $O(mk^2)$（而不是按每个候选权重做昂贵的 $O(nmk)$ 残差更新）。

------

## 7）Step 实现过程：你要复现/写代码时可以按这个流程走

下面我按“单层压缩”给出完整 step（多层就是从前到后逐层做，并用上一层稀疏输出作为下一层输入的 $X$）：

### Step A：准备校准输入（dense 与 sparse 两份）

1. 选校准数据（若干序列，总 token 数 $m$）
2. 对目标层，收集 dense 模型输入 $\tilde X\in\mathbb{R}^{k\times m}$
3. 从“逐层剪枝流程中的当前模型”得到输入 $X$，再按列做幅值剪枝得到 $\hat X$，并控制每列稀疏率为 $p_x$
4. 计算

$$
\Delta X = \tilde X-\hat X.
$$

### Step B：构建 Hessian 与 Cholesky 结构

1. 构建

$$
H=\hat X\hat X^\top
$$

并做稳定化（如 dampening，SparseGPT 常用）

6. 计算 $L$ 使

$$
H^{-1}=LL^\top
$$

（实现上可直接求 $H$ 的 Cholesky 再解三角系统得到 inverse-cholesky 形式）

### Step C：预计算打分所需中间量（向量化）

1. 预计算

$$
Q=\Delta X\hat X^\top L
$$

1. 由 $Q$ 得到

$$
a=\mathrm{diag}(\Delta X\Delta X^\top)
$$

### Step D：分块/按列处理，选 mask + 做补偿更新（类似 SparseGPT 的 blocking + lazy update）

1. 设 block size $B$。对每个 block 的列区间 $[i,i+B)$：

- 计算该 block 内每列的 score（对所有行并行）：

$$
S_{:,p}=W_{:,p}^2\left(\frac{1}{(\mathrm{diag}(L)_p)^2}+a_p-b_p+2c_p\right)
$$

（这里用 $1/H^{-1}_{pp}=1/L_{pp}^2$ 的关系）

- 在 block 内对每行选择要剪掉的比例 $p_w$（最小 score 的权重置 0），得到 mask $M_w$

1. 对每一列 $j$（按固定顺序，不必严格最优顺序），执行补偿更新：

- 令当前列剪掉后的权重为 $P_{:,j}=W_{:,j}\odot M_{w,:,j}$
- 定义误差项（与 SparseGPT 类似的三角更新形式）：

$$
E_{:,j-i}=\frac{W_{:,j}-P_{:,j}}{L_{jj}}
$$

- 更新后续列（block 内）：

$$
W_{:,j:(i+B)} \leftarrow W_{:,j:(i+B)} - E_{:,j-i}L^\top_{j,\,j:(i+B)} + W_{:,j}D_{j,\,j:(i+B)}
$$

其中第二项 $+W_{:,j}D$ 就是把“残差校正效应”以预计算形式融进更新。

- 对 block 外做 lazy update（合并写回）：

$$
W_{:,(i+B):}\leftarrow W_{:,(i+B):}-E\,L^\top_{i:(i+B),\,(i+B):}+W_{:,i:(i+B)}D_{i:(i+B),\,(i+B):}
$$

1. 最后对整层应用 mask：

$$
W\leftarrow W\odot M_w.
$$

这就得到：在“运行时激活稀疏”设定下更鲁棒的权重稀疏模型（dual-sparse）。

------

## 8）一个理论点：DuoGPT 对 SparseGPT 的保证改进（趋势解释）

论文给出一个“校准损失改进下界”的形式，核心结论是：
$$
\Delta L = L_{\text{SparseGPT}}-L_{\text{DuoGPT}}
= r\hat X^\top H^{-1}_{-p}\hat X r^\top \ge 0,
$$
并进一步给出与激活稀疏率 $p_x$ 线性相关的下界趋势（省略常数细节）：
$$
\Delta L \gtrsim \frac{p_x \cdot \sigma_r^2 \cdot \|w\|^2}{\lambda_{\max}(H)}.
$$
直觉：激活越稀疏（$p_x$ 越大），dense-sparse 差异残差越明显，DuoGPT 的“残差校正项”带来的收益越大，这也解释了它在高双稀疏率下优势更明显。

------

## 9）一些可延展的 idea（先给方向，不展开详细融合建议）

1. 把 DuoGPT 的“残差校正”从逐行扩展到“低秩子空间校正”
    当前 $r=w\Delta X$ 是行级残差。可以考虑对 $\Delta X$ 的列空间做一个低秩基 $B$（例如取 $\Delta X$ 的主成分），把校正项从 $r\hat X^\top H^{-1}_{-p}$ 改成在子空间上的更稳定估计，减少小样本噪声。
2. 把激活稀疏从“统一 $p_x$”升级为“按层/按通道的风险分配”
    DuoGPT 用统一 $p_x$ 简化问题，但从公式上看，关键量其实是 $a_p,b_p,c_p$（都由 $\Delta X,\hat X,L$ 决定）。可以基于这些量构造“每层可承受的激活稀疏预算”，让整体误差更均衡。
3. 用 score 的分解结构做“误差归因/可解释剪枝”

$$
S_{:,p}=W_{:,p}^2\Big(\underbrace{1/H^{-1}_{pp}}_{\text{OBC本体}}
+\underbrace{a_p}_{\Delta X能量}
-\underbrace{b_p}_{\text{可解释部分}}
+\underbrace{2c_p}_{\text{互相关}}\Big)
$$

这实际上给了一个天然的归因：到底是“权重本体重要性”主导，还是“激活稀疏残差”主导。你可以把它做成可解释分析，甚至做成自适应策略（例如当 $a_p$ 主导时更保守）。

4.运行时 kernel 视角：让离线 mask 更“行均匀”以降低最坏情况 SRAM load

论文提到双稀疏在最坏情况下的权重加载比例可能接近 $(1-p_x)$，而经验上他们观察更接近 $(1-p_w)(1-p_x)$。可以把“行稀疏均匀性”显式加入 mask 选择的正则项，避免少数行过密导致最坏情况退化。
