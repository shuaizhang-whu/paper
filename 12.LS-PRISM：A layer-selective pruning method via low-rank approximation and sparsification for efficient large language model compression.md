# LS-PRISM（Layer-Selective Pruning via Low-Rank Approximation and Sparsification）论文分析

（详细理论 + 公式链 + Step 实现流程 + Ideas）

LS-PRISM 的核心不是“再提出一种新的 SVD 近似”，而是把 LLM 压缩当成一个**“层/矩阵异质敏感”的选择问题**：

- **Stage 1**：对每层中若干关键矩阵（Q/K/V/O、MLP gate/up/down）尝试低秩近似，但**只保留那些在校准集上不降精度且不增 loss 的近似**；同时 rank 不是手工设，而是通过 **KMIF（KMeans + Isolation Forest）\**从奇异值谱里\**动态选取**。
- **Stage 2**：对 Stage 1 未被低秩替换的原始矩阵做 **Wanda 非结构化剪枝**得到稀疏；可选用 **LoRA**对剪枝造成的性能损失做轻量恢复。

------

## 1) 从“统一压缩”到“选择性压缩”：优化目标如何变化

### 1.1 传统低秩压缩（单矩阵目标）

对权重矩阵 $W\in\mathbb{R}^{m\times n}$，经典截断 SVD/低秩近似是：
$$
\min_{\operatorname{rank}(W')\le r}\ \|W-W'\|_F^2,
\quad W'=U_r\Sigma_rV_r^\top
\tag{1}
$$
或写成因子分解形式 $W'\approx AB$：
$$
\min_{A\in\mathbb{R}^{m\times r},\,B\in\mathbb{R}^{r\times n}}\ \|W-AB\|_F^2
\tag{2}
$$
现实里很多方法会对所有层/矩阵用同一比例或同一 rank（“统一压缩强度”），导致对敏感层过压缩。

------

### 1.2 LS-PRISM：把“压缩”写成**约束选择优化**

LS-PRISM 的 Stage 1 本质是：在校准集 $\mathcal{D}_{calib}$ 上，以原模型性能为基线，**逐个矩阵尝试替换**，只接受“性能不差”的替换。

可形式化为一个组合优化（把“选哪些矩阵做低秩、每个选多大 rank”一起决定）：

- 设候选矩阵集合 $\mathcal{L}$（例如每层的 Q/K/V/O、gate/up/down）。
- 对每个候选矩阵 $\ell$，引入决策变量：
  - $z_\ell\in\{0,1\}$：是否将该矩阵替换为低秩近似
  - $r_\ell$：替换时选择的截断 rank

用压缩后模型记为 $M(\mathbf{z},\mathbf{r})$。以“参数量/存储”作为压缩代价（你也可以换 FLOPs）：
$$
\min_{\mathbf{z},\mathbf{r}}\ \mathrm{Cost}\big(M(\mathbf{z},\mathbf{r})\big)
\quad\text{s.t.}\quad
\mathrm{ACC}\big(M(\mathbf{z},\mathbf{r});\mathcal{D}_{calib}\big)\ge \mathrm{ACC}(M_{orig})
,\ 
\mathcal{L}\big(M(\mathbf{z},\mathbf{r});\mathcal{D}_{calib}\big)\le \mathcal{L}(M_{orig})
\tag{3}
$$
这就是 LS-PRISM 的“改进后目标”：**不是单矩阵最小化 $\|W-W'\|$**，而是**“在不劣化校准指标的前提下尽可能压缩”**（约束/筛选式目标）。

> 论文实现上不是直接求解(3)，而是用贪心：逐个矩阵尝试替换并验证约束是否成立（见 Algorithm 2 的 accept/reject 逻辑）。

------

## 2) Stage 1：动态低秩近似（核心：动态 rank + 动态矩阵选择）

### 2.1 动态 rank 选择（KMIF）：从奇异谱里自动找 $r$

对候选矩阵 $W$，先做 SVD：
$$
W = U\Sigma V^\top,\quad \Sigma=\operatorname{diag}(\sigma_1,\dots,\sigma_p),\ p=\min(m,n),\ \sigma_1\ge\dots\ge\sigma_p
\tag{4}
$$
由于奇异值跨尺度很大，先做对数变换：
$$
\hat{\sigma}_i=\log(\sigma_i)
\tag{5}
$$
**KMeans 聚类（$k=2$）**把 $\{\hat\sigma_i\}$ 分成两簇：
$$
C_1,C_2=\mathrm{KMeans}(\{\hat\sigma_i\},k=2)
\tag{6}
$$
定义“dominant cluster” $C_{dom}$：包含最大奇异值的那一簇（对应“更重要”的谱段）。

然后对 $C_{dom}$ 用 **Isolation Forest**做异常检测，得到每个奇异值是否为 outlier：
$$
\mathrm{IForest}(\hat\sigma)=
\begin{cases}
-1,& \hat\sigma\ \text{是异常点}\\
+1,& \text{否则}
\end{cases}
\tag{7}
$$
把异常点索引收集成有序集合：
$$
O=\{\,i\mid \mathrm{IForest}(\hat\sigma_i)=-1,\ \hat\sigma_i\in C_{dom}\,\}
\tag{8}
$$
最后用“**异常索引序列的第一个显著断点**”确定截断 rank：
$$
r=\min\{\,O_k\ \mid\ O_{k+1}-O_k>1\,\}
\quad (\text{若无断点则 } r=p)
\tag{9}
$$
得到低秩近似：
$$
W' = U_r\Sigma_r V_r^\top
\tag{10}
$$

> 直觉：KMIF 试图从奇异谱里找到“重要谱段的自然边界”，避免固定去掉 30%/50% 那种粗暴策略。

------

### 2.2 动态矩阵选择：只保留“有利/无害”的近似

LS-PRISM 对每个候选矩阵 $\ell$ 做如下“试替换—评估—决定”：

- 原模型在 $\mathcal{D}_{calib}$ 上先评估得到基线：

$$
\mathrm{ACC}_{orig},\ \mathcal{L}_{orig}
\tag{11}
$$

- 对矩阵 $\ell$：
  1. 用 KMIF 算出 $r_\ell$，构造 $W'_\ell$
  2. 暂时替换得到临时模型 $M'$
  3. 在 $\mathcal{D}_{calib}$ 上评估得到 $\mathrm{ACC}',\mathcal{L}'$
  4. 接受条件：

$$
(\mathrm{ACC}'\ge \mathrm{ACC}_{orig})\ \land\ (\mathcal{L}'\le \mathcal{L}_{orig})
\tag{12}
$$

满足则“永久替换”，否则回滚。

这一套机制就是在用“硬约束”近似求解前面(3)式的选择优化。

------

## 3) Stage 2：稀疏化 + 可选 LoRA 恢复

### 3.1 Wanda 非结构化剪枝：对剩余矩阵做稀疏化

对 Stage 1 未被低秩替换的原始权重矩阵 $W$，Wanda 使用校准激活的列范数来估计权重重要性。对元素 $W_{ij}$ 的剪枝分数：
$$
s_{ij} = |W_{ij}|\cdot \|X_j\|_2
\tag{13}
$$
其中 $X_j$ 是输入激活的第 $j$ 个通道（列向量），$\|X_j\|_2$ 反映该输入通道在校准数据上的强度。

剪枝策略：剪掉分数最低的比例 $s$（论文实验常设 50% 左右的稀疏度）。

------

### 3.2 可选 LoRA：在保持稀疏结构的前提下恢复性能

剪枝后得到稀疏权重 $W_{sparse}$。LoRA 增加一个低秩增量：
$$
W_{tuned} = W_{sparse} + \Delta W,
\quad \Delta W = AB^\top,
\quad A\in\mathbb{R}^{d\times r},\,B\in\mathbb{R}^{r\times d}
\tag{14}
$$
训练时仅更新 $A,B$，不改动 $W_{sparse}$，从而保持稀疏带来的压缩收益。

------

## 4) LS-PRISM 的完整 Step-by-step 实现流程（可直接照着写工程）

下面按“你要复现整套管线”的角度写成可执行步骤。

### Step 0：定义候选矩阵集合与校准数据

- 校准集 $\mathcal{D}_{calib}$：Stage 1 和 Stage 2 可以用不同数据（论文中也这么做），但实现上你也可以统一。
- 候选矩阵集合 $\mathcal{L}$：每层 attention 的 $W_Q,W_K,W_V,W_O$，以及 MLP 的 $W_{gate},W_{up},W_{down}$。

------

### Step 1：评估原模型基线指标

在 $\mathcal{D}_{calib}$ 上计算：
$$
\mathrm{ACC}_{orig},\ \mathcal{L}_{orig}
$$

------

### Step 2：Stage 1（动态低秩近似 + 动态选择）

对每个候选矩阵 $\ell\in\mathcal{L}$（可按层顺序，也可按模块顺序）：

1. **SVD**：$W_\ell=U\Sigma V^\top$
2. **KMIF 选 rank**：按(5)-(9)得到 $r_\ell$
3. **构造近似**：$W'_\ell=U_{r_\ell}\Sigma_{r_\ell}V_{r_\ell}^\top$
4. **临时替换并评估**：得到 $\mathrm{ACC}',\mathcal{L}'$
5. **accept/reject**：用(12)判断
   - accept：保留 $W'_\ell$
   - reject：回滚 $W_\ell$

实现细节建议：

- 为了存储效率，把 $W'_\ell$ 存为 $(U_{r_\ell},\Sigma_{r_\ell},V_{r_\ell})$ 或直接存为两层线性形式（如 $U_{r_\ell}\Sigma_{r_\ell}$ 和 $V_{r_\ell}^\top$），避免存满矩阵。
- 评估指标若是困惑度/负对数似然，ACC 可替换成你关心任务的 proxy（或仅用 loss）。

得到部分低秩化模型 $M_{LR}$。

------

### Step 3：Stage 2（Wanda 稀疏化）

对 $M_{LR}$ 中“仍是原始全矩阵”的权重：

1. 收集校准激活通道范数 $\|X_j\|_2$
2. 计算剪枝分数 $s_{ij}$（式(13)）
3. 剪掉最低分的比例 $s$，得到 $W_{sparse}$

得到稀疏模型 $M_{LR+sp}$。

------

### Step 4（可选）：LoRA 微调恢复

若启用 LoRA：

1. 对被剪枝的层挂载 LoRA $A,B$，$\Delta W=AB^\top$
2. 用少量样本、少量 step 微调，仅更新 $A,B$
3. 导出最终模型：

$$
M_{compressed}=\{W_{LR},\ W_{sparse},\ \Delta W\}
\tag{15}
$$

------

## 5) 方法的关键“理论直觉”总结（你写新论文时可复用的观点）

1. **rank 选择应该是“谱形状驱动”的**：同样的去除比例对不同矩阵可能完全不合适；KMIF 用聚类+异常检测去找谱断点。
2. **矩阵是否能被低秩替换是“性能条件”的**：哪怕某矩阵谱衰减很好，也可能是任务敏感矩阵；LS-PRISM 用校准集上的 loss/acc 约束做过滤。
3. **低秩与稀疏互补**：低秩更像“结构压缩”，稀疏更像“细粒度修剪”；先低秩选一部分“好压缩的矩阵”，剩下的再稀疏化，整体更稳。

------

## 6) Ideas（只给方向，不展开结合细节）

### Idea 1：把 accept/reject 从“硬阈值”变成“可控的多目标打分”

当前是：
$$
\mathrm{ACC}'\ge \mathrm{ACC}_{orig},\ \mathcal{L}'\le \mathcal{L}_{orig}
$$
你可以改成软目标：
$$
\max\ \Delta \mathrm{Score} = \lambda_1(\mathrm{ACC}'-\mathrm{ACC}_{orig})-\lambda_2(\mathcal{L}'-\mathcal{L}_{orig})-\lambda_3\Delta\mathrm{Cost}
$$
让选择更平滑、可调压缩强度（更像 Pareto）。

### Idea 2：KMIF 的“谱断点”可以换成更稳的统计准则

例如：

- 用累计能量阈值：选最小 $r$ 满足 $\sum_{i\le r}\sigma_i/\sum_i\sigma_i\ge \tau$
- 用二阶差分/拐点检测替代 IForest（更便宜、更确定）
- 让 rank 依赖激活统计（把谱断点与数据分布耦合）

### Idea 3：Wanda 的非结构化稀疏可替换为“硬件友好稀疏”

把 element-wise 稀疏改成：

- $n:m$ 结构化稀疏
- block 稀疏
- channel-wise 稀疏
   并且把剪枝分数从 $|W_{ij}|\|X_j\|_2$ 扩展为“输出误差敏感度”proxy。

### Idea 4：把校准数据从“少量通用样本”升级为“任务/能力覆盖”的混合校准

论文也观察到复杂推理任务更敏感，你可以构造：
$$
\mathcal{D}_{calib}=\bigcup_{t=1}^T \mathcal{D}_t
$$
并在 accept/reject 中按任务加权评估，避免只对少数能力过拟合。

### Idea 5：两阶段顺序可做“闭环迭代”

当前是 Stage1 → Stage2（→ LoRA）。可尝试：

- Stage2 后再回到 Stage1 重新评估 rank/选择（模型已变，敏感性也变）
- 或在每层“低秩替换”后局部做轻量稀疏/再评估，形成 layer-wise 闭环。